\documentclass[letterpaper,twoside,12pt]{book} 
%\usepackage[left = 0.5in, right = 0.5in, top = 0.9in, bottom = 0.9in]{geometry}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage[spanish,es-nodecimaldot,es-tabla]{babel}
\usepackage[utf8]{inputenc}
\usepackage[dvipsnames,svgnames,table]{xcolor}  
\usepackage{CIMATpreamble}  
% \usepackage[dvipsnames]{xcolor}
%\usepackage[12pt]{extsizes}
\linespread{1.1}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathrsfs}
\usepackage{tikz-cd}
\usepackage[bbgreekl]{mathbbol}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage[colorlinks=true,linkcolor=red,citecolor=red]{hyperref}
\usepackage{apptools}
\AtAppendix{\counterwithin{lema}{chapter}}
\graphicspath{{img/}}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\B}{\mathcal{B}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\W}{\dot{W}}
\newcommand{\1}{\mathds{1}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\inv}{^{-1}}
\renewcommand{\to}{\rightarrow}
\newcommand{\ent}{\Longrightarrow}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\renewcommand{\qedsymbol}{$\blacksquare$}

\theoremstyle{definition}
\newtheorem{dfn}{Definición}
\theoremstyle{definition}
\newtheorem{teo}{Teorema}
\theoremstyle{remark}
\newtheorem{proofpart}{Parte}
\theoremstyle{definition}
\newtheorem{cor}{Corolario}
\theoremstyle{definition}
\newtheorem{prop}{Proposición}
\theoremstyle{definition}
\newtheorem{obs}{Observación}
\theoremstyle{definition}
\newtheorem{ejem}{Ejemplo}
\theoremstyle{definition}
\newtheorem{lema}{Lema}



\title{\textbf{}}
\author{Iván Irving Rosas Domínguez}
\date{\today}

\DeclareSymbolFontAlphabet{\mathbbm}{bbold}
\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
\DeclareMathSymbol\bbDelta  \mathord{bbold}{"01}


\author{Ivan Irving Rosas Domínguez}
\documentType{T E S I S}  
\title{Sobre convergencia de densidades de promedios espaciales para la ecuación estocástica del calor}
\degree{Maestría en Ciencias con Orientación en Probabilidad y Estadística}
\supervisor{Dr. Arturo Jaramillo Gil}
%\supervisorSecond{} 
\cityandyear{Guanajuato, Gto., ?? de ?? de 2024}


\begin{document}
\maketitle 
\thispagestyle{empty}  
\frontmatter

% % Dedication
\chapter*{}
\begin{flushright}%
 \emph{Dedicatoria \dots}
  \thispagestyle{empty}
\end{flushright}

% % Abstract
\chapter*{Introducción}
\addcontentsline{toc}{chapter}{Introducción}

Este trabajo está basado principalmente en el artículo de \c{S}efika Kuzgun y David Nualart \cite{KUZGUN202268} publicado en el año 2022. El objetivo principal es estudiar la convergencia de las densidades de los promedios espaciales de la solución de la ecuación estocástica del calor (SHE, por sus siglas en inglés), en distancia uniforme, a la densidad de una distribución normal estándar. 

Trabajo previo se puede hallar desde el artículo de 1998 de Maria Emilia Caballero, Begoña Fernández Fernández y David Nualart \cite{Caballero1998-hz}, en donde se demuestra la existencia y unicidad de una densidad continua y acotada para una variable aleatoria $X$ que cumpla ciertas condiciones. Con la llegada del nuevo milenio, técnicas que mezclan al cálculo de Malliavin junto con el método de Stein son propuestas por Nourdin y Peccati en su monografía \cite{Nourdin_Peccati_2012}. La fusión de ambas disciplinas conlleva a una amalgama que resulta muy útil a la hora de establecer teoremas tipo límite central, así como dar tasas de convergencia explícitas para los mismos. 

Dentro de esta explosión de resultados, podemos citar el trabajo de Yaozhong Hu, Fei Lu y David Nualart \cite{HU2014814}, en donde se estudia la convergencia de las densidades de algunos procesos gaussianos. Posteriormente, en 2020, Jingyu Huang, Lauri Viitasaari y David Nualart en \cite{HUANG20207170} logran demostrar un teorema central del límite para la solución a la ecuación estocástica del calor. Más aún, ellos son capaces de demostrar cotas explícitas para la velocidad de convergencia de las distribuciones de la solución a la ecuación estocástica del calor a la distribución normal, utilizando para ello la distancia en variación total. 

\c{S}efika Kuzgun y David Nualart, \cite{KUZGUN202268} logran, a partir de estos resultados, exhibir la existencia y unicidad de las densidades de los promedios espaciales de la solución a la ecuación estocástica del calor, y mostrar la convergencia de estas densidades, en distancia uniforme, a la densidad de una variable aleatoria normal, otorgando una tasa de convergencia explícita para la misma.

Para lograr este objetivo, es menester estudiar los conceptos utilizados en el trabajo de Kuzgun y Nualart. Es de destacar desde ahora que en este texto la intención no es, en lo más mínimo, dar un estudio amplio y completo de cada una de las disciplinas anteriores, sino cubrir de manera suficiente pero sucinta la herramienta técnica necesaria para abordar el problema. Así, en esta tesis se da un breve recorrido por las ideas que yacen en el núcleo de la teoría de ecuaciones diferenciales parciales estocásticas (SPDE, por sus siglas en inglés), del cálculo de Malliavin, del método de Stein, así como la amalgama que de estas últimas dos disciplinas surge con la intención de estudiar la existencia de densidades de variables aleatorias bajo ciertas hipótesis, y la deducción de teoremas límite. 

De lo anterior se desprende la estructura del presente texto. En el primer capítulo se estudiarán los elementos esenciales de ecuaciones diferenciales parciales estocásticas. Específicamente, se enuncia brevemente la heurística que da origen a la formulación de las mismas. Se realiza la construcción de la integral de Itô-Walsh, y con dicha integral se formula la solución mild de una ecuación diferencial parcial estocástica. Posteriormente nos enfocamos en el contexto de la ecuación estocástica del calor, así como en su solución. Finalmente se enuncian algunos resultados necesarios para abordar el problema principal.

El capítulo dos está dedicado al estudio de los elementos básicos del cálculo de Malliavin. Este capítulo se divide a su vez en dos partes. En la primera se hace un estudio del caso unidimensional, siendo siendo este caso estudiado con mediano detalle, aunque siempre manteniéndose al mínimo la teoría requerida para nuestro propósito. La segunda parte del mismo se compone del caso general. En esta segunda parte sólo se estudia en mediano detalle el aparato necesario para enunciar los resultados análogos al caso unidimensional, mientras que dichos resultados en múltiples ocasiones serán solo postulados, pero no demostrados. 

La composición del capítulo tres es similar a la del capítulo anterior. La primera parte del mismo consiste en un estudio breve del método de Stein, mencionando tangencialmente las aplicaciones que esta disciplina puede otorgar. La segunda parte del capítulo consiste principalmente en estudiar un resultado que garantiza la existencia y unicidad de la densidad de una variable aleatoria, así como en estudiar la existencia de una cota para la distancia uniforme entre la densidad de una variable adecuada, y la densidad de una variable normal estándar. Este último resultado resulta ser la columna vertebral sobre la que se sostiene el resultado principal, y es demostrada con detalle, y además es en esta parte en donde se hace uso de la amalgama que el cálculo de Malliavin y el método de Stein conforman. El estudio de las herramientas necesarias para abortar el problema principal culmina en esta parte.

El enunciado y prueba del resultado principal se encuentran hasta el capítulo cuarto. Es aquí en donde se hace rigurosa la formulación del problema que se plantea. Se enuncian dos lemas de carácter técnico que son esenciales en la prueba, y finalmente se estudia y concluye el problema principal. Se agrega un último capítulo en el cual se hace una breve reflexión sobre lo realizado, y se echa una mirada al panorama que este problema nos presenta.

Los primeros tres capítulos comienzan con un muy breve resumen histórico de la disciplina a estudiar, así como de la relevancia de la misma en la actualidad. El cuarto capítulo repasa el trabajo previo que ha sido mencionado al inicio de esta introducción, pero con mayor rigurosidad. 

Es importante mencionar que ninguno de los resultados enunciados y tampoco la gran mayoría de heurísticas sobre algún tema en particular, que están presentes en este trabajo, pertenecen al autor del mismo, por lo que siempre que se usen proposiciones o ideas ajenas, se procurará hacer la correspondiente mención de su origen. De esta forma, el aporte del autor está en explicar de la manera más simple que le ha sido posible los resultados, así como rellenar algunos detalles (y no todos) en las pruebas, o bien unificar algunas ideas sueltas de las fuentes consultadas. 


\textbf{Palabras clave:} \textit{Integral de Itô-Walsh, Ecuación del calor estocástica, promedios espaciales, cálculo de Malliavin, método de Stein, existencia de densidades, convergencia uniforme, teoremas tipo límite, tasas de convergencia.}
% % Acknowledgements
\chapter*{Agradecimientos}
 \addcontentsline{toc}{chapter}{Agradecimientos}

A mis padres \ldots

% Table of contents and list of figures
\tableofcontents

%\listoffigures

% Chapters





\mainmatter

\chapter{Elementos de ecuaciones diferenciales parciales estocásticas}

En este capítulo hablaremos acerca del concepto de ecuación diferencial parcial estocástica (SPDE, por sus siglas en inglés).
Dichos objetos comenzaron a ser estudiados a mediados del siglo pasado con la llegada de la integral de Itô.
El trabajo de Itô y demás colegas para tratar ecuaciones diferenciales estocásticas inmediatamente sugirió el estudio de la posible generalización a ecuaciones diferenciales en varias variables.
Durante las décadas de los 50, 60 y 70, varios artículos hacían alusión a estos objetos, aunque no fue sino hasta finales de la década de 1970 cuando estas ecuaciones comenzaron a ser estudiadas como objetos matemáticos en sí.
Finalmente, en la década de 1980, varias monografías surgieron con el propósito de construir una teoría que cohesionara las ideas existentes en el área. Destaca en particular la monografía \cite{Walsh_J.B_Introduction_to_SPDEs}, escrita por Walsh  y publicada en 1986, en donde el autor aborda el estudio de estas ecuaciones, dando un significado preciso a lo que significa plantear y resolver una ecuación diferencial que involucre procesos estocásticos cuyas trayectorias generalmente no son diferenciables, y que tengan tanto una variable espacial como una temporal.

Desde esta década múltiples trabajos en esta área han sido publicados. Esto, junto con el uso de herramientas como cálculo de Malliavin en el estudio de las mismas, han hecho de las Ecuaciones diferenciales parciales estocásticas una área bastante activa en la probabilidad moderna.

En la primera parte de este capítulo, hacemos un recuento de las ideas de las soluciones débiles a una ecuación diferencial parcial clásica. Dichas ideas son clave para estudiar el caso estocástico. La idea de la integral de Itô-Walsh surge de manera natural en esta parte. Es importante destacar que esta primera sección introductoria, se presenta solo ideas y razonamientos formales, dejando la parte rigurosa y en el contexto que nos interesa para el final del capítulo. Dichas ideas están basadas en \cite{evans2010partial}, \cite{BonderEcuaciones} y \cite{Khoshnevisan2009}.

\section{Introducción a las SPDE}

Comenzamos en este capítulo con la pregunta clave: ¿qué es una ecuación diferencial parcial estocástica? Para responder a esta pregunta, podemos repasar cada uno de los conceptos que nos llevan a esta idea. Partimos del mundo clásico en $\R^{N}:$ sean $x\in \R^{N}$, $t\in [0,\infty)$ y $u:\R^{N}\times[0,\infty)\to \R$ una función en las variables $x$ y $t$. Una ecuación diferencial parcial es una ecuación del tipo 

\begin{equation}\label{ec_dif_parc}
    F(t,x,u,Du,D^2u,...)=0,    
\end{equation}

donde $x$ y $t$ se interpretan como las variables espacial y temporal respectivamente, y $F$ es una función arbitraria que depende tanto de las variables espaciales como de la temporal, así como de la misma función $u$ y de sus derivadas de orden $\alpha=(\alpha_1,...,\alpha_n)$, donde $\alpha_i$ representa la derivada parcial en la $i$-ésima coordenada.

Si existe una función $u$ tal que $u\in C^{\alpha}\left(\R^{N}\times [0,\infty)\right)$ y además $u$ satisface la ecuación \eqref{ec_dif_parc}, donde $\alpha$ es el multi-índice más grande tal que el orden de todas las derivadas que aparecen en la ecuación son cubiertas por el mismo, entonces decimos que $u$ es una solución clásica a la ecuación diferencial parcial anterior. De esta manera, y en particular para la ecuación estocástica del calor homogénea clásica, la cual está dada por 
   \begin{equation*}
      \partial_tu(x,t)-\frac{1}{2}\Delta_x u(x,t)=0,
   \end{equation*}
se entiende por solución clásica a una función $u:\R^{N}\times[0,\infty)\to \R$ tal que $u\in C^{2,1}\left(\R^{N}\times [0,\infty)\right)$, y que cumpla la ecuación anterior. En particular, la solución clásica cumple que al menos todas las segundas derivadas en las variables espaciales existen, son continuas, y al menos la derivada en la variable temporal existe y es continua.

Cabe destacar que la noción de solución de un ecuación diferencial parcial en el sentido clásico coincide con lo esperable: una función $u$ será solución de una ecuación diferencial si al menos tiene tantas derivadas como aquellas involucradas en la igualdad \eqref{ec_dif_parc}.

No obstante, en algunas ocasiones los problemas planteados requieren soluciones que no necesariamente cumplan tales características de regularidad, pero que intuitivamente deberían tener sentido.

\subsection{Soluciones débiles de una EDP}

Con motivo de resolver problemas como el planteado anteriormente, pero en los que en principio la regularidad es un problema, surge la idea de las \textit{soluciones débiles} de las ecuaciones diferenciales parciales. Un ejemplo de ellos es aquél problema de onda con una condición inicial que no necesariamente es regular. Por ejemplo, si hablamos del siguiente problema de onda unidimensional homogéneo dado por:
\begin{equation}\label{wave_eq_sobolev}
    \begin{cases}
        \partial_{tt}u(x,t)-\kappa^2\partial_{xx}u(x,t)=0, & (x,t)\in [-\pi,\pi]\times[0,\infty),\\
        u(-\pi,t)=u(\pi,t)=0, & t\in [0,\infty),\\
        u(x,0)=u_0(x)=\pi-\abs{x}, & x\in [-\pi,\pi],\\
        u_t(x,0)=0, & (x,t) \in [-\pi,\pi]\times[0,\infty),\\
    \end{cases}
\end{equation}
es claro que tenemos una inconsistencia al momento de colocar la condición inicial dada por el valor absoluto $\pi-|x|$ pues dicha función no es derivable en $x=0$. No obstante, desde un punto de vista intuitivo, claramente el problema tiene sentido: estamos modelando el comportamiento de una cuerda sujeta en sus extremos a los puntos $-\pi$ y $\pi$, la cual al soltarla tiene una posición inicial dada por $\pi-|x|$ y cuya velocidad inicial es cero en cualquier punto de la cuerda.

La idea para hallar una solución al problema anterior, a grandes rasgos, consiste en reformular el problema de forma que se conserve la esencia del problema, y posteriormente obtener una solución a este problema reformulado. Una manera estándar es utilizar la teoría de distribuciones. En el caso particular de la ecuación de onda anterior, se procede formalmente de la siguiente manera:

Supongamos que $u$ es una solución al problema $\eqref{wave_eq_sobolev}$. En particular se debería cumplir que 
\[
\partial_{tt}u(x,t)=\kappa^2\partial_{xx}u(x,t),
\]
por lo que para cualquier función $\phi\in C^{\infty}\left([-\pi,\pi]\times [0,\infty)\right)$ tal que sus derivadas se anulan en la frontera de $[-\pi, \pi]$, se tiene que al multiplicar e integrar, la ecuación anterior se convierte en:
\[
    \int_{[-\pi,\pi]\times [0,\infty)}\partial_{tt}u(x,t)\phi(x,t)dx dt=\kappa^2\int_{[-\pi,\pi]\times [0,\infty)}\partial_{xx}u(x,t)\phi(x,t)dx dt,
\]
y suponiendo que es válido utilizar integración por partes, la igualdad anterior equivale a
\[
\int_{[-\pi,\pi]\times [0,\infty)}u(x,t)\partial_{tt}\phi(x,t)dx dt=\kappa^2\int_{[-\pi,\pi]\times [0,\infty)}u(x,t)\partial_{xx}\phi(x,t)dx dt,
\]
en donde los términos evaluados en la frontera se anulan gracias a que las funciones $\phi$ (a menudo denominadas funciones de prueba o funciones \textit{test}) se anulan en la misma. Reordenando los términos anteriores, se obtiene una reformulación de la ecuación diferencial del problema \eqref{wave_eq_sobolev} la cual no necesita que la función $u$ sea derivable. A saber, vamos a decir que $u$ es una \textit{solución débil} de la ecuación diferencial asociada al problema \eqref{wave_eq_sobolev} si se cumple que 
\[
\int_{[-\pi,\pi]\times [0,\infty)}u(x,t)(\partial_{tt}\phi(x,t)-\kappa^2\partial_{xx}\phi(x,t))dx dt=0,    
\]
para cualquier función $\phi\in C^{\infty}([-\pi,\pi]\times [0,\infty))$ cuyas derivadas se anulen en la frontera.

Claramente toda solución clásica (o fuerte) es a su vez una solución débil, ya que precisamente la regularidad de dichas soluciones permiten hacer de manera rigurosa los cálculos formales anteriores. No obstante, la existencia de una solución débil en el sentido anterior no garantiza que dicha solución sea regular, por lo que en principio es más sencillo demostrar la existencia o unicidad de las soluciones débiles, a pesar de que estas gocen de menos propiedades que las soluciones fuertes o clásicas, las cuales no siempre existirán, como en el caso del problema $\eqref{wave_eq_sobolev}$.

Llegados a este punto, reflexionamos ahora en dirección de la probabilidad. Supongamos nuevamente que tenemos la ecuación de onda unidimensional, pero considerando el caso no homogéneo. Tenemos entonces la siguiente ecuación
\begin{equation}\label{wave_spde}
\partial_{tt}u(x,t)=\kappa^2\partial_{xx}u(x,t)+F(x,t), \qquad (x,t)\in [-\pi,\pi]\times[0,\infty)
\end{equation}
donde ahora $F$ es una función que representa la cantidad de presión por unidad de longitud que se aplica a la cuerda. Bajo condiciones de regularidad en la función $F$, podemos resolver el problema de manera clásica utilizando el método de separación de variables. Sin embargo, nos preguntamos ¿qué sucede si ahora $F$ representa una función que no sea diferenciable? Más aún, ¿qué pasa si consideramos a $F$ una \textit{perturbación aleatoria}?

Para dar un sentido más preciso a las ideas anteriores podemos pensar, para $x$ y $t$ fijos, a $F(x,t)$ como una variable aleatoria. Dicha variable depende tanto del espacio como del tiempo, y por ende podemos interpretar a $F$ como un proceso estocástico que evoluciona de manera espacio-temporal. Tal y como se comentó al inicio, es conocido que existen procesos estocásticos cuyas trayectorias son no diferenciables, por lo que en principio pensar en resolver una ecuación diferencial de este estilo de manera clásica se vislumbra una tarea complicada.

Más aún, ¿en qué sentido se debe interpretar una ecuación como \eqref{wave_spde}? O bien, considerando la siguiente ecuación del calor no homogénea (ecuación que será de nuestro interés),  
\begin{equation}\label{heat_spde}
    \partial_t{u(x,t)}=\frac{1}{2}\Delta_{x}u(x,t)+F(x,t), \qquad (x,t)\in \R^{N}\times[0,\infty), \ N\geq1,
\end{equation}
en donde el término $F$ es aleatorio, ¿qué significa hablar de una solución a dicha ecuación?

Obsérvese que, si bien la teoría de distribuciones nos permite lidiar con la parte de la no regularidad de las condiciones del problema, para replicar el mecanismo que fue utilizado para reformular\eqref{wave_eq_sobolev} en un sentido débil, nos encontramos con una dificultad pues $F(x,t)$ es ahora un proceso estocástico, y a no ser que dicho proceso sea de variación finita, no será posible realizar de manera rigurosa la transición al momento de integrar por partes.

\subsection{Heurística de la formulación mild}

Un vistazo más a detalle de lo anterior puede arrojarnos luz sobre cómo replantear de manera rigurosa un \textit{problema del calor estocástico} como el anterior. Haciendo nuevamente cálculos formales, una manera de proceder sería la siguiente: supongamos que tenemos la siguiente ecuación diferencial del calor unidimensional no lineal,
\[
\partial_tu(x,t)=\frac{1}{2}\partial_{xx}u(x,t)+f(u(x,t))W(x,t), \qquad (x,t)\in [0,L]\times [0,\infty) 
\]
donde $W(x,t)$ representa nuestra fuente de aleatoriedad (aunque bien puede pensarse como una función suficientemente suave en principio). Utilizando la técnica presentada en el ejemplo de la solución débil a la ecuación de onda, podríamos tomar una función $\phi \in C^{\infty}([0,L])$ cuya derivada se anule en la frontera, multiplicarla en ambos miembros de la ecuación anterior e integrar con respecto al espacio y al tiempo, y suponiendo que de alguna manera logramos integrar con respecto la fuente aleatoria $W(t,x)$, tendremos la siguiente ecuación formal, 
\[
\int_{0}^{L}u(x,t)\phi(x)dx-\int_{0}^{L}u(x,0)\phi(x)=\int_{0}^{L}\int_{0}^{t}\partial_{xx}u(x,s)\phi(x)dx ds+\int_{0}^{L}\int_{0}^{t}\partial_{xx}f(x,s)\phi(x)W(dx,ds).
\]
Es importante destacar que hasta ahora lo que significa integrar con respecto a la fuente aleatoria $W$ es desconocido, pero una vez que la integral anterior haya cobrado sentido, definir con rigor lo que significa un problema como el planteado anteriormente y lo que significa resolverlo es similar a la idea de las soluciones débiles, aunque por supuesto que con mayor complejidad debido a la naturaleza estocástica del problema. Dichas soluciones se conocen en probabilidad con el nombre de \textit{soluciones mild}.

Echando mano de la intuición que la integral de Itô nos da al formalizar la idea de integrar una función con respecto a un movimiento browniano, nos resta estudiar una manera de dar rigor a la integral anterior, al menos suponiendo que la fuente de aleatoriedad $W$ de dos variables proviene de distribuciones gaussianas, las cuales son relativamente <<nobles>> en su manejo. 

\section{Integral de Itô-Walsh}
En esta sección presentamos la construcción de la integral de Itô-Walsh. Dicha integral nos permite construir procesos estocásticos que más adelante y en cierto sentido pueden verse como la solución de una ecuación diferencial parcial estocástica, y es la integral que resuelve el problema planteado anteriormente sobre cómo integrar con respecto a una fuente de aleatoriedad $W$. Para lograr dicho objetivo, presentamos primeramente la teoría necesaria para construir la integral.

\subsection{Variables gaussianas y Procesos gaussianos}
Recordamos aquí las propiedades fundamentales de los procesos gaussianos. Salvo algunos casos especiales, la mayoría de las propiedades se enuncian sin demostración, por lo que nos referiremos a \cite{gall2016brownian} para revisar las pruebas de las mismas. Comenzamos definiendo la distribución gaussiana.

\begin{dfn}
Sea $(\Omega, \F, \P)$ un espacio de probabilidad y consideremos a los reales dotados de la $\sigma-$álgebra de Borel, la cual denotamos por $\B(\R)$. Decimos que una variable aleatoria $X:(\Omega, \F)\to (\R,\B(\R))$ tiene distribución normal estándar (o bien, es una variable gaussiana estándar), si la función de distribución de la misma está dada por
\[
F_X(t)=\int_{-\infty}^{t}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}dx.    
\]
En otras palabras, $X$ tiene distribución normal estándar si tiene densidad (con respecto a la medida de Lebesgue en $\R$) dada por la función 
\[
f_X(t)=\frac{1}{\sqrt{2\pi}}e^{-t^2/2}.
\]
En tal caso, denotamos $X\sim$ Normal(0,1).
\end{dfn}

Directamente de la definición, tenemos la caracterización de una variable gaussiana en términos de su transformada de Laplace y de su transformada de Fourier.
\begin{prop}\label{definiciongaussiana}
 Sea $X$ una variable aleatoria con distribución normal estándar. Su transformada de Laplace en todo $\C$ está dada por 
 \[
 \mathcal{L}_X(\lambda):=\E\left[e^{\lambda X}\right]=e^{\lambda^2/2}, \qquad \lambda\in \C,
 \]
mientras que su transformada de Fourier (o función característica) en $\R$ está dada por 
\[
\mathcal{F}_X(\xi):=\E\left[e^{-i\xi X}\right]=e^{-\xi^2/2}, \qquad \xi \in \R. 
\]
 \end{prop}
Se puede ver usando alguna de las transformadas anteriores y usando un argumento inductivo que una variable normal estándar tiene momentos de cualquier orden, y que los mismos están dados por 
 \begin{itemize}
   \item $\E\left[X^{2n}\right]=(n-1)!!=\frac{(2n)!}{2^nn!}=1\cdot3\cdot5\cdot_...\cdot (2n-1), \qquad n\geq0$.
   \item $\E\left[X^{2n+1}\right]=0, \qquad n\geq0$.
 \end{itemize}
 En particular, se cumple que $\E\left[X^{n+1}\right]=n\cdot \E\left[X^{n-1}\right]$, para $n\geq1$.

Diremos también que una variable aleatoria gaussiana $Y$ tiene media $\mu\in \R$ y varianza $\sigma^2>0$ si se cumple que $Y=\sigma X+\mu$, donde $X$ es una variable normal estándar. Denotaremos la situación anterior escribiendo $Y\sim$ Normal$(\mu,\sigma^2)$. En vista de la proposición \ref{definiciongaussiana} y de la definición, se tiene el siguiente resultado.
\begin{prop} 
 Sea $Y$ una variable aleatoria normal con media $\mu\in \R$ y varianza $\sigma^2>0$. Entonces las siguientes tres proposiciones son equivalentes a este hecho.
 \begin{itemize}
    \item $\mathcal{L}_Y(\lambda)=e^{\mu\lambda+\sigma^2\lambda^2/2}, \qquad \lambda \in \C,$
    \item $\F_X(\xi)=e^{i\mu\xi-\sigma^2\xi^2/2}, \qquad \xi \in \R$,
    \item La distribución de $Y$ tiene densidad con respecto a la medida de Lebesgue en $\R$ dada por 
    \[
        f_Y(y)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(y-\mu)/2\sigma^2}, \qquad t\in \R.    
    \]
 \end{itemize}
 \end{prop}
 Por convención, diremos que una variable $Y$ tiene distribución normal con media $\mu\in \R$ y varianza $\sigma^2=0$ si se cumple que $Y=\mu$, salvo un conjunto de medida de Lebesgue cero. Pasamos ahora a definir los vectores aleatorios gaussianos o vectores normales conjuntos. 
 \begin{dfn} 
  Sea $(\R^d, \langle\cdot,\cdot\rangle)$ el espacio $\R^{d}$ dotado de su producto interno usual. Decimos que $X:(\Omega,\F)\to (\R^{d},\B(\R^d))$ es un vector gaussiano, o un vector normal conjunto, si para cualquier $u\in \R^d$, se tiene que $\langle u,X\rangle$ es una variable aleatoria gaussiana (en $\R$).
  \end{dfn}
Presentamos un primer resultado sobre vectores aleatorios gaussianos.
\begin{prop} 
Sea $X$ un vector aleatorio gaussiano en $\R^{d}$. Entonces existe un vector $\mu_X\in\R^d$ y una forma cuadrática no negativa $q_X:\R^d\to \R$ tales que para cualquier $u\in \R^d$,
\begin{itemize}
    \item $\E\left[\langle u,X\rangle\right]=\langle u,\mu_X\rangle$
    \item $\text{Var}\left(\langle u,X\rangle\right)=q_X(u)$.
\end{itemize}
De hecho podemos caracterizar a los objetos anteriores, pues si tenemos una base ortonormal $(e_{j})_{j\geq1}$ de $\R^{d}$, entonces podemos escribir a $X$ de la siguiente manera:
\[
X=\sum_{j=1}^{d}\langle X,e_j\rangle e_j,  
\]
donde $X_j:=\langle X,e_j\rangle$, por definición, es una variable aleatoria gaussiana, para cualquier $j\geq1$. De lo anterior se sigue que, escribiendo a $u\in \R^d$ como $u=\sum_{j=1}^{d}u_je_j$, 
\begin{itemize}
    \item $\mu_X=\E\left[X_j\right]e_j$.
    \item $q_X(u)=\sum_{j=1}^{d}\sum_{i=1}^{d}u_iu_j \text{Cov}\left(X_i,X_j\right)$.
\end{itemize}
 \end{prop}

Se deduce de la proposición anterior la fórmula de la función característica de un vector gaussiano.
\begin{prop} 
 Sea $X$ un vector aleatorio gaussiano y $(e_j)_{j\geq1}$ una base ortonormal de $\R^{d}$. Entonces, al ser $\langle u,X\rangle\sim$ Normal$(\langle u,\mu_X\rangle,q_X(u))$, se tiene que %Si denotamos por $\E\left[X\right]:=\sum_{j=1}^{d}\E\left[X_j\right]e_j$, entonces 
 \[
 \E\left[e^{i\langle u,X\rangle}\right]=e^{i \langle u,\mu_X\rangle-\frac{1}{2}q_X(u)}, \qquad u\in \R^d.
 \]
 \end{prop}
Y en vista de las dos proposiciones anteriores, está la siguiente caracterización de variables aleatorias gaussianas independientes.
\begin{prop} 
 Sea $X$ un vector aleatorio gaussiano en $\R^{d}$, y sea $(e_j)_{j\geq1}$ una base ortonormal de $\R^{d}$. Entonces $X_1,...,X_d$ son variables aleatorias independientes si y solo sí la matriz de covarianzas $\Sigma=(\text{Cov}\left(X_i,X_j\right))_{1\leq i,j\leq d}$ es diagonal. Equivalentemente, $X_1,...,X_d$ son independientes si y solo si la forma cuadrática $q_X$ está en forma diagonal en la base $(e_j)_{j\geq1}$.
 \end{prop}

 Pasamos ahora a estudiar procesos estocásticos formados por variables aleatorias gaussianas.
\begin{dfn} 
 Sea $T$ un conjunto arbitrario y sea $G=(G(t))_{t\in T}$ una colección de variables aleatorias indexadas por $T$. Decimos que $G$ es un proceso gaussiano, o un campo aleatorio gaussiano, si para cualesquiera $t_1,...,t_k\in T$, se tiene que $X=(G(t_1),...,G(t_k))$ es un vector gaussiano, $k\geq1$. 
\end{dfn}

Equivalentemente, un proceso estocástico $(G(t))_{t\in T}$ es gaussiano si cualquier combinación lineal finita de variables $G(t)$, con $t\in T$, es gaussiana, lo cual de acuerdo a nuestra exposición, también equivale a decir que las distribuciones finito-dimensionales de un proceso gaussiano $(G(t))_{t\in T}$ están dictadas por medio de vectores aleatorios gaussianos.

A menudo es importante saber cuando un proceso gaussiano es independiente de otro proceso gaussiano. Para ello, recordamos la definición de $\sigma$-álgebra generada por un conjunto de variables.

\begin{dfn} 
 Sea $G$ una familia de variables aleatorias definidas en un espacio de probabilidad $(\Omega, \F, \P)$. La $\sigma$-álgebra generada por $G$ es la $\sigma$-álgebra más pequeña en $\Omega$ tal que todas las variables $X\in G$ son medibles con respecto a dicha $\sigma$-álgebra. A tal estructura la denotamos como $\sigma(G)$.  
 \end{dfn}
Enunciamos ahora un importante resultado con respecto a la independencia de dos conjuntos de variables aleatorias gaussianas. Este resultado nos dice que, en cierto sentido, dos conjuntos de variables aleatorias gaussianas son ortogonales si y solo si son independientes entre sí. 

\begin{prop}\label{Gaussi_indep} 
 Sean $G_1$, $G_2$ dos familias de variables aleatorias gaussianas centradas (i.e. con media cero). Denotemos por $H_1$ y $H_2$ al subespacio lineal cerrado de $L^{2}(\P)$ generado por dichas variables. Entonces dichos espacios están formados por variables aleatorias gaussianas centradas. Más aún, los siguientes hechos son equivalentes.
 \begin{itemize}
    \item Los subespacios $H_1$ y $H_2$ son ortogonales entre sí en $L^{2}(\P)$.
    \item Las $\sigma$-álgebras $\sigma(G_1)$ y $\sigma(G_2)$ son independientes.
 \end{itemize}
 \end{prop}
 Es importante recordar que esta última propiedad es muy particular del contexto gaussiano. Ahora bien, dado un proceso gaussiano $G$, tenemos dos funciones asociadas al mismo.
\begin{dfn} 
 Sea $G=(G(t))_{t\in T}$ un proceso gaussiano. Definimos las funciones de medias y covarianzas del proceso $G$ respectivamente como sigue.
 \begin{enumerate}
    \item $\mu(t):=\E\left[G(t)\right]$, \qquad para cualquier $t\in T$,
    \item $\Gamma(s,t):= \text{Cov}\left(G(s),G(t)\right)$, \qquad para cualesquiera $s,t\in T$.
 \end{enumerate}
 \end{dfn}
Dichas funciones con valores en $\R$ determinan la colección de distribuciones finito-dimensionales del proceso. 
De hecho, gracias a las proposiciones anteriores es claro que para cualesquiera subíndices $(t_1,...,t_k)\subseteq T$, la ley del vector gaussiano $X=(G(t_1),...,G(t_k))$ está determinada de manera única. 

Lo anterior pues $\mu_X:=(\E\left[G(t_1)\right],...,\E\left[G(t_k)\right])=(\mu(t_1),...,\mu(t_k))$ y $q_X$ la forma cuadrática asociada que está determinada por la matriz de covarianzas de $X$, está en términos de la función $\Gamma$ definida antes, a saber, $\Sigma_X=\left(\text{Cov}\left(G(t_i),G(t_j)\right)\right)_{1\leq i,j\le k}=\left(\Gamma(t_i,t_j)\right)_{1\le i,j\le k }$.

Es consecuencia de que la forma cuadrática $q_X$ para cualquier vector $X$ que componga una distribución finito-dimensional de $G$, sea no negativa definida y simétrica, el hecho de que la función de covarianzas $\Gamma$ sea no negativa definida. A saber, si $c:T\to \R$ es una función con soporte finito, entonces 
\[  
    \sum_{T\times T}^{}c(s)c(t)\Gamma(s,t)\geq0
\]
Lo valioso de conocer las funciones anteriores es que, al caracterizar completamente las distribuciones finito-dimensionales del proceso gaussiano $G$, todo el proceso está caracterizado por medio de dichas funciones, por lo que basta conocerlas para reconstruir el proceso gaussiano. Éste es el contenido del siguiente teorema.

\begin{prop}\label{caractprocgauss}
 Sean $\Gamma:T\times T\to \R$ y $\mu:T\to \R$ funciones tales que $\Gamma$ es simétrica y no negativa definida. Entonces existe un proceso gaussiano $(G(t))_{t\in T}$ en un espacio de probabilidad $(\Omega, \F,\P)$ apropiado, tal que para cualesquiera $t\in T$, $G(t)\sim$ Normal$(\mu(t),\Gamma(t,t))$, con $\mu$ y $\Gamma$ las respectivas funciones de medias y covarianzas del proceso $G$.
 \end{prop}
 La demostración de lo anterior es una consecuencia del Teorema de Consistencia (o extensión) de Kolmogorov (ver Teorema 6.3 en \cite{gall2016brownian} ). Para terminar esta sección, a continuación presentamos un par de ejemplos de procesos gaussianos construidos de esta forma.
\begin{ejem}[\textbf{El movimiento Browniano}]
Consideremos el caso en el que $T=[0,\infty)$, $\mu(t)=0$ para cualquier $t\geq0$ y $\Gamma(s,t)=s\wedge t=\min\{s,t\}$, para $s,t\geq0$. Claramente $\Gamma$ es simétrica, y además nótese que para cualesquiera $c:T\to\R$ función de soporte finito, se tiene que
\begin{align*}
\sum_{T\times T}^{}c(s)c(t)\Gamma(s,t)&=\sum_{s\in T}^{}\sum_{t\in T}c(s)c(t)(s\wedge t)\\
&=\sum_{s\in T}^{}\sum_{t\in T}c(s)c(t)\int_{0}^{\infty}\1_{[0,s]}(x)\1_{[0,t]}(x)dx\\
&=\int_{0}^{\infty}\sum_{s\in T}\sum_{t\in T}\1_{[0,s]}(x)c(s)\1_{[0,t]}(x)c(t)dx\\
&=\int_{0}^{\infty}\abs{\sum_{t\in T}\1_{[0,t]}(x)c(t)}^2dx\\
&\geq0,
\end{align*}
por lo que por la proposición \ref{caractprocgauss}, existe en un espacio de probabilidad adecuado un proceso gaussiano que denotaremos por $(B(t))_{t\geq0}$, con función de medias la constante 0 y función de covarianzas $\Gamma(s,t)=s\wedge t$. Dicho proceso es el \textit{movimiento Browniano estándar} en $\R$.
 \end{ejem}
El siguiente ejemplo es de suma importancia y es parte esencial en el resto del texto.

\begin{ejem}[\textbf{El ruido blanco en $\R^{d}$}] 
Sea $T=\B(\R^{d})$ el conjunto de los borelianos en $\R^d$.
Dado que estamos hablando de subconjuntos de $\R^{d}$, cambiaremos la notación al indicar con letras mayúsculas a los elementos de $T$.
Consideremos nuevamente la función de medias $\mu(A):=0$, para cualquier $A\in \B(\R^{d})$ y ahora consideremos la función $\Gamma(A,B):=\lambda^{d}(A\cap B)$, donde $\lambda^{d}:\B(\R^{d})\to [0,1]$ es la medida de Lebesgue en $\R^{d}$.

 Claramente $\Gamma$ es una función simétrica, y además, para $c:T\to\R$ función de soporte finito, se tiene que 
 \begin{align*}
    \sum_{T\times T}^{}c(A)c(B)\Gamma(A,B)&=\sum_{A\in T}^{}\sum_{B\in T}c(A)c(B)\lambda^{d}(A\cap B)\\
    &=\sum_{A\in T}^{}\sum_{B\in T}c(A)c(B)\int_{\R^d}\1_{A}(x)\1_{B}(x)\lambda^{d}(dx)\\
    &=\int_{\R^{d}}\sum_{A\in T}\sum_{B\in T}\1_{A}(x)c(A)\1_{B}(x)c(B)\lambda^{d}(dx)\\
    &=\int_{\R^{d}}\abs{\sum_{A\in T}\1_{A}(x)c(A)}^2\lambda^{d}(dx)\\
    &\geq0,
    \end{align*}
por lo que existe un proceso gaussiano que denotaremos por $(\dot{W}(A))_{A\in \B(\R^{d})}$ en un espacio de probabilidad adecuado, tal que su función de medias es $0$ y su función de covarianzas es $\Gamma(A,B)=\lambda^{d}(A\cap B)$. Dicho proceso estocástico es conocido como \textit{ruido blanco} en $\R^{d}$.

Observemos que, si $A\cap B=\varnothing$, entonces $\Gamma(A,B)=\text{Cov}\left(\W(A),\W(B)\right)=0$, por lo que al ser variables gaussianas, éstas son independientes según lo visto antes.

Se sigue que si $A,B\in \B(\R^d)$, entonces aprovechando que el proceso es gaussiano centrado, tenemos que
\begin{align*}
    &\text{Cov}\left(\W(A\cup B)-\W(A)-\W(B)+\W(A\cap B),\W(A\cup B)-\W(A)-\W(B)+\W(A\cap B)\right)\\
    &=\text{Cov}\left(\W(A\cup B),\W(A\cup B)\right)+\text{Cov}\left(\W(A),\W(A)\right)+\text{Cov}\left(\W(B),\W(B)\right)\\
    &\quad +\text{Cov}\left(\W(A\cap B),\W(A\cap B)\right)-2 \text{Cov}\left(\W(A\cup B),\W(A)\right)-2 \text{Cov}\left(\W(A\cup B), \W(B)\right)\\
    &\quad +2 \text{Cov}\left(\W(A\cup B), \W(A\cap B)\right)+2 \text{Cov}\left(\W(A),\W(B)\right)-2 \text{Cov}\left(\W(A),\W(A\cap B)\right)\\
    &\quad -2 \text{Cov}\left(\W(B),\W(A\cap B)\right)\\
    &=\lambda^{d}(A\cup B)+\lambda^{d}(A)+\lambda^{d}(B)+\lambda^{d}(A\cap B)-2\lambda^{d}((A\cup B)\cap A)-2\lambda^{d}((A\cup B)\cap B)\\
    &\quad+2\lambda^{d}((A\cup B)\cap (A\cap B))+2\lambda^{d}(A\cap B)-2\lambda^{d}(A\cap (A\cap B))-2\lambda(B\cap(A\cap B))\\
    &=\lambda^{d}(A\cup B)+\lambda^d(A)+\lambda^{d}(B)+\lambda^{d}(A\cap B)-2\lambda^{d}(A)-2\lambda^{d}(B)+2\lambda^{d}(A\cap B)\\
    &\quad +2\lambda^{d}(A\cap B)-2\lambda^{d}(A\cap B)-2\lambda^{d}(A\cap B)\\
    &=\lambda^{d}(A\cup B)-\lambda^d(A)-\lambda^{d}(B)+\lambda^{d}(A\cap B)\\
    &=\lambda^{d}(A\cup B)-\lambda^{d}(A\cup B)\\
    &=0.
\end{align*}
por lo que la variable $\W(A\cup B)-(\W(A)+\W(B)-\W(A\cap B))$ tiene varianza cero. Luego,
\begin{equation}\label{Wdotmeasure}    
    \W(A\cup B)=\W(A)+\W(B)-\W(A\cap B), \qquad \P-\text{casi seguramente}.
\end{equation}


\end{ejem}
Lo anterior nos podría hacer pensar que el ruido blanco es una medida signada. Sin embargo, esto es falso (ver ejemplo 3.16 de \cite{Khoshnevisan2009}). No obstante, podemos utilizar dicho proceso como base para construir una integral.

\begin{ejem}[\textbf{El proceso isonormal}]\label{ruidoprocesoiso}
Dado un ruido blanco $\left(\W(A)\right)_{A\in \B(\R^d)}$, nos gustaría definir $\W(h)$ para alguna función $h$ adecuada. Es bien conocido en teoría de la medida un mecanismo estándar para definir nuevos objetos a partir de objetos más sencillos, y luego extender por aproximación. Siguiendo esta maquinaria estándar, para un conjunto $A\in \B(\R^d)$ definimos $\W(\1_A):=\W(A)$, y para cualesquiera $A_1,...,A_n\in \B(\R^{d})$ disjuntos, y constantes $c_1,...,c_n\in \R$, definimos
\[
\W \left(\sum_{k=1}^{n}c_k\1_{A_k}\right):=\sum_{k=1}^{n}c_k\W(A_k).    
\]
Nótese que si existen dos representaciones distintas de una función simple, de acuerdo a \eqref{Wdotmeasure}, la variable de la definición anterior es consistente ya que habrá una igualdad $\P-$ casi seguramente. Por otro lado, dado que los conjuntos $A_1,...,A_n$ son disjuntos, las variables $\W(A_1),...,\W(A_n)$ son independientes entre sí. Luego, al ser centradas, observamos que
\begin{align*}
    \norm{\W \left(\sum_{k=1}^{n}c_k\1_{A_k}\right)}_{L^{2}(\P)}^2&=\text{Var}\left(\sum_{k=1}^{n}c_k\W(A_k)\right)\\
    &=\sum_{k=1}^{n}c_k^2 \E\left[\W^2(A_k)\right]\\
    &=\sum_{k=1}^{n}c_k^2 \Gamma(A_k,A_k)\\
    &=\sum_{k=1}^{n}c_k^2\lambda^{d}(A_k)\\
    &=\int_{\R^d}\sum_{k=1}^{n}c_k^2\1_{A_k}(x)\lambda^d(dx)\\
    &=\norm{\sum_{k=1}^{n}c_k\1_{A_k}(x)}^2_{L^{2}(\R^{d})},
\end{align*}
donde en la última igualdad utilizamos que $A_i\cap A_j=\varnothing$ y por lo tanto $\1_{A_i}\1_{A_j}=0$ para $i\neq j$. De lo anterior deducimos que la aplicación $f\longmapsto \W(f)$, que envía funciones simples en variables gaussianas, preserva la norma de los respectivos espacios.

Es también un resultado conocido en teoría de la medida que, si tenemos ahora una función $f\in L^{2}(\R^{d})$, existe una sucesión de variables aleatorias simples en $L^{2}(\R^d)$ tales que $f_n\to f$ en la norma de dicho espacio. Luego, la sucesión $(f_n)_{n\ge 1}$ forma una sucesión de Cauchy de funciones simples en $L^2(\R^{d})$. Dado que la norma se preserva para estas funciones entre los espacios $L^2$ según lo visto antes, la sucesión de variables $(\W(f_n))_{n\ge 1}$ es de Cauchy en $L^{2}(\P)$, por lo que al ser un espacio completo, existe una única variable en $L^{2}(\P)$, que denotaremos por $\W(f)$, tal que $\W(f_n)\longrightarrow \W(f)$ en norma $L^{2}(\P)$.

Consideramos así al espacio $L^{2}(\R^{d})$, y a la colección de variables aleatorias $(\W(f))_{f\in L^{2}(\R^{d})}$. A dicho proceso estocástico se le conoce como el \textit{proceso isonormal}. Una característica fundamental de dicho proceso es que la aplicación $A\mapsto\W(A)$ es una isometría, conocida como \textit{isometría de Wiener}. Al elemento $\W(f)$ para $f\in L^2(\R^{d})$ se le conoce como \textit{integral de Wiener de f}, y a menudo se denota como
\[
\W(f)=\int f \ W(dx).    
\] 
El proceso isonormal se puede también obtener vía su función de medias y su función de covarianzas. En efecto, tomando $T=L^{2}(\R^{d})$, haciendo nuevamente $\mu(f)=0$ para cualquier $f\in L^2(\R)$, y definiendo la función de covarianzas $\Gamma$ como
\[
\Gamma(f,g)=\int_{\R^d}fg \ \lambda^d(dx)=\langle f,g\rangle_{L^{2}(\R^{d})}, \qquad f,g\in L^2(\R^{d}),
\]
entonces el proceso gaussiano resultante, que denotamos por $(\W(f))_{f\in L^{2}(\R^{d})}$ es el proceso isonormal que construimos antes vía un ruido blanco. 
 \end{ejem}

 Como veremos más adelante, los procesos isonormales juegan un papel importante en el Cálculo de Malliavin.
\subsection{Medidas martingala}

En esta parte del texto, estudiamos el concepto de medidas martingala. Dichos procesos estocásticos son una pierda angular en la construcción de la integral de Itô-Walsh, como veremos a continuación.

Recordemos que si tenemos un ruido blanco $(W(A))_{A\in \B(R^{d})}$, entonces para cualesquiera $A,B \in \B(\R^{d})$, se tiene la siguiente igualdad
\[
\W(A\cup B)=\W(A)+\W(B)-\W(A\cap B), \qquad \P-\text{casi seguramente.}   
\]
Sin embargo, como también se mencionó en la subsección pasada, a pesar de que esto nos puede hacer pensar en que el ruido blanco constituye una medida aleatoria signada, lo anterior es falso. No obstante, se tiene el siguiente resultado con respecto al mismo.
\begin{teo}\label{Finito_aditiv_ruido_blanco}
 El proceso $(\W(A))_{A\in \R^{d}}$ cumple las siguientes propiedades
 \begin{itemize}
    \item $\W(\varnothing)=0$, \qquad $\P-\text{casi seguramente,}$
    \item Para cualquier sucesión de conjuntos ajenos $(A_n)_{n\geq 1}\subseteq \B(\R^d)$, se cumple que 
    \[
    \W \left(\bigcup_{k=1}^\infty A_k\right)=\sum_{k=1}^{\infty}\W(A_k), \qquad \P-\text{casi seguramente,} 
    \]
 \end{itemize}
 en donde la suma infinita anterior converge en $L^2(\P)$. En otras palabras, el 
 ruido blanco es una medida signada, $\sigma$-finita, pero con valores en $L^2(\P)$.
\end{teo}
\begin{proof} 
   Es un resultado conocido de teoría de la medida que, si queremos probar que la aplicación anterior es una medida signada, y ya hemos probado que es finito-aditiva (igualdad \eqref{Wdotmeasure}), entonces basta con tomarnos una sucesión decreciente de conjuntos $(A_n)_{n\geq1}\subseteq\B(\R^{d})$ tal que al intersecar todos los elementos obtengamos el conjunto vacío, y probar que $\W(A_n)\to 0$ en $L^{2}(\P)$ para concluir.

   En efecto, sea $(A_n)_{n\geq1}$ una sucesión como la descrita antes. Directamente por definición del ruido blanco, 
   \[
   \norm{\W(A_n)}_{L^2(\P)}^2=\E\left[\W^2(A_n)\right]=\Gamma(A_n,A_n)=\lambda^{d}(A_n)\xrightarrow[n\to\infty]{}0,    
   \]
   ya que al ser $\lambda^d$ una medida, se cumple la propiedad de continuidad con la sucesión $(A_n)_{n\geq1}$.

   Para concluir que la medida es $\sigma-$finita, dado que $\R^{d}$ puede ser cubierto con una sucesión numerable de conjuntos compactos, basta ver que para cualquier conjunto compacto $K\subseteq\R^{d}$, se tiene que $\norm{\W(K)}_{L^ {2}(\R^{d})}^2<\infty$. Pero esto es claro, ya que 
   \[
       \norm{\W(K)}_{L^ {2}(\R^{d})}^2=\E\left[\W^2(K)\right]=\lambda^{d}(K)<\infty,
   \]
   ya que los compactos son acotados en $\R^{d}$.
 \end{proof}  
 \begin{obs}
   Es importante distinguir el que el ruido blanco forme una medida con valores en $L^{2}(\P)$ a que forme una medida signada aleatoria en los borelianos de $\R^d$. Esto es, nótese que la segunda propiedad de la proposición posee el cuantificador fuera del conjunto de probabilidad 1. Más específicamente, la proposición anterior \textit{no} está asegurando que 
   \[
   \P\left(\left\{\forall \ (A_n)_{n\geq1}\subseteq\B(\R^{d}), \ \ \W \left(\bigcup_{k=1}^\infty A_k\right)=\sum_{k=1}^{\infty}\W(A_k)\right\}\right)=1.
   \]
\end{obs}
Ahora, a menudo es posible considerar una dimensión como la dimensión temporal, de forma que se puede estudiar un ruido blanco definido en el espacio $[0,\infty)\times\R^{d}$. El hecho de pensar al intervalo $[0,\infty)$ como una dimensión temporal nos permite definir un nuevo proceso conocido como el \textit{proceso de ruido blanco}, $(W_t)_{t\geq0}$, definido como 
\[
W_t(A):=\W \left([0,t]\times A\right), \qquad A\in \B(\R^{d}).    
\]
Este es un proceso estocástico que <<evoluciona en el tiempo>>, pero a su vez es un ruido blanco en $A$. Recordemos también que la noción de filtración como sucesión de $\sigma$-álgebras ordenadas con respecto a la contención concreta la idea de proceso que evoluciona en el tiempo. Definimos entonces la filtración del proceso de ruido blanco como sigue

\begin{equation}\label{filtracionruidoblanco}
   \F_t:=\sigma \left(\W([0,t]\times A):0\leq s\leq t, A\in \B(\R^{d})\right), \qquad t\geq0.    
\end{equation}
Claramente la familia $(\F_t)_{t\geq0}$ forma una filtración, y además el proceso $(W_t)_{t\geq0}$ es adaptado a dicha filtración.

Una propiedad muy interesante que posee el proceso $(W_t)_{t\geq0}$, visto como un proceso que evoluciona en el tiempo, es que es una medida aleatoria signada, con valores en $L^2(\P)$, y también una martingala. De manera precisa, se tiene el siguiente resultado.

\begin{teo} 
Sea $(W_t(A))_{t\geq0, \ A\in \B(\R)}$ un proceso de ruido blanco en $[0,\infty)\times \R^{d}$. Entonces dicho proceso cumple las siguientes propiedades.
\begin{itemize}
   \item Para cualquier $A\in \B(\R^{d})$, se cumple la igualdad $W_0(A)=0, \ \P-$casi seguramente.
   \item Si $t>0$, entonces la aplicación $A\mapsto W_t(A)$ es una medida signada, $\sigma$-finita, con valores en $L^{2}(\P)$.
   \item Para cualquier $A\in \R^{d}$, el proceso $(W_t(A))_{t\geq0}$ es una martingala de media cero.
\end{itemize}
\end{teo}
\begin{proof} 
 Nótese que por definición, $W_0(A)=\W(\{0\}\times A)$, y dicha variable es tal que 
 \[
 \E\left[\W^2(\{0\}\times A)\right]=\Gamma(0\times A,0\times A)=\lambda{0}\cdot\lambda^{d}(A)=0,  
 \]
 por lo que dicha variable aleatoria es la variable 0 con probabilidad 1.

 Por otro lado, para $t>0$, el que la aplicación $W_t:\B(\R^{d})\to L^{2}(\P)$ sea una medida signada $\sigma$-finita se sigue de la propiedad siguiente: para cualquier $A\in \B(\R^{d})$, 
 \[
 \E\left[\W([0,t]\times A)\right]=\Gamma([0,t]\times A, [0,t]\times A)=\lambda([0,t])\cdot\lambda^{d}(A)=t\lambda^{d}(A). 
 \] 
 Por lo tanto, si $A=\varnothing$, claramente $\W_t(\varnothing)=0$ con probabilidad 1, ya que $\Gamma([0,t]\times\varnothing,[0,t]\times \varnothing)=0$. Ahora, para mostrar la $\sigma$-aditividad y la $\sigma$-finitud se procede de manera análoga a lo hecho en la proposición \ref{Finito_aditiv_ruido_blanco}: primero, se prueba que la aplicación $W_t:\B(\R)\to L^{2}(\P)$ es finito aditiva, y posteriormente se generaliza. Para la parte de la $\sigma$-finitud se procede igual.

 Finalmente, para mostrar que para cualquier $A\in \B(\R^{d})$ el proceso $(W_t(A))_{t\geq0}$ es una martingala con respecto a la filtración $(\F_t)_{t\geq0}$, observamos que el proceso claramente es integrable, pues lo componen variables gaussianas y es adaptado a la filtración por definición. Resta probar que para $A\in \B(\R)$ fijo,
 \[
 \E\left[W_{s+t}(A)\lvert \F_t\right]=W_t(A), \qquad s,t\geq0  
 \]
 Para ello haremos uso de la proposición \ref{Gaussi_indep}. Consideramos a la familia de variables aleatorias $C_1=\left\{W_u(A):u\leq t\right\}$ y
 $C_2=\left\{W_{t+s}-W_t(A):s>0 \right\}$, para $t\geq0$. Notemos que para $0\le u\le t$,
 \begin{align*}
   \E\left[(W_{t+s}(A)-W_t(A))W_u(A)\right]&=\Gamma([0,t+s]\times A, [0,u]\times A)-\Gamma([0,t]\times A,[0,u]\times A)\\
   &=\lambda([0,t+s]\cap[0,u])\cdot\lambda^{d}(A)-\lambda([0,t]\cap[0,u])\cdot\lambda^{d}(A)\\
   &=\lambda([0,u])\cdot\lambda^{d}(A)-\lambda([0,u])\cdot\lambda^{d}(A)\\
   &=0.
 \end{align*}
 Por lo tanto, para cualesquiera dos variables $\xi_1$ y $\xi_2$ en $C_1$ y $C_2$ respectivamente, se tiene que $\langle \xi_1,\xi_2\rangle_{L^2(\P)}= \E\left[\xi_1\xi_2\right]=0$, esto es, son ortogonales. Por lo tanto, si $H_1$ y $H_2$ representan la cerradura del subespacio lineal generado por las familias $C_1$ y $C_2$ respectivamente, dichos subespacios son ortogonales.
 
 Por lo tanto, por la proposición mencionada antes, las $\sigma$-álgebras generadas por las familias $C_1$ y $C_2$ son independientes. En particular, para $s,t\geq0$, $\F_t\subseteq \sigma(C_1)$, por lo que al ser $W_{t+s}-W_t$ una variable $\sigma(C_2)$ medible, se sigue que 
 \[
     \E\left[W_{t+s}(A)|\F_t\right]=\E\left[W_{t+s}(A)-W_{t}(A)+W_t(A)|\F_t\right]=\E\left[W_{t+s}(A)-W_t(A)\right]+W_t(A)=W_t(A),
     \]
     por lo que la propiedad de martingala se satisface.  
\end{proof}

El proceso de ruido blanco es entonces un ejemplo de un proceso que funciona, en cierto sentido, tanto como una medida como una martingala. Este concepto se puede definir de manera rigurosa, como vemos a continuación.

\begin{dfn} 
Sea $(\F_t)_{t\geq0}$ una sucesión de $\sigma$-álgebras continuas por derecha. Un proceso $(M_t(A))_{t\geq0, A\in \B(\R^{d})}$ es una medida martingala con respecto a dicha filtración si 
\begin{itemize}
   \item $M_0(A)=0$, $\P$- casi seguramente.
   \item Si $t>0$, entonces $M_t$ es una medida signada, $\sigma$-finita, y con valores en $L^2(\P)$.
   \item Para cualquier $A\in \B(\R^{d})$, se tiene que $(M_t(A))_{t\geq0}$ es una martingala de media cero con respecto a la filtración $(\F_t)_{t\geq0}$.
\end{itemize}
\end{dfn}
Por construcción, el proceso del ruido blanco es un primer ejemplo de medida martingala. El motivo para la introducción de este concepto es que estos objetos forman una clase adecuada de procesos que pueden fungir como integradores. No obstante, no cualquier medida martingala resulta ser un integrador adecuado. A partir de este punto de la sección, buscaremos establecer las propiedades que una medida martingala debe poseer para poder fungir como un integrador. 

A continuación, definimos un concepto relacionado íntimamente con las medidas martingala, y que se puede interpretar como una generalización del proceso de covariación para dos martingalas en el caso unidimensional.

\begin{dfn} 
 Sea $M$ una medida martingala. El funcional de covarianza de $M$ se define como
 \[
 \overline{Q}_t(A,B):= \langle M_\cdot(A),M_\cdot (B)\rangle_t, \qquad t\geq 0, \ \ A, B \in \B(\R^{d}). 
 \]
 \end{dfn}
 Dado que para $M$ una medida martingala, si $A$ y $B \in \B(\R^{d})$ se tiene que $(M_t(A))_{t\geq0}$ y $(M_t(B))_{t\geq0}$ son martingalas, entonces el proceso de covariación de ambas martingalas evaluado en $t\geq0$ coincide con el funcional de covarianza evaluado en los borelianos $A$ y $B$, y en $t\geq0$.

 Gracias a lo anterior, se deducen directamente de las propiedades de la covariación entre dos martingalas las siguientes propiedades del funcional de covarianza de una medida martingala.

 \begin{teo} 
  Sea $M$ una medida martingala y denotemos por $\overline{Q}$ a su funcional de covarianza. Entonces $\overline{Q}$ cumple que
  \begin{itemize}
   \item $\overline{Q}_t(A,B)=\overline{Q}_t(B,A)$, \ $\P$- casi seguramente.
   \item Si $B\cap C=\varnothing$, entonces $\overline{Q}_t(A,B\cup C)=\overline{Q}_t(A,B)+\overline{Q}_t(A,C)$, \ $\P$- casi seguramente.
   \item $\abs{\overline{Q}_t(A,B)}^2\leq \overline{Q}_t(A,A)\overline{Q}_t(B,B)$, \ $\P$- casi seguramente, 
   \item $t\mapsto\overline{Q}_t(A,A)$ es $\P$- casi seguramente no decreciente.
  \end{itemize}
  \end{teo}

Con el concepto de funcional de covarianza, podemos definir una función aleatoria de conjuntos, que denotaremos $Q$, como sigue:
\[
   Q(A\times B\times(s,t]):=\overline{Q}_t(A,B)-\overline{Q}_s(A,B), \qquad A,B\in \B(\R^{d}), \ \ t\geq s\geq 0.
   \]
Extendemos dicha función a uniones ajenas de elementos (rectángulos)
$(A_i\times B_i\times(s_i,t_i])$ donde $A_i,B_i\in \B(\R^{d})$, $(s_i,t_i]\subseteq \R$  y $1\leq i \leq m$, como 
\[
Q \left(\bigcup_{i=1}^{n}(A_i\times B_i\times(s_i,t_i])\right):=\sum_{i=1}^{n}Q(A_i\times B_i\times (s_i,t_i]).
\]
  
Como es estándar en teoría de la medida, se puede demostrar que la definición de dicha función $Q$ no se ve alterada por un cambio en los representantes de los conjuntos. Siendo también un procedimiento rutinario, lo natural es pensar en extender la función aleatoria $Q$ a una medida en $\B(\R^{d})\times \B(\R^{d})\times \B([0,\infty))$, pero lamentablemente no es posible hacerlo para cualquier medida martingala. Es posible encontrar un contraejemplo en \cite[pp. 305 - 307]{Walsh_J.B_Introduction_to_SPDEs}. 

No obstante, sí es posible encontrar una clase de medidas martingala que sean útiles para nuestros propósitos. Para ello, estudiamos el concepto de \textit{worthiness} de una medida martingala. Dicho concepto es de carácter técnico, pero será fundamental en la construcción de la integral de Itô-Walsh al permitirnos extender la definición de $Q$ a conjuntos más generales.

Para ello, primero definimos lo que significa que una medida signada sea positiva definida en nuestro contexto.

\begin{dfn} 
 Sea $K$ una medida signada definida en la $\sigma$-álgebra $\B(\R^{d})\times\B(\R^{d})\times\B([0,\infty))$. Decimos que $K$ es positiva definida si para cualquier función $f$ integrable y acotada, se tiene que 
 \[
 \int_{\R^{d}\times\R^{d}\times [0,\infty)} f(x,s)f(y,s)K(dx, dy, dz) \geq 0.
 \]
 \end{dfn}
 Introducimos ahora el concepto de \textit{worthiness} de una medida martingala.
\begin{dfn} 
Sea $M$ una medida martingala. Decimos que $M$ es \textit{worthy} (traducido literalmente al español como \textit{digna}) si existe una medida $\sigma$-finita $K:\B(\R^{d})\times \B(\R^{d})\times \B([0,\infty))\times \Omega \to [0,1]$ tal que para cualesquiera $A,B\in \B(\R^{d})$, $C\in \B([0,\infty))$ y $\omega \in \Omega$, 
\begin{itemize}
   \item La aplicación $A\times B\mapsto K(A\times B \times C,\omega)$ es positiva definida y simétrica, para $C$ y $\omega$ fijos.
   \item $\{K(A\times B \times (0,t])\}_{t\geq0}$ es un proceso predecible, para $A$ y $B$ fijos.
   \item Para cualesquiera dos conjuntos compactos $A,B \in \B(\R^{d})$, y $t>0$, 
   \[
   \E\left[K(A\times B \times (0,t])\right]<\infty.
   \]
   \item Para $t>0$ y $A,B\in \B(\R^{d})$, 
   \[
   \abs{Q(A\times B \times (0,t])}\leq K(A\times B \times (0,t]), \qquad \P-\text{ casi seguramente.}   
   \]
\end{itemize}
\end{dfn}
Como es usual en probabilidad, la dependencia en $\omega$ es omitida en la notación. Cuando dicha medida $K$ existe, decimos que es una \textit{medida dominante} para $M$. Como convención, utilizaremos el adjetivo \textit{digna} para una medida martingala $M$ que cumpla la definición anterior.

Lo relevante para nosotros es que, si $M$ es una medida martingala digna y $Q_M$ es la función aleatoria de conjuntos construida a partir de ella, entonces $Q_M$ puede ser extendida como una medida signada a toda la $\sigma$-álgebra $\B(\R^{d})\times\B(\R^{d})\times\B([0,\infty))$. Más aún, $Q_M$ resulta ser positiva definida (ver \cite[p. 291]{Walsh_J.B_Introduction_to_SPDEs}).  


\subsection{Definición de la integral y algunas propiedades}
Hemos reunido suficiente teoría para definir la integral de Itô-Walsh con respecto a medidas martingala. Conforme vayamos desarrollando la construcción, una cierta familiaridad con la construcción de la integral de Lebesgue y la integral de Itô será notoria. 

\begin{dfn}
Sean $(\F_t)_{t\geq0}$ una filtración continua por derecha y $M$ una medida martingala asociada a dicha filtración. Una función $f:\R^{d}\times [0,\infty)\times \Omega\to \R$ se dice que es \textit{elemental} si es de la forma 
\begin{equation}\label{funcionelemental}
      f(x,t,\omega)=X(\omega)\1_{(a,b]}(t)\1_A(x),
\end{equation}
donde $X$ es una variable aleatoria acotada y $\F_a$-medible, y además $A\in \B(\R^{d})$. Definimos la integral de Itô-Walsh para la función elemental $f$ con respecto a la medida martingala $M$ como
\[
(f\cdot M)_t(B)(\omega):= X(\omega)\left[M_{t\wedge b}(A\cap B)-M_{t\wedge a}(A\cap B)\right](\omega).
\]
\end{dfn}
Observamos en la definición una similitud con la construcción de Itô de una función (o bien, puede decirse, de un proceso) elemental con respecto a una martingala adecuada, pues en tal contexto, la variable $X$ está multiplicada por un incremento de dicha martingala en el intervalo $(a,b]$. En el contexto de la integral de Itô-Walsh, la martingala encuentra su análogo en las medidas martingala $M$. Como una primera propiedad, tenemos la siguiente proposición.
\begin{teo} 
Sea $f$ una función elemental como en \eqref{funcionelemental} y $M$ una medida martingala. Entonces $(f\cdot M)$ es nuevamente una medida martingala. En particular, la integral de funciones elementales con respecto a martingalas se vuelve una forma de construir nuevas medidas martingala.
\end{teo}
\begin{proof} 
 Claramente, $(f\cdot M)_0(B)=0$ por definición de la integral, para cualquier $B\in \B(\R^{d})$. Por otro lado, $(f\cdot M)_t(\varnothing)=0$ usando que $M$ es medida martingala y por lo tanto es una medida signada cuando $t>0$ es fijo. Ahora bien, si tenemos una unión numerable de conjuntos ajenos, 
\begin{align*}
      (f\cdot M)_t\left(\bigcup_{k\geq1}B_k\right)&=X \left[M_{t\wedge b}\left(A\cap \bigcup_{k\geq1}B_k\right)-M_{t\wedge a}\left(A\cap \bigcup_{k\geq1}B_k\right)\right]\\
      &=X \left[\sum_{k\geq1}M_{t\wedge b}(A\cap B_k)-\sum_{k\geq1}M_{t\wedge a}(A\cap B_k)\right]\\
      &=\sum_{k\geq1}X \left[M_{t\wedge b}(A\cap B_k)-M_{t\wedge a}(A\cap B_k)\right]\\
      &=\sum_{k\geq1}(f\cdot M)_t(B_k),     
   \end{align*}
en donde hemos hecho uso nuevamente de que para $t>0$ fijo, $M$ se comporta como una medida signada. La $\sigma$-finitud de la integral se deduce de manera similar, y claramente se trata de una medida $L^2(\P)$-valuada.

Por último, fijando un conjunto $B\in \B(\R^{d})$, tenemos que para $t\in (a,b]$ (el único caso en que $f$ no se anula),
\begin{align*}
   \E\left[(f\cdot M)_{t+s}(B)|\F_{t}\right]&=\E\left[X\left[M_{(t+s)\wedge b}(A\cap B)-M_{(t+s)\wedge a}(A\cap B)\right]|\F_t\right]\\
   &=X\E\left[M_{(t+s)\wedge b}(A\cap B)-M_ a (A\cap B)|\F_t\right]\\
   &=X\left(\E\left[M_{(t+s)\wedge b}(A\cap B)|\F_t\right]-M_a(A\cap B)\right)\\
   &=X\left(M_t-M_a\right)\\
   &=X\left(M_{t\wedge b}-M_{t\wedge a}\right)\\
   &=(f\cdot M)_t(B),
\end{align*}
en donde hemos hecho uso de que $X$ es $\F_a$ medible y para $B$ fijo, $M$ se comporta como una martingala.
\end{proof}
\begin{ejem} 
Sea $\W$ un ruido blanco en $\R^{d}\times[0,\infty)$ y consideremos a $(W_t(A))_{t\geq0,A\in \B(\R^d)}$ el proceso de ruido blanco. Ya vimos que dicho proceso forma una medida martingala. Sea $f$ una función elemental. Entonces $(f\cdot W)$ es una medida martingala.
\end{ejem}

El siguiente paso en la construcción de la integral es extender la definición de la integral a funciones simples. 

\begin{dfn} 
   Decimos que una función $f$ es una función simple si existen $c_1,...,c_n$ constantes reales, y $f_1,...,f_n$ funciones elementales, tales que 
   \[
     f=\sum_{k=1}^{n}c_kf_k.  
   \] Denotaremos por $\mathscr{S}$ el conjunto de las funciones simples. Definimos la integral de Itô-Walsh para funciones simples como 
   \[
   (f\cdot M)_t(B):=\sum_{j=1}^{k}c_j(f_j\cdot M)_t(B).
   \]
 \end{dfn}
 \begin{prop} 
  La definición anterior es consistente, esto es, no depende de la representación de $f$ en términos de funciones elementales.
  \end{prop}
También es posible demostrar que si $f\in \mathscr{S}$, entonces $(f\cdot M)$ es nuevamente una medida martingala. 

Resulta ser que una clase adecuada de funciones integrables son las funciones $f$ que son predecibles. Esto es, funciones medibles con respecto a aquella $\sigma$-álgebra que es generada por todas las funciones simples. Dicha $\sigma$-álgebra la denotamos por $\mathscr{P}:=\sigma(\mathscr{S})$.

En este punto, deseamos extender la definición de la integral para funciones más generales, definidas sobre conjuntos no necesariamente rectangulares. Es en este punto en donde entra en juego la función $Q$.  Dicha función es crucial, según se ve en el siguiente 

\begin{teo}\label{rolmedidaq}
Sea $f\in \mathscr{S}$ y supongamos que $M$ es una medida martingala digna. Sea $Q_M$ la función aleatoria de conjuntos asociada a $M$. Entonces para $B\in \B(\R^{d})$,
\[
\E\left[(f\cdot M)_t^2(B)\right]=\E\left[\int_{B\times B\times (0,t]}f(x,t)f(y,t)Q_M(dx,dy,dt)\right].   
\]
\end{teo}

\begin{proof} 
   Primero hacemos la prueba para funciones elementales. Supongamos que $f$ es de la forma \eqref{funcionelemental}. Entonces 
   \begin{align*}
      \E\left[(f\cdot M)^2_t(B)\right]&=\E\left[X^2 \left(M_{t\wedge b}(A\cap B)-M_{t\wedge a}(A\cap B)\right)^2\right]\\
      &=\E\left[X^2M_{t\wedge b}^2(A\cap B)\right]-2 \E\left[X^2M_{t\wedge b}(A\cap B)M_{t\wedge a}(A\cap B)\right]\\
      &\ \ \ +\E\left[X^2M^2_{t\wedge a}(A\cap B)\right].
   \end{align*}
   Ahora bien, como $M$ es medida martingala, es una martingala para $A\cap B$. Por lo tanto, por definición del proceso de variación cuadrática, $M^2_t(A\cap B)-\langle M(A\cap B),M(A\cap B)\rangle_t$ es una martingala. Se sigue que 
   \[
   \E\left[M_{t\wedge b}^2(A\cap B)-\langle M(A\cap B),M(A\cap B)\rangle_{t\wedge b}\right]=\E\left[M_{t\wedge a}^2(A\cap B)-\langle M(A\cap B),M(A\cap B)\rangle_{t\wedge a}\right],
   \]
   y al ser $X$ una variable $\F_a$ medible, también se cumple 
      \begin{align*}
         &\E\left[X^2 \left(M_{t\wedge b}^2(A\cap B)-\langle M(A\cap B),M(A\cap B)\rangle_{t\wedge b}\right)\right]\\
         &=\E\left[X^2 \left(M_{t\wedge a}^2(A\cap B)-\langle M(A\cap B),M(A\cap B)\rangle_{t\wedge a}\right)\right].
      \end{align*}
   De manera similar, se tiene la igualdad 
   \begin{align*}
      &\E\left[X^2 \left(M_{t\wedge b}^2(A\cap B)M_{t\wedge a}(A\cap B)-\langle M(A\cap B),M(A\cap B)\rangle_{t\wedge b}\right)\right]\\
      &=\E\left[X^2 \left(M_{t\wedge a}^2(A\cap B)-\langle M(A\cap B),M(A\cap B)\rangle_{t\wedge a}\right)\right].
   \end{align*}
   Combinando estas expresiones, y recordando la definición de $Q_M$, obtenemos el resultado. La extensión al caso en que $f$ es simple es rutinaria.
 \end{proof}
La reelevancia del resultado anterior se debe nuevamente a la analogía que existe con el cálculo estocástico convencional. En tal contexto, la isometría de Itô nos dice que la esperanza del cuadrado de la integral estocástica equivale a la esperanza de la integral, con respecto al proceso de covariación, del cuadrado del proceso en cuestión. En nuestro contexto, integrar con respecto al proceso de covariación del proceso tiene su análogo en integrar con respecto a la función de conjuntos $Q$, la cual vimos que forma una medida cuando la medida martingala $M$ es digna, y está construida con base en los incrementos de los procesos de covariación cruzada de las martingalas formadas a partir de $M$.
\begin{ejem} 
Sea $\W$ un ruido blanco en $\R^{d}\times[0,\infty)$ y consideremos al proceso de ruido blanco asociado al mismo. Entonces dicho proceso forma una medida martingala digna con medida dominante $K(A\times B \times C):=\lambda^{d}(A\cap B)\lambda(C)$.
\end{ejem}

Ya hemos visto que la integral de Itô-Walsh es una medida martingala para funciones elementales y comentamos que lo mismo sucede para funciones simples. Más aún, resulta ser que si $M$ es una medida martingala digna, entonces la integral de Itô-Walsh de funciones simples también es una medida martingala digna. Este es el contenido del siguiente resultado.

\begin{teo}\label{propiedadesintegralwalsh}
Sea $M$ una medida martingala digna y $f$ una función simple. Entonces $(f\cdot M)$ es una medida martingala digna. 
Mas aún, si $Q_M$ y $K_M$ representan al funcional de covarianza y a la medida dominante de la medida martingala digna $M$, entonces 
\begin{align*}
   &Q_{f\cdot M}(dx,  dy,  dz)=f(x,t)f(y,t)Q_M(dx,  dy,  dt)\\
   &K_{f\cdot M}(dx, dy,  dt)=\abs{f(x,t)f(y,t)}K_M(dx,  dy,  dt). 
\end{align*}
\end{teo}
La prueba la omitimos, pero se puede consultar en \cite[proposición 5.23]{Khoshnevisan2009}.

A partir de ahora, solo estaremos interesados en el caso cuando la variable temporal está en algún intervalo finito $(0,T]$.

Hasta ahora hemos definido la integral de Itô-Walsh en el conjunto $\mathscr{S}$ de las funciones simples, y hemos probado propiedades básicas de la misma. Siguiendo la maquinaria estándar para definir nuevos objetos a partir de aproximaciones, el siguiente paso natural es extender la definición a todo un conjunto $\mathscr{P}$ de funciones predecibles, para alguna medida martingala $M$. Dotamos primero a dicho conjunto de una norma apropiada para dicho propósito.
\begin{dfn} 
Sea $M$ una medida martingala digna. Supongamos que $K_M$ es su medida dominante. Sea $f\in \mathscr{P}$ una función predecible. Definimos la norma $\norm{\cdot}_M$ en dicho espacio como sigue 
\[
   \norm{f}_M^2:=\E\left[\int_{\R^{d}\times\R^{d}\times (0,T]} \abs{f(x,t)f(y,t)}K_M(dx, dy, dt)\right].
\]
\end{dfn}
Observamos que dicha norma está bien definida pues $K_M$ es una medida signada definida positiva. Denotamos por $\mathscr{P}_M$ al conjunto de funciones predecibles $f$ tales que $\norm{f}_M<\infty$.

\begin{teo} 
La norma anteriormente definida en efecto es una norma, y el conjunto $\mathscr{P}_M$ es completo con esta norma. Por lo tanto, el espacio $(\mathscr{P}_M,\norm{.}_M)$ es un espacio de Banach.
\end{teo}
\begin{proof} 
  Suponer que la norma es idénticamente cero es equivalente a decir que 
  \[
   \E\left[\int_{\R^{d}\times\R^{d}\times (0,T]} \abs{f(x,t)f(y,t)}K_M(dx, dy, dt)\right]=0,
  \]
  lo cual sólo es posible cuando $\abs{f(x,t)f(y,t)}=0$, independientemente de $x,y$ y $t$. Esto se cumple sólo cuando $f$ es la función cero salvo conjuntos de medida $K_M$ igual a cero. El producto por escalar claramente se factoriza fuera de la norma por las propiedades del valor absoluto y de las integrales.
  
  Para la parte de la desigualdad del triángulo, dado que $\mathscr{P}_M$ es por definición el conjunto de todas las funciones medibles con respecto a una $\sigma$-álgebra, utilizamos el lema de Dynkin funcional: primero mostramos la propiedad para las funciones elementales, luego probamos la propiedad en $\mathscr{S}$ (las generadoras del espacio $\mathscr{P}_M$ ) y posteriormente extendemos con Dynkin. El argumento completo se omite en el texto.

  Finalmente, para probar que el espacio $(\mathscr{P}_M,\norm{.}_M)$ es completo, usamos \cite[proposición 2.2]{Walsh_J.B_Introduction_to_SPDEs} y concluimos.
 \end{proof}

Ya que el espacio $(\mathscr{P}_M,\norm{.}_M)$ tiene una estructura adecuada, construimos un <<puente>> para extender la integral de $\mathscr{S}$ a todo $\mathscr{P}_M$. Ese es el objetivo de la siguiente proposición. 

\begin{teo} 
El conjunto $\mathscr{S}$ es denso en $\mathscr{P}_M$.
\end{teo}
Para una prueba de este hecho remitimos a \cite[proposición 2.3]{Walsh_J.B_Introduction_to_SPDEs}. Lo anterior junto con la proposición \ref{rolmedidaq} y las propiedades de la medida dominante $K_M$ nos dice que para rectángulos,
\begin{equation}\label{integralcotacauchy}
   \E\left[(f\cdot M)_t^{2}(B)\right]\leq \norm{f}_M^2,\qquad \text{ para } \ t\in (0,T], \ \ \ f\in \mathscr{S}, \ \ B\in \B(\R^d).
\end{equation}
Armados con la cota anterior, procedemos a extender la integral a un función $f\in \mathscr{P}_M$ de la siguiente forma. Dado que $\mathscr{S}\subseteq \mathscr{P}_M$ es un conjunto denso, para $f\in \mathscr{P}_M$ existe una sucesión de funciones $(f_n)_{n\geq1}\subseteq \mathscr{S}$ tales que $\norm{f_n-f}_{M}\xrightarrow[n\to\infty]{}0$. Esto hace a $(f_n)_{n\geq0}$ una sucesión de Cauchy. Luego, gracias a la cota \eqref{integralcotacauchy}, la sucesión $\left((f_m\cdot M)_t(B)\right)_{m\geq1}$ también es de Cauchy en $L^{2}(\P)$. Obsérvese que los elementos de esta última sucesión están bien definidos gracias a que la integral de Itô-Walsh está bien definida en $\mathscr{S}$. Dado que $L^{2}(\P)$ es completo, tenemos la existencia de una martingala, que denotaremos por $(f\cdot M)_t(B)$, de tal forma que 
\[
f_m\xrightarrow[m\to\infty]{\norm{\cdot}_M}f \quad \ent \quad (f_m\cdot M)_t(B) \xrightarrow[m\to\infty]{L^{2}(\P)} (f\cdot M)(B),  
\]
y dicho objeto no depende de la sucesión $(f_n)_{n\geq1}$ que aproxima a $f\in \mathscr{P}_M$. Dicho objeto es, por definición, la integral de Itô-Walsh para una función $f$ predecible. Todo lo anterior nos lleva al siguiente resultado.

\begin{teo}\label{integralwalshgeneral}
 Sea $M$ una medida martingala digna. Entonces para cualquier $f\in \mathscr{P}_M$, el objeto $(f\cdot M)$ es una medida martingala digna que satisface las propiedades de la proposición \ref{propiedadesintegralwalsh}. Más aún, para cualquier $t\in (0,T]$ y $A,B\in \B(\R^{d})$, 
 \[
 \left\langle (f\cdot M)(A),(f\cdot M)(B)\right\rangle _t= \int_{A\times B\times (0,t]}f(x,s)f(y,s)Q_M(dx, dy,  ds),
 \]
 y además se tiene la siguiente cota en $L^{2}(\P)$.
 \[
 \E\left[(f\cdot M)^{2}_t(B)\right] \leq \norm{f}_M^{2}.
 \]
 \end{teo}
 La prueba del teorema anterior se puede consultar en \cite[teorema 2.5]{Walsh_J.B_Introduction_to_SPDEs}. A partir de aquí, adoptamos la siguiente notación para la integral de Itô-Walsh, la cual será usada principalmente en el capítulo 4:
 \begin{equation*}
   \int_{A\times [0,t]} f(x,s) M(dx,ds):=(f\cdot M)_t(A), \ \ \text{ para } \ f\in \mathscr{P}_M, \ A\in \B(\R^{d}) \ \text{ y } \ t>0.
 \end{equation*}
  Es posible extender la cota en $L^2(\P)$ a una cota en $L^{p}(\P)$, creando así una desigualdad del tipo Burkholder.
 \begin{teo}[\textbf{Desiguadades de Burkholder}]
  Sea $M$ una medida martingala digna. Entonces para cualquier $p\geq2$ existe una constante $c_p\in (0,\infty)$ tal que para cualquier $f$ predecible y cualquier $t>0$, 
  \[
  \E\left[\abs{(f\cdot M)_t(B)}^p\right]\leq c_p \E\left[\left(\int_{\R^{d}\times \R^{d}\times (0,T]}\abs{f(x,t)f(y,t)}K_M(dx, dy,  dt)\right)^{p/2}\right] 
  \]
  \end{teo}
  Para una prueba de un caso especial, se puede consultar \cite[teorema 5.27]{Khoshnevisan2009}. 

  A menudo será de utilidad intercambiar el orden de integración cuando una integral de Lebesgue <<usual>> y una integral de Itô-Walsh conviven. Este es el contenido del siguiente resultado.
  \begin{teo}[\textbf{Fubini estocástico}]\label{fubiniestocastico}
   Sea $(G, \mathcal{G},\mu)$ un espacio de medida finita y sea $M$ una medida martingala con medida dominante $K_M$. Sea $f(x,s,\omega, \lambda)$ una función $\mathscr{P}_M\times\mathcal{G}$ medible, donde $x\in \R^{d}$, $s\geq0$, $\omega \in \Omega$ y $\lambda \in G$. Supongamos que se cumple la condición
   \[
   \E\left[\int_{\R^{d}\times \R^{d}\times [0,T] \times G} \abs{f(x,s,\omega,\lambda)f(y,s,\omega,\lambda)}K_M(dx, dy, ds)\mu(d\lambda)\right]<\infty.
   \]
   Entonces 
   \[
   \int_G \left[\int_{\R^{d}\times [0,t]}f(x,s,\lambda)M(dx,ds)\right]\mu(d\lambda)= \int_{\R^{d}\times[0,t]}\left[\int_Gf(x,s,\lambda)\mu(d\lambda)\right] M(dx,ds).
   \]
   \end{teo}
   La prueba de este resultado se puede hallar en \cite[teorema 2.6]{Walsh_J.B_Introduction_to_SPDEs}.

  Con esto terminamos la construcción de la integral de Itô-Walsh con respecto a una medida martingala. Para finalizar esta sección, presentamos como ejemplo a la integral de Itô-Walsh con respecto a un ruido blanco.
  \begin{ejem} 
   Sea $\dot W$ un ruido blanco en $\R^{d}\times [0,\infty)$. Sea $(W_t(A))_{t\geq0, \ A\in \B(\R^{d})}$ el proceso de ruido blanco asociado al mismo. Ya hemos visto que dicho proceso forma una medida martingala digna con respecto a la filtración \eqref{filtracionruidoblanco}. 

   Entonces de acuerdo al teorema \ref{integralwalshgeneral}, para una función $f$ predecible con respecto a la filtración generada por el proceso de ruido blanco, existe una nueva medida martingala digna en $L^2(\P)$, denotada 
   \[
   (f\cdot W)_t(A)=\int_{A\times [0,t]}f(x,s)W(dx,ds), \qquad \text{ para } A\in \B(\R^{d}), \ t\geq0.
   \]
   Dicho objeto es la integral de Itô-Walsh de $f$ con respecto a un ruido blanco $\dot W$.
   \end{ejem}
  
   Ahora que ya nos es posible hablar de integrales con respecto a un ruido blanco $\W$, es posible estudiar de manera rigurosa lo que significa resolver una  ecuación diferencial parcial estocástica cuya componente aleatoria está dada por un ruido blanco.

\section{Formulación mild de una SPDE. Existencia y unicidad de las soluciones}
Como vimos en la introducción del capítulo, la heurística para reformular un problema de ecuaciones diferenciales parciales estocásticas de tal forma que se logre <<evadir>> el uso de las derivadas convencionales, pero que sea equivalente al problema original, sigue de cerca las ideas de las soluciones débiles en ecuaciones diferenciales parciales clásicas. Concretamente, el uso de integración por partes para lograr el objetivo es clave. 

Para ejemplificar lo anterior, presentamos un ejemplo de reformulación a una ecuación diferencial parcial estocástica conocida: la ecuación estocástica del calor en $[0,L]\times[0,\infty)$. Este ejemplo se puede consultar con más detalle en \cite[pp. 23 - 32]{Khoshnevisan2009}. Consideremos el siguiente problema del calor,
\begin{equation}\label{calorejemplo}
   \begin{cases}
      \partial_tu(x,t)=\partial_{xx}u(x,t)+f(u(x,t))\W(x,t), & \text{ si } t>0, x\in [0,L]\\
      \partial_xu(0,t)=\partial_xu(L,t)=0, & \text{ si } t>0,\\
      u(x,0)=u_0(x), & \text{ si } x\in [0,L],\\
   \end{cases}
\end{equation}

donde $\W$ es un ruido blanco con respecto a alguna filtración $(\F_t)_{t\geq0}$ continua por derecha, y $u_0:[0,L]\to\R$ es una función determinista, medible y acotada, mientras que $f:\R\to\R$ es una función globalmente Lipschitz y acotada.

Si formalmente multiplicamos la ecuación diferencial por una función $\phi \in C^{\infty}(\R)$ tal que $\phi'(0)=\phi'(L)=0$ e integramos con respecto al tiempo y posteriormente con respecto al espacio, obtendremos que el problema anterior se puede reformular como 
\begin{align*}
   \int_{0}^{L}u(x,t)\phi(x)dx-\int_{0}^{L}u_0(x)\phi(x)dx&=\int_0^{t}\int_0^{L}\partial_{xx}u(x,s)\phi(x)dxds\\
   &+\int_{0}^{t}\int_{0}^{L}f(u(x,s))\phi(x)W(dx, ds).    
   \end{align*}
Es destacable que ahora tenemos una ecuación integral estocástica en donde la integral con respecto al ruido blanco cobra total sentido. No obstante, lo anterior aún no puede ser tratado de manera rigurosa, ya que al ser $u$ un objeto aleatorio no necesariamente será derivable. Por lo tanto, si de manera formal utilizamos integración por partes, al ser $\phi$ una función cuya derivada se anula en las fronteras, la integral de la parcial de $u$ multiplicado por $\phi$ se transforma en
\[
   \int_{0}^{t}\int_{0}^{L}\partial_{xx}u(x,s)\phi(x)dx ds=\int_{0}^{t}\int_{0}^{L}u(x,s)\phi''(x)dx ds.
\]
Sustituyendo la expresión anterior en la reformulación de nuestro problema, obtenemos una ecuación integral estocástica. Tal ecuación integral tiene completo sentido, y dado que es una reformulación de nuestro problema inicial pero que puede tratarse de manera rigurosa, resolver dicha reformulación integral se puede considerar como resolver el problema original. Estas maneras de resolver una ecuación diferencial parcial estocástica varían, dependiendo de la naturaleza del problema a tratar. Las soluciones a estas ecuaciones halladas con la reformulación del problema en términos de ecuaciones integrales se conocen como \textit{soluciones mild}.

En el ejemplo que presentamos, la solución mild al problema del calor \eqref{calorejemplo} se establece con rigor en la siguiente definición.

\begin{dfn} 
Decimos que $u$ es una solución \textit{mild} del problema inicial anterior, si existe un proceso estocástico (también llamado \textit{campo aleatorio}) $u(x,t)$ medible y adaptado a la filtración generada por $W$ tal que para cualquier función $\phi\in C^{\infty}([0,L])$, donde $\phi'(0)=\phi'(L)=0$, se tiene la igualdad 
\begin{align*}
   \int_{0}^{L}u(x,t)\phi(x)dx-\int_{0}^{L}u_0(x)\phi(x)dx&=\int_{0}^{t}\int_{0}^{L}u(x,s)\phi''(x)dx ds\\
   & \ \ \ +\int_{0}^{t}\int_{0}^{L}f(u(x,s))\phi(x)W(dx, ds).  
\end{align*}
\end{dfn}

Es conocida teoría que resuelve el problema \eqref{calorejemplo}. Presentamos aquí el teorema correspondiente.

\begin{teo} 
El problema del calor \eqref{calorejemplo}, con las condiciones especificadas anteriormente sobre $\dot W$, $f$ y $u_0$, tiene una única solución $\P$- casi seguramente, que satisface lo siguiente para cualquier $T>0$.
\[
\sup_{0\leq x\leq L}\sup_{0\leq t \leq T}\E\left[\abs{u(x,t)}^2\right]<\infty.   
\]
\end{teo}
% \begin{teo}[\textbf{Lema de Gronwall}] 
% Supongamos $\phi_1,\phi_2,...:[0,T]\to[0,+\infty)$ son medibles y no decrecientes. Supongamos también que existe una constante $A\in \R$ tal que para cualquier entero $n\geq1$ y para cualquier $t\in[0,T]$,
% \[
% \phi_{n+1}(t)\leq A\int_{0}^{t}\phi_n(s)ds.   
% \]
% Entonces 
% \[
% \phi_n(t)\leq \phi_1(T)\frac{(At)^{n-1}}{(n-1)!}, \qquad \text{ para cualquier }n\geq1, \text{ y } t\in[0,T].   
% \]
% \end{teo}
Mas aún, el siguiente teorema nos muestra que no solamente hay existencia y unicidad de las solución al problema \eqref{calorejemplo}, sino que también existe una modificación continua de la misma.

\begin{teo} 
Sea $\Tilde{u}(x,t)$ la solución única $\P$- casi seguramente del problema \eqref{calorejemplo}. Entonces existe una modificación $u(x,t)$ de dicha solución cuyas trayectorias son continuas.
\end{teo}
La prueba de ambos resultados se puede encontrar en \cite[teorema 6.4, teorema 6.7]{Khoshnevisan2009}, respectivamente.

Para concluir el capítulo y en preparación para el problema central de la tesis, estamos interesados la solución al problema del calor estocástico siguiente.
\begin{equation}\label{shedefinitiva}
   \begin{cases}
      \partial_t u=\frac{1}{2}\partial_{xx}u+\sigma(u)\dot{W} & \qquad \forall x\in \R, \ \forall t>0,\\
      u_0(x):=u(x,0)=1 & \qquad \forall x\in \R,
   \end{cases}
\end{equation}
con $\dot{W}$ un ruido blanco definido en $[0,\infty)\times\R$, el cual podemos interpretar como un ruido blanco espacio-temporal, y $\sigma$ una función globalmente Lipschitz tal que $\sigma(1)\neq 0$.

Aunque la solución al problema anterior será fuertemente usada, dado que no es el propósito principal del texto, presentamos sin demostración el resultado que nos garantiza la existencia y unicidad del problema de calor anterior. El resultado que presentaremos está tomado de \cite[proposición 2.1]{KUZGUN202268}, pero una versión general junto con su prueba se pueden encontrar en \cite[teorema 2.4]{ChenDalang2014}. Para enunciar el teorema que nos interesa, debemos otorgar algo de contexto, el cual también es extraído de \cite{ChenDalang2014}.

Consideremos primero la solución fundamental al problema de calor clásico en $\R\times [0,\infty)$,
\begin{equation*}
   \begin{cases}
      \partial_t u=\frac{\nu}{2}\partial_{xx}u, & \qquad \forall x\in \R, \ \forall t>0, \ \nu>0.\\
      u(x,0)=\delta(x), & \qquad \forall x\in \R.
   \end{cases}
\end{equation*}
donde $\delta(x)$ es la delta de Dirac. En nuestro caso, estamos interesados simplemente en $\nu=1$. Dicha solución fundamental se conoce que está dada por el núcleo de calor, que denotamos por 
\[
p_t(x)=\frac{1}{\sqrt{2\pi t}}e^{-x^2/2 t}, \qquad x\in \R, \ t\geq0.
\]
Dentemos por $J_{0}(t,x)$ la solución al problema de calor pero ahora con condición inicial $\mu$ y $\nu=1$, 
\begin{equation*}
   \begin{cases}
      \partial_t u=\frac{1}{2}\partial_{xx}u, & \qquad \forall x\in \R, \ \forall t>0,\\
      u(x,0)=\mu(x), & \qquad \forall x\in \R.
   \end{cases}
\end{equation*}
donde $\mu$ es una medida signada de Borel sobre todo $\R$ tal que 

   \begin{equation}\label{medidasignadaborel}
      \int_{\R}e^{-ax^2}|\mu|(dx)<\infty, \qquad \forall a>0.
   \end{equation}

Dicha solución fundamental se conoce que está dada está dada por la convolución de $\mu$ con el núcleo del calor, a saber, 
\[
J_0(t,x)=(\mu*p_t(\cdot))(x)=\int_{\R}p_t(x-y)\mu(dy).
\]
Entonces la ecuación del calor del problema estocástico \eqref{shedefinitiva} con condición inicial $u(x,0)=\mu(x)$ (no necesariamente $u(0,x)\equiv1)$ se puede reescribir combinando la solución a la ecuación homogénea $J_0$ y la integral del término estocástico, obteniendo al menos de manera formal, la siguiente forma integral,
\begin{align}
   u(t,x)&=J_0(t,x)+I(t,x)\notag\\
   &=\int_{\R}p_t(x-y)\mu(dy)+\int_{[0,t]\times\R}p_{t-s}(x-y)\sigma(u)W(ds,dy)\label{solucionmildoriginal}.
\end{align}
Por lo tanto, una solución mild a la ecuación del calor estocástica sobre $\R\times[0,\infty)$ pero con condición inicial $u(0,x)=\mu(t)$ una medida signada de Borel en $\R$ que cumple la condición \eqref{medidasignadaborel}, debería cumplir la ecuación \eqref{solucionmildoriginal}. En particular, si consideramos $\mu(x)\equiv 1$, directamente tenemos que $J_0(t,x)=1$, como es el caso que nos interesa. Llegamos así al siguiente teorema, el cual se enuncia en concordancia con \cite[proposición 2.1]{KUZGUN202268}.

\begin{teo}\label{solucionmild}
 Consideremos el problema de calor \eqref{shedefinitiva}. Entonces existe un único campo aleatorio $u=\{u(t,x): (t,x)\in [0,\infty)\times \R\}$ medible y adaptado, tal que para cualquier $T>0$ y $p\geq2$, 
 \begin{equation}\label{cotasolucionmild}
   \sup_{(t,x)\in [0,T]\times\R}\E\left[\abs{u(t,x)}^p\right]=C_{T,p},
 \end{equation}
 
 y para cualquier $t\geq0$ y $x\in \R$, $u$ satisface
 \begin{equation}\label{eqsolucionmild}
   u(t,x)=1+\int_{[0,t]\times \R}^{}p_{t-s}(x-y)\sigma(u(s,y))W(ds,dy).
 \end{equation}
 \end{teo}
Concluimos aquí nuestro breve estudio de ecuaciones diferenciales parciales estocásticas. Retomaremos estos conceptos en el capítulo 4, cuando la teoría necesaria para abordar el problema principal de la tesis esté cubierta.
% \begin{itemize}
%     \item Planteamiento de la ecuación integral como una versión débil del problema planteado. Definición de la solución Mild utilizando funciones de Green.
%     \item Existencia de una única solución bajo condiciones Lipschitz. Argumento usando Lema de Gronwall para unicidad. Argumento usando iteraciones de Piccard para la existencia.
% \end{itemize}

\chapter{Elementos de Cálculo de Malliavin}% y Método de Stein}
En este capítulo se revisa material esencial de cálculo de Malliavin. Esta disciplina nació a finales de la década de 1970 a partir de los trabajos de Paul Malliavin. La intención de Malliavin era hallar una prueba probabilista del teorema de Hörmander sobre el efecto regularizante de las soluciones de cierta ecuación diferencial estocástica, puesto que Lars Hörmander en 1967 encontró una prueba que utilizaba herramientas puramente analíticas. Más allá de dicha motivación inicial, en las últimas décadas el cálculo de Malliavin se ha convertido en una poderosa herramienta matemática para estudiar problemas diversos en probabilidad. 

En palabras de Nourdin y Peccati (ver prefacio de \cite{Nourdin_Peccati_2012}), el cálculo de Malliavin se puede entender como un cálculo diferencial en infinitas dimensiones, cuyos operadores actúan sobre funcionales de procesos gaussianos generales, y que en conjunto con el método de Stein, forman una amalgama poderosa para deducir teoremas tipo límite central, así como tasas de convergencia explícitas para los mismos. 

Tres operadores son esenciales en esta área: la derivada de Malliavin $D$, el operador de divergencia $\delta$ (o integral de Skorokhod) y el generador infinitesimal. El propósito de este capítulo es estudiar el material nuclear correspondiente a estos operadores.
\section{Cálculo de Malliavin en el caso unidimensional}
Siguiendo de cerca la exposición hecha por \cite{Nourdin_Peccati_2012}, comenzamos el estudio del tópico por el caso unidimensional. Aquí es más sencillo observar propiedades de estos objetos, los cuales se generalizan a dimensiones infinitas en la siguiente sección.

\subsection{La derivada de Malliavin.}

El operador con el que comenzamos nuestro estudio es la derivada de Malliavin. La idea es definir la derivada de variables aleatorias del tipo $f(N)$, donde $f$ es una función determinista, y $N$ es una variable aleatoria con distribución normal estándar.

En el contexto unidimensional, la definición de este operador se realiza utilizando ideas similares a aquellas que nos llevan a la definición de derivada débil en los espacios de Sobolev. Dichas ideas consisten, a grandes rasgos, en generalizar la noción de derivada a objetos más generales que las funciones diferenciables en el sentido usual. Para ello primero buscamos una clase de funciones diferenciables con propiedades nobles pero que a su vez sea suficientemente rica, y posteriormente extendemos el operador en algún sentido.

Dado que buscamos definir la derivada de funciones de variables normales estándar, es natural definir el operador derivada en un subconjunto de los espacios $L^q(\R,\B(\R))$ con $q\in [1,\infty)$, pero dotados de la medida gaussiana estándar. 

Dicho lo anterior, denotemos por $\gamma:\B(\R)\to [0,1]$ a la medida mencionada, a saber, 
\[
\gamma(A)=\int_A\frac{1}{2\pi}e^{-\frac{x^2}{2}}dx.    
\]
Consideremos el espacio $L^{q}(\R,\B(\R),\gamma(dx))$, con $q\in [1,\infty)$, que durante esta subsección denotaremos simplemente como $L^{q}(\gamma)$. En este espacio, se valen algunas propiedades que resultan de gran utilidad. Tal es el caso del siguiente lema.

\begin{lema}\label{lema1}
Sea $f:\R\to\R$ una función absolutamente continua con respecto a la medida de Lebesgue y tal que $f'\in L^{1}(\gamma)$. Entonces la función $h:\R\to\R$ dada por $h(x)=xf(x)$ está en $L^1(\gamma)$ y se cumple que 
\[
\int_\R xf(x)d\gamma(x)=\int_\R f'(x)d\gamma(x).
\]
\end{lema}
\begin{proof} 
  Supongamos primero que $f(0)=0$. Probamos que $h\in L^{1}(\gamma)$. En efecto,  
  \begin{align*}
   \int_{-\infty}^{\infty}|x||f(x)|d\gamma(x)&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty\abs{\int_0^{x}f'(y)dy}|x|e^{-\frac{x^2}{2}}dx\\
   &\leq \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \left(\int_0^{x}\abs{f'(y)}dy\right) \ |x|e^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \left(\int_x^{0}\abs{f'(y)}dy\right) \ (-x)e^{-\frac{x^2}{2}}dx\\
   & \ \ \  +\frac{1}{\sqrt{2\pi}}\int_{0}^\infty \left(\int_0^{x}\abs{f'(y)}dy\right) \ xe^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \int_{-\infty}^{y}\abs{f'(y)}(-x)e^{-\frac{x^2}{2}}dx \ dy+\frac{1}{\sqrt{2\pi}} \int_{0}^{\infty}\int_{y}^\infty \abs{f'(y)}xe^{-\frac{x^2}{2}}dx \ dy\\
   &=\int_{-\infty}^0 \abs{f'(y)}\frac{1}{\sqrt{2\pi}}\left(e^{-\frac{x^2}{2}}\Big|_{-\infty}^{y}\right) dy + \int_{0}^{\infty}\abs{f'(y)}\frac{1}{\sqrt{2\pi}}\left(-e^{-\frac{x^2}{2}}\Big|_{y}^{\infty}\right)dy\\
   &=\int_{-\infty}^{\infty}|f'(y)|d\gamma(y)<\infty,
  \end{align*}
  en donde hemos hecho uso del teorema de Fubini en la tercera igualdad. Por lo tanto, la función $h(x)=xf(x)$ en efecto está en $L^1(\gamma)$. Realizando cuentas similares, obtenemos que 
  \begin{align*}
   \int_{-\infty}^{\infty}xf(x)d\gamma(x)&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \left(\int_0^{x}f'(y)dy\right) xe^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \left(\int_x^{0}f'(y)dy\right) \ (-x)e^{-\frac{x^2}{2}}dx\\
   & \ \ \  +\frac{1}{\sqrt{2\pi}}\int_{0}^\infty \left(\int_0^{x}f'(y) dy\right) \ xe^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \int_{-\infty}^{y}f'(y)(-x)e^{-\frac{x^2}{2}}dx\ dy+\frac{1}{\sqrt{2\pi}} \int_{0}^{\infty}\int_{y}^\infty f'(y)xe^{-\frac{x^2}{2}}dx\ dy\\
   &=\int_{-\infty}^0 f'(y)\frac{1}{\sqrt{2\pi}}\left(e^{-\frac{x^2}{2}}\Big|_{-\infty}^{y}\right) dy + \int_{0}^{\infty}f'(y)\frac{1}{\sqrt{2\pi}}\left(-e^{-\frac{x^2}{2}}\Big|_{y}^{\infty}\right)dy\\
   &=\int_{-\infty}^{\infty}f(y)d\gamma(y),
  \end{align*}
  como queríamos. Para el caso en el que $f(0)\neq 0$, utilizamos la función $g(x)=f(x)-f(0)$ y repetimos las cuentas.
\end{proof}
En la búsqueda de la generalización de la derivada a funciones más generales, definimos a la clase de Schwarz $\mathcal{S}$ como el conjunto de funciones en $C^{\infty}(\R,\R)$ tales que ella y sus derivadas tienen crecimiento a lo más polinomial, es decir,
\[
\mathcal{S}:=\left\{f\in C^{\infty}(\R) : \forall p\in \{0,1,2,...\} \ \exists n\geq1 \text{ tal que }\lim_{x\to\pm\infty}\frac{|f^{(p)}(x)|}{1+|x|^{n}}<\infty \right\}.    
\]
Observamos que este conjunto dotado de la restricción correspondiente de la norma $L^q(\gamma)$ forma subespacio lineal de $L^{q}(\gamma)$, puesto que las combinaciones lineales de elementos infinitamente derivables vuelven a ser infinitamente derivables, y las combinaciones lineales también son compatibles con los límites. Es de destacar que este espacio es suficientemente amplio como para poseer a los polinomios. Más aún, este último conjunto es denso en $L^{q}(\gamma)$ para cualquier $q\in [1,\infty)$. Esto es consecuencia del siguiente lema.

\begin{lema} 
Para cualquier $q\in [1,\infty)$, el conjunto de los monomios de la forma $\{x^n: n\geq0\}$ genera un subespacio denso del espacio $L^{q}(\gamma)$. En particular, la clase de Schwarz $\S$ forma un subconjunto denso de $L^q(\gamma)$.
\end{lema}
\begin{proof} 
   Un corolario del teorema de Hann-Banach caracteriza la propiedad de ser un subespacio lineal denso de un espacio $L^{q}$. En nuestro caso basta con probar que para cualquier elemento $g \in L^{p}(\gamma)$, 
   \[
   \int_\R g(x)x^{n} \ d\gamma(x)=0, \ \ \forall n\geq0  \quad \ent \quad g=0 \ \ \gamma- \text{casi en todas partes},
   \]
   donde $p\in (1,\infty]$ es el exponente conjugado de $q$. Sea pues $g\in L^{p}(\gamma)$ y supongamos que para cualquier $n\geq0$, el antecedente de la implicación anterior es válido. Usaremos un argumento que involucra la transformada de Fourier. Notamos primero que la transformada de $g(x)e^{-\frac{x^2}{2}}$ existe. En efecto, por la desigualdad de Hölder, para $t\in \R$ arbitrario,
      \begin{align*}
         \int_{\R}|g(x)e^{-\frac{x^2}{2}}e^{itx}|dx&\leq \int_{\R}|g(x)|e^{-\frac{x^2}{2}}dx\\
         &=\int_{\R}|g(x)|d\gamma(x)\\
         &\leq\norm{g}_{L^{p}(\gamma)}\left(\int_{\R}d\gamma(x)\right)^{\frac{1}{q}}\\
         &=\norm{g}_{L^{p}(\gamma)}<\infty.
      \end{align*}
   
   Por otro lado, para cualquier $x\in \R$,  
   \[
   \abs{g(x)\sum_{k=0}^{n}\frac{(itx)^k}{k!}}\leq |g(x)|\sum_{k=0}^{n}\frac{|tx|^{k}}{k!}\leq |g(x)|\sum_{k=0}^{\infty}\frac{|tx|^{k}}{k!}=|g(x)|e^{|tx|},
   \]
   y la cota anterior vale para cualquier $n\geq1$. Nuevamente usando la desigualdad de Hölder,
   \[
   \int_\R\abs{g(x)\sum_{k=0}^{n}\frac{(itx)^{k}}{k!}}d\gamma(x)\leq \int_\R |g(x)|e^{|tx|}d\gamma(x)\leq \norm{g}_{L^{p}(\gamma)}\left(\int_\R (e^{|tx|})^qd\gamma(x)\right)^{\frac{1}{q}}. 
   \]
   Observamos que se tiene la igualdad 
   \[
   \int_\R (e^{|tx|})^{q}d\gamma(x)=\frac{1}{\sqrt{2\pi}}\int_\R e^{q|tx|}e^{-\frac{x^2}{2}}dx,
   \]
   y notamos que esta última integral es finita para cualquier $q\in [1,\infty)$, ya que el término correspondiente a la medida gaussiana domina al otro término exponencial. Por lo tanto, $|g(x)|e^{|tx|}$ es una función en $L^{1}(\gamma)$ que domina a las sumas parciales anteriores. Así, usando el teorema de convergencia dominada, 
   \[
   \int_\R g(x)e^{-\frac{x^2}{2}}e^{itx}dx=\int_\R \lim_{n\to \infty} g(x)\sum_{k=0}^{n}\frac{(itx)^k}{k!}d\gamma(x)=\lim_{n\to \infty}\sum_{k=0}^{n}\frac{(it)^k}{k!}\int_\R g(x)x^k d\gamma(x)=0, 
   \]
   donde hemos hecho uso de la hipótesis del enunciado en la última igualdad. Hemos hallado que la transformada de Fourier de $g(x)e^{-\frac{x^2}{2}}$ es idénticamente la función 0. Al ser la transformada de Fourier inyectiva en $L^{1}(\R,\B(\R),dx)$, se sigue que $g(x)e^{-\frac{x^2}{2}}$ es la función idénticamente 0 salvo conjuntos de medida de Lebesgue 0. En particular, $g$ es la función idénticamente 0, $\gamma \ -$ casi en todas partes.  

   Concluimos la prueba notando que los monomios son un subconjunto de la clase de Schwarz $\S$.
 \end{proof}
Sabiendo que $\mathcal{S}$ es un subespacio lineal denso de $L^{q}(\gamma)$, podemos generalizar la derivada de las funciones en el espacio de Schwarz a funciones más generales que pertenecen a $L^{q}(\gamma)$. Para ello denotemos por ahora como $\frac{d^p}{dx^p}f$ a la derivada de orden $p$ de una función $f\in \S$, para $p\in \{0,1,2,...\}$. Dicha derivada siempre está definida para tales funciones. Si pensamos en la derivación de orden $p$ como aplicar un operador a un elemento del espacio de Schwarz, resulta que dicho operador es lineal. Dado que buscamos extender la derivada a funciones más generales, es natural pensar en la posibilidad de extender el operador $\frac{d^p}{dx^{p}}:\S\to L^{q}(\gamma)$ a un operador que sea cerrado en un dominio más grande que $\S$. Este es el contenido del siguiente teorema.
\begin{teo} 
Sea $\frac{d^{p}}{dx^p}:\mathcal{S}\longrightarrow L^{q}(\gamma)$ el operador derivada de orden $p$. Dicho operador es cerrable para todo $q\in [1,\infty)$ y todo entero $p\geq1$
\end{teo}
\begin{proof} 
  Usamos la siguiente caracterización de ser un operador cerrable. Supongamos que $(f_n)_{n\geq1}$ es una sucesión en $\S$ y que $g\in L^{q}(\gamma)$ son tales que 
  \[
  f_n\xrightarrow[n\to \infty]{}0 \qquad \text{ y } \qquad \frac{d^{p}}{dx^{p}}f_n\xrightarrow[n\to \infty]{}g.
  \] 
   Buscamos probar que $g=0$, $\gamma \ - $ casi en todas partes.
   
   Probamos solamente el caso $q>1$. Sea $h\in \S$. Definimos $\delta h(x):=xh(x)-h'(x)$ y para $n\geq1$, $\delta^{n+1}h(x):=\delta^{n} h(x)$, donde $\delta^{1}:=\delta$. Observemos primero que para cualquier $n\in \{1,2,...,p\}$, $\delta^{n}h\in \S$. Esto se sigue de que $xh(x)-h'(x)$ sigue siendo una función infinitamente diferenciable con crecimiento a lo más polinomial, y un argumento inductivo.
   
   Ahora bien, usando la igualdad
   \[
   \int_\R \left(\frac{d^{p-1}}{dx^{p-1}}f_n(x)h(x)\right)'d\gamma(x)=\int_\R \frac{d^{p}}{dx^{p}}f_n(x)h(x)d\gamma(x) +\int _\R \frac{d^{p-1}}{dx^{p-1}}f_n(x)h'(x)d\gamma(x),
   \] 
   la cual se obtiene con la derivada del producto, se tiene que 
   \begin{align*}
       \int_{\R}h(x)g(x)d\gamma(x)&=\lim_{n\to\infty}\int_\R \frac{d^{p}}{dx^{p}}f_n(x) h(x) d\gamma(x)\\
       &=\lim_{n\to\infty}\left(\int_\R \left(\frac{d^{p-1}}{dx^{p-1}}f_n(x)h(x)\right)'d\gamma(x)-\int _\R \frac{d^{p-1}}{dx^{p-1}}f_n(x)h'(x)d\gamma(x)\right)\\
       &=\lim_{n\to\infty}\left(\int_\R x\frac{d^{p-1}}{dx^{p-1}}f_n(x)h(x)d\gamma(x)-\int _\R \frac{d^{p-1}}{dx^{p-1}}f_n(x)h'(x)d\gamma(x)\right)\\
       &=\lim_{n\to\infty}\int_\R \frac{d^{p-1}}{dx^{p-1}}f_n(x) \left(xh(x)-h'(x)\right) d\gamma(x)\\
       &=\lim_{n\to\infty}\int_\R \frac{d^{p-1}}{dx^{p-1}}f_n(x) \delta h(x) d\gamma(x),
   \end{align*}
   donde en la primera igualdad hemos hecho uso del teorema de convergencia dominada, y en la tercera igualdad hemos hecho uso del lema \ref{lema1}. Utilizando un argumento inductivo y la continuidad del valor absoluto, obtenemos que 
   \[
       \abs{\int_{\R}h(x)g(x)d\gamma(x)}\leq\int_{\R}\abs{h(x)g(x)} d\gamma(x)=\lim_{n\to\infty}\int_\R \abs{f_n(x) \delta^{p} h(x)} d\gamma(x).
   \]
   Por hipótesis, dado que la sucesión $f_n$ converge a 0 en $L^{q}(\gamma)$ y $\delta^{p}h \in \S\subseteq L^{\frac{q}{q-1}}$ según el Lema 2, usando la desigualdad de Hölder, 
    \[
       \abs{\int_{\R}h(x)g(x)d\gamma(x)}\leq\lim_{n\to\infty}\int_\R |f_n(x) \delta^{p} h(x)| d\gamma(x)\leq \lim_{n\to \infty}\norm{f_n}_{L^{q}(\gamma)}\norm{\delta^{p}h}_{L^{\frac{q}{q-1}}(\gamma)}=0,
    \]
    de donde deducimos que para cualquier $h\in \S$, 
    \[
      \int_{\R}h(x)g(x)d\gamma(x)=0.
      \]
      Dado que $\S$ es denso en $L^{q}(\gamma)$, se sigue que $g=0$, $\gamma \ - $ casi en todas partes.
      
   \end{proof}
Hemos demostrado que el operador derivada de orden $p$ es cerrable en $\S$. Por lo tanto, por definición existe un operador cerrado, que vamos a denotar por $D^{p}$, con dominio $\text{Dom}(D^{p})$ un subespacio lineal de $L^{q}(\gamma)$ tal que $\S\subseteq \text{Dom}(D^{p})$ y $D^{p}f=\frac{d^{p}}{dx^{p}}f$ para cualquier $f\in \S$. Estos elementos nos permiten definir la derivada de Malliavin.

\begin{dfn}
   Sea $p\geq1$ un entero positivo. Definimos la \textit{derivada de Malliavin} como el operador $D^{p}$ que extiende a la derivada $\frac{d^{p}}{dx^{p}}$ vista como operador de $\S$ a $L^{q}(\gamma)$, para $q\in [1,\infty)$.
\end{dfn}

Denotaremos al dominio de $D^{p}$ como $\D^{p,q}:=\text{Dom}(D^{p})$. A dichos espacios también se les conoce como espacios de Watanabe-Sobolev. 
% Es conocido que al ser $D^{p}$ un operador cerrado, podemos definir la \textit{norma de la gráfica} de dicho operador en su dominio como la función $\|.\|_{G}:\D^{p,q}\longrightarrow[0,\infty)$ dada por,
% \[
%        \|f\|_{G}:=\norm{f}_{L^q(\gamma)}+\norm{D^{p}f}_{L^{q}(\gamma)}.
%    \]
% Dicha norma vuelve a la derivada de Malliavin $D^{p}$ un operador continuo. Más aún, es un resultado conocido de análisis funcional que, al ser $D^{p}$ un operador cerrado, su dominio $\D^{p,q}$ es un espacio de Banach con la norma $\norm{\cdot}_{G}$ definida antes.

Ahora bien, es natural pensar en lo que sucede con una aplicación sucesiva de las derivadas de Malliavin. Se tiene que para cualquier $p\geq1$ entero, y $f\in \D^{p+1,q}$, 
\[
   D^{p+1}f=D(D^{p}f)=D^{p}(Df),
\]
lo que nos dice que la derivada de Malliavin en efecto respeta la composición, tal y como ocurre en la derivación usual. Se sigue de esto que, al ser consistente la derivación sucesiva en en el sentido de Malliavin, para una función $f\in \D^{p,q}$, las derivadas $D^{k}f$ existen para cualquier $k\in \{1,...,p\}$, y por lo tanto, las normas $\norm{D^kf}_{L^2(\gamma)}$ son finitas para cualquier $k\in \{1,...,p\}$. Luego, se puede demostrar que la función $\|.\|_{\mathbb{D}^{p,q}}:\D^{p,q}\longrightarrow[0,\infty)$ dada por
\[
   \|f\|_{\D^{p,q}}:=\left(\norm{f}_{L^{q}(\gamma)}^q+\sum_{k=1}^p\norm{D^{k}f}_{L^{q}(\gamma)}^{q}\right)^{\frac{1}{q}}
\] 
es una norma que hace a $\D^{p,q}$ un espacio de Banach.

Concluimos observando que $\D^{p,q}$ también puede ser obtenido como la cerradura del subespacio $\S$ con respecto a la norma $\norm{\cdot}_{\D^{p,q}}$ definida antes, pero restringida al espacio $\S$, en donde la derivada de Malliavin coincide con la derivada usual.

Lo anterior permite hacer una caracterización del espacio $\D^{p,q}$ y una definición alternativa de la derivada de Malliavin: una función $f$ está en dicho espacio si y solo si existe una sucesión de funciones $(f_n)_{n\geq1}\subseteq \S$ tales que $f_n\xrightarrow[n\to\infty]{}f$ y $(\frac{d^{p}}{dx^{p}}f_n)_{n\geq1}$ es una sucesión de Cauchy en $L^q(\gamma)$, para cualquier $k\in \{1,...,p\}$. Para tal sucesión de funciones, definimos 
\[
D^{k}f:=\lim_{n\to \infty}\frac{d^k}{dx^{k}}f_n,
\]
en donde el límite es en sentido $L^{q}(\gamma)$, y dicha función límite coincide con la derivada de Malliavin definida antes.

El espacio $\D^{p,q}$ cumple las siguientes relaciones de contención.

\begin{teo}\label{teocontencionessobolev}
Sean $p\geq0$ y $q'>q$. Entonces se cumple que 
\[
\D^{p+1,q'}\subseteq \D^{p,q},
\]
además de que por convención, $\D^{0,q}=L^{q}(\Omega)$ y $\norm{\cdot}_{0,q}=\norm{\cdot}_{q}$, y denotaremos 
\[
\D^{\infty,q}:=\bigcap_{p\geq0} \D^{p,q}.
\]

Más aún, la derivada $D^{p}$ es compatible en la intersección $\D^{p,q}\cap \D^{p,q'}$.
\end{teo}
La prueba de este teorema se puede encontrar en \cite[p. 27]{nualart2006malliavin}. En particular el resultado nos dice que, al ser $\D^{p,q}$ subconjuntos de los espacios $L^{q}(\Omega)$, y ser este último un espacio de medid finita, entonces la contención $L^{q'}$ para $q'>q$ se sigue preservando en los espacios de Watanabe-Sobolev. La contención en la variable $p$ nos dice simplemente que si una variable es derivable $n+1$ veces, claramente será $n$ veces derivable.

Este operador es de suma importancia. En la siguiente sección veremos que en el caso en que $q=2$, la derivada de Malliavin vista como operador posee propiedades interesantes. En preparación para esto, tenemos el siguiente resultado
\begin{teo} 
 Sea $p\geq1$ y consideremos el espacio $\D^{p,2}$. Dicho espacio es un espacio de Hilbert con producto interno dado por 
 \[
 \langle f,g\rangle_{\D^{p,2}}:=\E\left[fg\right]+\sum_{k=1}^{p}\E\left[\langle D^{k}f,D^{k}g\rangle_{\mathfrak{H}^{\otimes k}}\right].
 \]
 \end{teo}

\subsection{El operador de divergencia}

A lo largo de esta sección consideraremos el espacio $L^2(\gamma)$, el cual es un espacio de Hilbert con producto interno dado por 
\[
\langle f,g\rangle=\int_\R f(x)g(x)\ d\gamma(x).
\]
A partir de ahora, y para el resto del texto, denotaremos simplemente por $D^{p}$ a la derivada de Malliavin de orden $p$, aún cuando nos estemos refiriendo a la derivada de orden $p$ convencional de una función $f$ en el espacio de Schwarz.

Hasta ahora tenemos que la derivada de Malliavin es un operador $D^{p}:\mathbb{D}^{p,2}\subseteq L^2(\gamma)\longrightarrow L^2(\gamma)$ y dicho operador, cerrado por construcción, extiende al operador derivada convencional $D^{p}$ definido en $\mathcal{S}\subseteq L^2(\gamma)$. 

Siendo $L^{2}(\gamma)$ un espacio de Hilbert, si denotamos por $\text{Dom}(\delta^{p})$ como el conjunto de funciones $g\in L^{2}(\gamma)$ tales que existe una constante $c_g>0$ que cumple
\[
\abs{\langle D^{p}f,g\rangle_{L^2(\gamma)}}\leq c_g\|f\|_{L^2(\gamma)}, \qquad \forall f\in \mathbb{D}^{p,2},                 
\]
entonces para cada $g\in \text{Dom}(\delta^{p})$ el funcional lineal, con dominio $\D^{2,q}$ y con regla de correspondencia $f\longmapsto \langle D^{p}f,g\rangle$, es acotado precisamente por la condición anterior. De esta forma, por el teorema de Hanh-Banach podemos extender dicho funcional a un funcional lineal acotado cuyo dominio es todo $L^2(\gamma)$.

Con el funcional anterior podemos usar el teorema de representación de Riesz para obtener la existencia de un único elemento en $L^2(\gamma)$, que denotaremos por $\delta^{p}g$ tal que 
\[
\langle D^pf,g\rangle_{L^2(\gamma)}=\langle f,\delta^p g\rangle_{L^2(\gamma)}, \qquad \forall f\in L^2(\gamma).
\]
En términos de integrales, se tiene que para cualquier $g\in \text{Dom}(\delta^p)$, existe un único elemento $\delta^p g\in L^2(\gamma)$ tal que 
\[
 \int_\R D^{p}f(x) g(x) \ d\gamma(x)=\int_\R f(x)\delta^{p}g(x) \ d\gamma(x), \qquad \forall f\in L^2(\gamma).  
\]
Con los elementos anteriores, podemos definir el operador de divergencia.

\begin{dfn}
   Sea $p\geq1$ un entero positivo. Denotamos $$\text{Dom}(\delta^p):=\left\{g\in L^2(\gamma) \ : \ \abs{\langle D^{p}f,g\rangle_{L^2(\gamma)}}\leq c\|f\|_{L^2(\gamma)}, \qquad \forall f\in \D^{2,p}\right\},$$ 

   y definimos el operador de divergencia $p$-ésimo como la aplicación $\delta^p:\text{Dom}(\delta^{p})\longrightarrow L^2(\gamma)$ tal que asigna a cada $g\in \text{Dom}(\delta^{p})$ al único elemento $\delta^{p} g\in L^2(\gamma)$ que, gracias al teorema de representación de Riesz, cumple la igualdad
       \[
       \int_\R D^{p}f(x)g(x) \ d\gamma(x)=\int_\R f(x)\delta^{p}g(x) \ d\gamma(x), \qquad \forall f\in L^2(\gamma).
      \]
\end{dfn}
En el caso en que $p=1$, directamente escribimos $\delta^1=\delta$. Observamos que el operador $\delta^p$ es directamente el operador adjunto de $D^{p}$, y como tal, es un operador cerrado.

Es de interés encontrar una forma de calcular el operador $\delta$ aplicado a alguna función en $\text{Dom}(\delta)$. En este sentido, existe una fórmula para calcular $\delta g$ para $g\in \D^{1,2}$. Este es el contenido del siguiente lema.

\begin{lema}\label{formuladelta}
   Se cumple que $\S\subseteq \text{Dom}(\delta)$ y para cualquier $g\in \D^{1,2}$, 
   \[
      \delta g(x)=xg(x)-Dg(x)=xg(x)-g'(x).
   \]
 En particular, para cualquier $g\in \S$, se cumple la fórmula anterior.
 \end{lema}

 \begin{proof} 
    Nótese que para $f, g\in \S$, por el lema \ref{lema1},
   \[
   \int_\R (f(x)g(x))'d\gamma(x)=\int_\R xf(x)g(x)d\gamma(x), 
   \]
   por lo que 
   \[
   \int_\R xf(x)g(x)d\gamma(x)=\int_\R D \left(f(x)g(x)\right) d\gamma(x)=\int_\R Df(x)g(x)d\gamma(x) +\int_\R f(x)Dg(x)d\gamma(x),
   \]
   así que restando, 
   \[
   \int_\R Df(x)g(x)d\gamma(x)=\int_\R xf(x)g(x)d\gamma(x)-\int_\R f(x)Dg(x)d\gamma(x),
   \]
   y por definición de $\delta g$, se tiene que 
   \[
   \int_\R f(x)\delta g(x)d\gamma(x)=\int_\R xf(x)g(x)d\gamma(x)-\int_\R f(x)Dg(x)d\gamma(x).   
   \]
   Pero lo anterior ocurre para cualquier $f\in \mathcal{S}$. Esto nos dice que $\mathcal{S}\subseteq \text{Dom}(\delta)$ y que para cualquier $g\in \mathcal{S}$, la fórmula anterior es válida. 

Utilizando un argumento de aproximación, tenemos que para cualquier $g\in \mathbb{D}^{1,2}$, $\delta g(x)=xg(x)-Dg(x)$.

  \end{proof}

\subsection{El semigrupo de Ornstein-Uhlenbeck}
El semigrupo de Ornstein-Uhlenbeck es una generalización del operador Laplaciano a dimensión infinita. Comenzamos directamente definiendo el semigrupo de Ornstein-Uhlenbeck. 
\begin{dfn}
   El semigrupo de Ornstein-Uhlenbeck, denotado por $(P_t)_{t\geq0}$, se define como una familia de operadores del espacio $\S$ dotado de la norma $\norm{\cdot}_{L^q(\gamma)}$ a $L^q(\gamma)$ como sigue: para $f\in \mathcal{S}$ y $t\geq0$,
   \[
   P_tf(x)=\int_\R f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y), \qquad  \forall x\in \R.
   \]
\end{dfn}
De manera inmediata se observa que esta es una familia de operadores lineales. Tal como se puede esperar, esta familia de operadores cumple la propiedad de semigrupo, y está directamente relacionada con el proceso de Ornstein-Uhlenbeck. Ambos hechos se exploran más adelante. Comenzamos estudiando propiedades básicas de $(P_t)_{t\geq0}$.

\begin{teo}
   Sea $f\in \mathcal{S}$ y $q\in [1,\infty)$. Entonces se cumplen las siguientes proposiciones:
   \begin{itemize}
       \item $P_0f(x)=f(x)$.
       \item $P_\infty f(x):=\displaystyle\lim_{t\to\infty}P_tf(x)=\int_\R f(y)d\gamma(y)$.
       \item $\displaystyle\int_\R \abs{P_tf(x)}^qd\gamma(x)\leq \int_\R\abs{f(x)}^qd\gamma(x)$.
   \end{itemize}
\end{teo}
\begin{proof} 
  \begin{itemize}
   \item  Calculamos el semigrupo en $t=0$. Dado que $\gamma(\R)=1$, se tiene que 
    \[
    P_0f(x)=\int_\R f\left(e^{0}x+\sqrt{1-e^{0}}y\right)d\gamma(y)=\int_\R f(x)d\gamma(y)=f(x).
    \]
    \item Como $f\in \S$, tiene crecimiento a lo más polinomial. Por lo tanto existe un real $M>0$ y un natural $n\geq1$ tal que para cualquier $y\in \R\setminus[-M,M]$, $|f(y)|\leq C(1+|y|^n)$, para alguna constante $C>0$. Observamos que esta última cota es una función en $L^q(\gamma)$. Por otro lado, es claro que en $[-M,M]$, la función $|f|$ alcanza su máximo, por lo que $\norm{f\1_{[-M,M]}}_\infty<\infty$. 
    
    Finalmente, notamos que la función $g(y)=e^{-t}x+\sqrt{1-e^{-2t}}y$ es una función continua y por lo tanto medible, así que
    
    \begin{align*}
       \abs{f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)}&\leq C(1+|y|^n)\1_{g^{-1}[-M,M]^{c}}(y)+\norm{f\1_{[-M,M]}}_\infty\1_{g^{-1}[-M,M]}(y)\\
       &\leq C(1+|y|^n)+\norm{f\1_{[-M,M]}}_\infty,
    \end{align*}
    función que está en $L^1(\gamma)$, y que es independiente de $t$. Por lo tanto, por el teorema de convergencia dominada, 
    \[
    P_\infty f(x)=\lim_{t\to\infty}P_tf(x)=\int_\R\lim_{t\to\infty}f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)=\int_\R f(y)d\gamma(y).
    \]
    \item Para $q\in [1,\infty)$, por definición,
    \begin{align*}
       \int_\R |P_tf(x)|^q d\gamma(x)&=\int_\R \abs{\int_\R f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)}^qd\gamma(x)\\
       &\leq \int_\R \int_\R \abs{f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)}^qd\gamma(y)d\gamma(x)\\
       &=\E\left[f\left(\abs{e^{-t}X+\sqrt{1-e^{-2t}}Y}^q \right)\right],
    \end{align*}
    donde hemos hecho uso de la desigualdad de Jensen, y $X,Y$ son variables aleatorias normales estándar independientes. Pero notemos que, denotando $Z:= e^{-t}X+\sqrt{1-e^{-2t}}Y$, la función característica de $Z$ es
    \begin{align*}
      \phi_Z(z)&=\E\left[e^{ize^{-t}X+iz\sqrt{1-e^{-2t}}Y}\right]\\
      &=\E\left[e^{ize^{-t}X}\right]\E\left[e^{iz\sqrt{1-e^{-2t}}Y}\right]\\
      &=e^{-\frac{z^2e^{-2t}}{2}}e^{-\frac{z^2(1-e^{-2t})}{2}}\\
      &=e^{-\frac{z^2}{2}},
    \end{align*}  
    por lo que $Z$ tiene un distribución normal estándar. Se sigue que 
    \[
       \int_\R |P_tf(x)|^q d\gamma(x)=\E\left[f\left(\abs{e^{-t}X+\sqrt{1-e^{-2t}}Y}^q\right)\right]\leq\E\left[\abs{f(Z)}^q\right]=\int_\R \abs{f(x)}^qd\gamma(x).
    \]
  \end{itemize}
\end{proof}
Del último ítem de la proposición anterior se deduce que para cualquier $f\in \S$ y cualquier $t\geq0$, $P_tf\in L^q(\gamma)$. Más aún, la familia de operadores $(P_t)_{t\geq0}$ que van del espacio $(\S,\norm{\cdot}_{L^q(\gamma)})$ a $L^q(\gamma)$ son uniformemente acotados por la constante $1$.

Al ser $\S$ un subconjunto denso de $L^q(\gamma)$ y ser este último un espacio completo, por el conocido teorema BLT de análisis funcional, los operadores de $(P_t)_{t\geq0}$ se pueden extender de manera única a una familia de operadores, que también denotaremos por $(P_t)_{t\geq0}$, con dominio todo el espacio $L^q(\gamma)$, y cuyas normas siguen siendo acotadas por la constante 1. Esto último hace a $(P_t)_{t\geq0}$ una familia de operadores lineales contractivos.

Probamos ahora que $(P_t)_{t\geq0}$ en efecto forma un semigrupo.
\begin{teo} 
Sean $s,t\geq0$. Se tiene que $P_tP_s=P_{t+s}$ en $L^q(\gamma)$, para $q\in [1,\infty)$.
\end{teo}
\begin{proof} 
  Dado que la medida del espacio es finita, basta con probar el resultado para $L^1(\gamma)$. Sea $f\in L^1(\gamma)$. Tenemos las siguientes igualdades para $x\in \R$:
\begin{align*}
   P_{t}(P_sf)(x)&=\int_\R P_sf\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(x)\\
   &=\int_\R \int_\R f\left(e^{-s}(e^{-t}x+\sqrt{1-e^{-2t}}y)+\sqrt{1-e^{-2s}}z\right)d\gamma(z)d\gamma(y)\\
   &=\int_\R\int_\R f \left(e^{-(t+s)}x+e^{-s}\sqrt{1-e^{-2t}}y+\sqrt{1-e^{-2s}}z\right)d\gamma(z)d\gamma(y)\\
   &=\E\left[f \left(e^{-(t+s)}x+e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z\right)\right],
\end{align*}
donde $Y$ y $Z$ denotan variables aleatorias normales estándar e independientes. Ahora bien, nótese que la función característica de la variable $W:=e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z$ está dada por 

\begin{align*}
   \phi_W(w)&=\E\left[\exp \left\{iw(e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z)\right\}\right]\\
   &=\E\left[\exp \left\{iwe^{-s}\sqrt{1-e^{-2t}}Y\right\}\right]\E\left[\exp \left\{iw\sqrt{1-e^{-2s}}Z\right\}\right]\\
   &=\exp \left\{\frac{-w^2e^{-2s}(1-e^{-2t})}{2}\right\}\exp \left\{\frac{-w^2(1-e^{-2s})}{2}\right\}\\
   &=\exp \left\{\frac{-w^2(e^{-2s}-e^{-2(t+s)}+1-e^{-2s})}{2}\right\}\\
   &=\exp \left\{\frac{-w^2(1-e^{-2(t+s)})}{2}\right\},
\end{align*}
la cual corresponde con la función característica de una variable $V:=\sqrt{1-e^{-2(t+s)}}N$, donde $N$ es gaussiana estándar. Se sigue que 
\begin{align*}
   P_{t}(P_sf)(x)&=\E\left[f \left(e^{-(t+s)}x+e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z\right)\right]\\
   &=\E\left[f \left(e^{-(t+s)}+\sqrt{1-e^{-2(t+s)}}V\right)\right]\\
   &=\int_\R f \left(e^{-(t+s)}+\sqrt{1-e^{-2(t+s)}}v\right)d\gamma(v)\\
   &=P_{t+s}f(x).
\end{align*}
 \end{proof}
Finalmente, la siguiente proposición nos dice que tanto $P_t$ como $D$, vistos como operadores, pueden ser intercambiados en el espacio $\mathbb{D}^{1,2}$.
\begin{teo} 
 Sea $f\in \D^{1,2}$ y $t\geq0$. Entonces $P_tf\in \D^{1,2}$ y $DP_tf=e^{-t}P_tDf$.
 \end{teo}
 \begin{proof} 
   Sea $f\in \S$. La derivada de Malliavin coincide con la derivada usual en dicho espacio y por lo tanto, dado que la función $e^{-t}f' \left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)$ está dominada por una función similar a la del ítem 3 del teorema 8, podemos derivar bajo el signo de la integral, hallando que
   \begin{align*}
       DP_tf(x)&=\frac{d}{dx}\int_\R f \left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)\\
       &=\int_\R e^{-t}\frac{d}{dx}f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)\\
       &=e^{-t}P_t Df(x),
   \end{align*}
   para cualquier $x\in \R$. Esto prueba la propiedad en $\S\subseteq \D^{1,2}$. Para $f$ en dicho espacio general, usamos que $\S$ es denso en $\D^{1,2}$.
  \end{proof}
  Antes de continuar con la siguiente sección, mostramos la conexión de esta familia de operadores con el semigrupo de Ornstein-Uhlenbeck:
  \begin{teo} 
   Sea $B$ un movimiento browniano estándar. Consideramos la ecuación diferencial estocástica dada por 
   \[
   dX_t^{x}=\sqrt{2}dB_t-X^{x}_tdt, \qquad t\geq0,
   \]
   donde $X^{x}_0=x\in \R$. Se tiene que la solución de la ecuación diferencial anterior está dada por 
   \[
   X_t^x=e^{-t}x+\sqrt{2}\int_0^te^{-(t-s)}dB_s, \qquad t\geq0,
   \]
   el cual es un proceso de Markov. Más aún, el semigrupo asociado a dicho proceso es justamente $(P_t)_{t\geq0}$. Tal proceso es justamente el proceso de Ornstein-Uhlenbeck.
  \end{teo}
   \begin{proof} 
     Comenzamos notando que en efecto el proceso anteriormente definido cumple que $X_0^x=x$. Pero esto es claro ya que 
     \[
     X_0^x=e^{0}x+\int_{0}^{0}e^{-(t-s)}dB_s=x.
     \] 
     Vemos ahora que se cumple la ecuación diferencial estocástica. Sea $t>0$ y nótese que 
     \begin{align*}
       X_t^x&=e^{-t}x+\sqrt{2}\int_0^te^{-(t-s)}dB_s\\
       &=e^{-t}x+\sqrt{2}e^{-t}\int_0^{t}e^{s}dB_s\\
       &=e^{-t}\left(X_0^{x}e^{0}+\sqrt{2}e^{-t}\int_0^te^{s}dB_s \right),
     \end{align*}
     de donde concluimos que 
     \[
     e^{t}X_t^x-e^{0}X_0^x=\sqrt{2}\int_{0}^{t}e^{s}dB_s,
     \]
     lo cual en notación diferencial equivale a que 
     \[
     d \left(e^tX_t^x\right)=\sqrt{2}e^{t}dB_t,  \qquad t>0.
     \]
     Usando ahora la regla del producto y el hecho de que $e^{-t}$ es un proceso de variación finita,  
     \begin{align*}
       dX_t^{x}&=d(e^{-t}e^{t}X_t^{x})\\
       &=e^{-t}d(e^{t}X_t^{x})+e^{t}X_t^{x}d(e^{-t})+0\\
       &=e^{-t}\sqrt{2}e^{t}dB_t-e^{t}X_t^{t}e^{-t}dt\\
       &=\sqrt{2}dB_t-X_t^xdt,
     \end{align*}
     tal como queríamos probar. Por lo tanto el proceso $(X_t^x)_{t\geq0}$ definido como antes es solución a la ecuación diferencial estocástica dada. Como tal, dicho proceso resulta ser una difusión, y en particular un proceso de Markov. Resta ver que su semigrupo justamente es el semigrupo de Ornstein-Uhlenbeck definido antes.

     Directamente de la expresión de $X_t^{x}$, observamos que tenemos una integral de Itô cuyo integrando es estocástico. Por lo tanto, dicha integral forma un proceso gaussiano adaptado a la filtración generada por $B$. Por otro lado, calculando la función de medias y covarianzas del proceso hallamos lo siguiente:
     \begin{itemize}
        \item $\mu(t)=\E\left[X_t^{x}\right]=e^{-t}x$
        \item \begin{align*}\Gamma(s,t)&=\text{Cov}\left(X_s^x,X_t^x\right)\\
            &=\E\left[\left(X_t^{x}-e^{-t}x\right)\left(X_s^x-e^{-s}x\right)\right]\\
            &=\E\left[\left(\sqrt{2}e^{-t}\int_{0}^{t}e^{u}dB_u\right)\left(\sqrt{2}e^{-s}\int_{0}^{s}e^{v}dB_v\right)\right]\\
            &=2e^{-(t+s)}\E\left[\int_{0}^{s\wedge t}e^{2u}du\right]\\
            &=\frac{2}{2}e^{-(t+s)}(e^{2s\wedge t}-1)\\
            &=e^{-|t-s|}-e^{-(t+s)}.
        \end{align*}
     \end{itemize}
     En particular, se tiene para $s=t$ que $X_t^x\sim Normal(e^{-t}x,1-e^{-2t})$. Finalmente, si $B=(B_t)_{t\geq0}$ representa el movimiento browniano sobre el cual $(X_t^x)_{t\geq0}$ está definido, notamos que para $t\geq0$,
     \[
     \E\left[e^{-t}x+e^{-t}B_{e^{2t}-1}\right]=e^{-t}x
     \]
     y además, 
     \[
     \E\left[\left(e^{-t}B_{e^{2t}-1}\right) \left(e^{-s}B_{e^{2s}-1}\right)\right]=e^{-(t+s)}(\min(e^{2t},e^{2s})-1)=e^{-(t+s)}(e^{2s\wedge t}-1)=e^{-|t-s|}-e^{-(t+s)}.
     \]
     Concluimos que el proceso de Ornstein-Uhlenbeck $(X_t^{x})_{t\geq0}$ como solución a la ecuación diferencial estocástica dada antes, puede verse como 
     \[
        X_t^{x}=e^{-t}x-e^{-t}B_{e^{2t}-1}, \qquad t\geq0.
     \]
     De lo anterior, es muy sencillo ver que para $f$ medible y acotada, 
     \begin{align*}
      P_tf(x)&=\E\left[f(X_{t+s}^{x})|\F_s\right]\\
      &=\E_x\left[f(e^{-(t+s)}x-e^{-(t+s)}B_{e^{2(t+s)}-1})|\F_s\right]\\
      &=\int_\R f \left(e^{-t}x-\sqrt{1-e^{-2t}}y\right)d\gamma(y),
     \end{align*}
     
     el cual es justamente el semigrupo que hemos estado trabajando.
   \end{proof}

\subsection{El generador infinitesimal}
 Como es clásico en el contexto de procesos de Markov, asociado a un semigrupo tenemos la noción de generador infinitesimal. En nuestro caso consideremos al conjunto siguiente:
 \[
 \text{Dom}(L):=\left\{f\in L^2(\gamma): \lim_{h\to 0} \frac{P_{h}f-f}{h} \text{ existe en } L^{2}(\gamma)\right\}.
 \]  
Dicho conjunto es directamente el dominio del generador infinitesimal $L$. Así, dada $f\in \text{Dom}(L)$, se tiene que 
\[
Lf:=\lim_{h\to 0}\frac{P_hf-f}{h}.    
\]
Ya que el límite anterior recrea la definición de derivada en funciones reales, denotaremos
\[
   \frac{d}{dt}P_tf:=\lim_{h\to0}\frac{P_{t+h}f-P_tf}{h}, \qquad \text{ y en particular } \qquad  Lf:=\frac{d}{dt}P_tf\bigg|_{t=0},
\]
omitiendo en ocasiones la función $f$ cuando estemos hablando de una función en un conjunto particular. Ahora bien, en el espacio $\S$, para cualquier $t\geq0$ se tiene el siguiente resultado:
\[
\frac{d}{dt}P_t=\lim_{h\to0}\frac{P_{t+h}-P_t}{h}=\lim_{h\to0}P_t\frac{P_h-Id}{h}=P_t\lim_{h\to0}\frac{P_h-Id}{h}=P_t\frac{d}{dh}\bigg|_{h=0}P_h=P_tL.
\]
Invirtiendo la manera en cómo se toman los límites, se obtiene también que
\[
\frac{d}{dt}P_t=LP_t,  
\]
así que para cualquier $t\geq0$, y $f\in \S$, tenemos que 
\[
\frac{d}{dt}P_t=P_tL=LP_t.
\]

Finalmente, una proposición sumamente importante es el siguiente teorema.
\begin{teo} 
En el espacio de Schwarz, se tiene que $Lf=-\delta Df$. Más específicamente, para cualquier $f\in \S$, 
\[
Lf(x)=-xf'(x)+f''(x).   
\]
\end{teo}
 \begin{proof} 
   Sea $f\in \S$ y $t>0$. Notamos que, utilizando una función dominante, que existe gracias al hecho de que $f$ es suave, intercambiamos derivada con integral, por lo que
   \begin{align*}
    \frac{d}{dt}P_tf(x)&=\frac{d}{dt}\int_\R f(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
    &=\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)\left(-e^{-t}x+\frac{-y(-2e^{-2t})}{2\sqrt{1-e^{-2t}}}\right)d\gamma(y)\\
    &=\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)\left(-e^{-t}x+\frac{ye^{-2t}}{\sqrt{1-e^{-2t}}}\right)d\gamma(y)\\
    &=-x\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)e^{-t}d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\int_\R yf'(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y).
   \end{align*}
   Reconocemos en el primer integrando la derivada con respecto a $x$ de $P_tf$, mientras que en el segundo término reconocemos la integral de una función de la forma $yg(y)$, donde $g(y)=f'(e^{-t}x+\sqrt{1-e^{-2t}}y)$, función que está en $L^1(\gamma)$. Por lo tanto, por el Lema \ref{lema1}, 
    \begin{align*}
        &-x\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)e^{-t}d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\int_\R yf'(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
        &=-x\int_\R \frac{d}{dx}f(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\int_\R \frac{d}{dy}f'(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
        &=-x\frac{d}{dx}\int_\R f(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\sqrt{1-e^{-2t}}\int_\R f''(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
        &=-x(P_tf(x))'+e^{-2t}P_tf''(x).
    \end{align*}
    Tomando límite cuando $t\to0$ hallamos que 
    \[
    Lf(x)=\frac{d}{dt}P_tf(x)\bigg|_{t=0}=-\left(xf'(x)+f''(x)\right).
    \]
    Finalmente, dado que $f\in \S$, su derivada de Malliavin coincide con su derivada usual, por lo que usando el lema \ref{formuladelta}, el generador infinitesimal de una función en el espacio de Schwarz viene dado por 
    \[
    Lf(x)=-\delta Df(x).
    \]
  \end{proof}
Como ejemplo de la potencia de la proposición anterior, se demuestra a continuación la desigualdad de Poincaré en el contexto de los espacios de Watanabe-Sobolev.
Para tener una idea de la comparación entre ambos contextos, presentamos tanto la desigualdad de Poincaré en el contexto de espacios de Sobolev clásicos en $\R$ como la desigualdad de Poincaré en $\D^{2,p}$.

Recordemos que $W^{k,p}(U)=\left\{f\in L^{p}(U,\B(U),dx): D^jf\in L^p(U,\B(U),dx), j\in \{1,2,...,k\}\right\}$ es el espacio de Sobolev que consiste en el conjunto de aquellas funciones en $L^p(U)$ ($U\subseteq \R$ subconjunto abierto) tales que todas sus derivadas hasta la $k$-ésima en el sentido distribucional también pertenecen a $L^p(U,\B(U),dx)$.

\begin{teo}\textbf{(Desigualdad de Poincaré clásica)}
   Sean $p\in [1,\infty)$ y $W_0^{1,p}(U)=\overline{C^{\infty}_c(U)}$ la cerradura del espacio de las funciones test, con respecto a la norma $\|.\|_{W^{1,p}(U)}$ definida como
   \[
     \|f\|_{W^{1,p}(U)}=\left(\|f\|_{L^{p}(U)}^p+\|Df\|_{L^p(U)}^p\right)^{1/p}, \qquad \text{ para } f\in W^{1,p}(U).
    \]
   Entonces si $U$ es un abierto acotado de $\R$, y $u\in W_0^{1,p}(U)$, se tiene la desigualdad 
   \[
   \|u\|_{L^{p}(U)}\leq C\|Du\|_{L^p(U)},   
   \]
   donde $C$ es una constante que solo depende de $p$ y $U$. 
   \end{teo}
   Esta cota nos da la posibilidad de controlar el crecimiento en $L^p(U)$de una función $u\in W_0^{k,p}$ vía la norma de su derivada $Du$ en $L^p(U)$.

   Más aún, gracias a la definición de $\|.\|_{W^{1,p}(U)}$, la cota anterior nos dice que si tenemos un subconjunto $U\subseteq \R$ abierto y acotado, en $W_0^{1,p}(U)$ las normas $\|\cdot\|_{W_{1,p}(U)}$ y $\|D\cdot\|_{L^p(U)}$ son equivalentes. Pasamos ahora a la versión de Cálculo de Malliavin de la desigualdad.
   \begin{teo} \textbf{(Desigualdad de Poincaré unidimensional)} Sea $N\sim Normal(0,1)$ y sea $f\in \D^{1,2}$. Entonces
       \[
       \text{Var}\left(f(N)\right)=\E\left[(Df(N))^2\right]    
       \]     
    \end{teo}
    \begin{proof} 
        Supongamos que $f\in \S$ y para $f\in \D^{1,2}$ utilizamos un argumento de aproximación. Dado que $f\in \S$, podemos intercambiar los símbolos de derivada e integral. Por lo tanto, tenemos que 
        \begin{align*}
            \text{Var}\left(f(N)\right)&=\E\left[f(N)(f(N)-\E\left[f(N)\right])\right]\\
            &=\E\left[f(N)(P_0f(N)-P_\infty)f(N)\right]\\
            &=-\int_{0}^{\infty}\E\left[f(N)\frac{d}{dt}P_tf(N)\right]dt\\
            &=\int_{0}^{\infty}\E\left[f(N)\delta DP_tf(N)\right]dt\\
            &=\int_{0}^{\infty}\E\left[f'(N)DP_tf(N)\right]dt\\
            &=\int_{0}^{\infty}e^{-t}\E\left[f'(N)P_tf'(N)\right]dt\\
            &\leq e^{-t}\sqrt{\E\left[f'^2(N)\right]}\sqrt{\E\left[(P_tf')^2(N)\right]}dt\\
            &\leq \E\left[f'^2(N)\right]\int_{0}^{\infty}e^{-t}dt\\
            &=\E\left[f'^2(N)\right].
        \end{align*} 
     \end{proof}
Nótese que esta desigualdad en términos de la norma $L^2(\gamma)$ se escribe como sigue:
\[
\|f\|^2_{L^2(\gamma)}\leq \E^2\left[f(N)\right]+\|Df\|^2_{L^2(\gamma)}, \qquad \forall f\in \D^{1,2},
\]
de manera que, si $\E\left[f(N)\right]=0$, directamente recuperamos la desigualdad de Poincaré clásica, aunque en distinto contexto. Y como tal, esta desigualdad nos permite controlar el crecimiento de las funciones $f\in \D^{1,2}$ utilizando la norma de su derivada de Malliavin.

Concluimos esta sección con la denominada \textit{relación de conmutatividad de Heisenberg}.
\begin{teo} 
 Para $f\in \S$, y $p\geq1$ se tiene que 
 \[
    (D\delta^p-\delta^p D)f=p\delta^{p-1}f.
 \]
 En particular para $p=1$, se tiene que
 \[
 (D\delta-\delta D)f=f.
 \]
 \end{teo}
\begin{proof} 
   Realizamos la prueba para el caso $p=1$. El caso general se sigue usando inducción. Sea $f\in \S$ y notemos que para cualquier $x\in \R$, 
\begin{align*}
   (D\delta-\delta D)f(x)&=D\delta f(x) - \delta D f(x)\\
   &=D \delta f(x) - (xDf(x)-D^2f(x))\\
   &=D \left(\delta f(x)+Df(x)\right) -xDf(x)\\
   &=D \left(xf(x)\right)-xDf(x)\\
   &=f(x)+xDf(x)-xDf(x)\\
   &=f(x),
\end{align*}
en donde en la segunda y cuarta igualdad hemos usado la fórmula del lema \ref{formuladelta}.
 \end{proof}
\section{Cálculo de Malliavin en el caso general.}

En esta sección se presentan los operadores de Malliavin generales, que serán utilizados en lo subsecuente. Seguimos de cerca los requerimientos de \cite[sección 2.1]{KUZGUN202268}. Dado que el caso unidimensional fue tratado con relativo detalle, y nuevamente teniendo en cuenta que un estudio exhaustivo de la disciplina no es el objetivo de este texto, las pruebas de los resultados que aparecen en el resto del capítulo serán omitidas, por lo que remitimos a \cite[capítulo 2]{Nourdin_Peccati_2012} para su consulta.

Para poder hablar del Cálculo de Malliavin en dimensiones infinitas, necesitamos un contexto adecuado. Parte de ese contexto es el concepto de \textit{proceso gaussiano isonormal}. En el ejemplo \ref{ruidoprocesoiso} del capítulo 1 hablamos del proceso gaussiano isonormal inducido por un ruido blanco. Sin embargo, este concepto puede extenderse mucho más allá. A continuación presentamos la definición de un proceso gaussiano isonormal en un contexto más general.

\begin{dfn} 
 Sean $(\mathfrak{H},\langle\cdot,\cdot\rangle_{\mathfrak{H}})$ un espacio de Hilbert separable sobre $\R$, y $(\Omega,\F,\P)$ un espacio de probabilidad. Un \textit{proceso gaussiano isonormal} sobre $\mathfrak{H}$ es un proceso gaussiano centrado $X=\{X(h):h\in \mathfrak{H}\}$ definido en $\Omega$, tal que para cualesquiera $g, h\in \mathfrak{H}$, 
 \[
 \E\left[X(g)X(h)\right]=\langle g,h\rangle_{\mathfrak{H}}.
 \]
 \end{dfn}
Dado un proceso gaussiano isonormal $X$, al ser sus elementos variables gaussianas centradas con densidad conjunta, la función de covarianzas (y por lo tanto todo el proceso) se codifica por medio del producto interno del espacio de Hilbert $\mathfrak{H}$. Es por lo tanto una manera de relacionar íntimamente dos estructuras distintas. De manera más precisa, consideremos un sistema ortonormal $(Z_n)_{n\geq1}$ en el espacio de Hilbert $\mathfrak{H}$. Sea $G=\{X(Z_n):n\geq1\}$ una familia de variables gaussianas independientes centradas en un espacio de probabilidad adecuado. Consideremos a $L^2(G)$ el subespacio lineal cerrado del $L^2(\P)$ generado por dichas variables. Entonces el mapeo $h\to X(h)$ resulta ser una isometría lineal del espacio $H$ en $L^2(G)$. En particular, dos procesos gaussianos isonormales definidos sobre el mismo espacio de Hilbert tendrán la misma ley.

Las ideas anteriormente expuestas son esencialmente la prueba de la existencia de un proceso isonormal dado un espacio de Hilbert $\mathfrak{H}$. Esto se establece en el siguiente resultado.

\begin{teo} 
Sea $(\mathfrak{H},\langle\cdot,\cdot\rangle_{\mathfrak{H}})$ un espacio de Hilbert real y separable. Entonces existe un proceso gaussiano isonormal sobre $\mathfrak{H}$.
\end{teo}

Dado que cualquier espacio de Hilbert puede ser la base para construir un proceso gaussiano isonormal, en preparación para el capítulo 4, podemos pensar en $\mathfrak{H}=L^2([0,\infty)\times \R)$, esto es, el espacio de todas las funciones de dos variables $f(t,x)$ que son Lebesgue cuadrado-integrables. Este espacio será el que usaremos en el capítulo cuatro para el resultado principal. Ahora denotaremos por $\S$ al siguiente conjunto de funciones:
\[
\S:=\left\{f(X(h_1),...,X(h_n)) \ : \ f\in C^{\infty}_p(\R^{n},\R), \ h_1,...,h_n\in \mathfrak{H} \right\},
\]
donde $X=\left\{X(h):h\in \mathfrak{H}\right\}$ es el proceso gaussiano isonormal sobre el cual estamos trabajando, y $C^{\infty}_p(\R^{n},\R)$ es el conjunto de funciones infinitamente derivables tales que tanto ella como sus derivadas parciales tienen crecimiento a lo más polinomial. Dicha clase $\S$ de funciones se conocen en general como \textit{funciones suaves}, y son el conjunto análogo a la clase de Schwarz que se manejó en la sección anterior en el caso unidimensional.

Presentamos a continuación una familia de espacios particular.

\begin{dfn} 
 Sea $(\Omega, \F, \P)$ un espacio de probabilidad y $(\mathfrak{H},\langle \cdot,\cdot\rangle_{\mathfrak{H}})$ un espacio de Hilbert real y separable. Definimos 
 \[
 L^q(\Omega;\mathfrak{H}):=\{Y(\omega)\in \mathfrak{H}:Y \text{ es }\F \text{-medible },\ \E\left[\norm{Y}_{\mathfrak{H}}^q\right]<\infty, \ \omega \in \Omega\}.
 \]
 \end{dfn}
 En otras palabras, $L^q(\Omega;\mathfrak{H})$ representa el conjunto de \textit{elementos aleatorios} $Y$ que toman valores en $\mathfrak{H}$, y que vistos como funciones de $(\Omega,\F)$ en $(\mathfrak{H},\B(\mathfrak{H}))$, son medibles, y cuya norma en $\mathfrak{H}$ tiene momentos de orden $q$ finitos. 

Llegados a este punto, estamos listos para definir la derivada de Malliavin en el caso general.
\begin{dfn} 
 Sea $F\in \S$ una función suave, y sea $p\geq1$ un entero. La derivada de Malliavin de orden $p$ de $F$ (con respecto al proceso isonormal $X$) es el elemento de $L^2(\Omega;\mathfrak{H}^{\odot})$ definido por 
 \[
 D^pF:=\sum_{i_1,...,i_p=1}^{n}\frac{\partial^pf}{\partial x_{i_1}...\partial x_{i_p}}(X(h_1),...,X(h_n))h_{i_1}\otimes...\otimes h_{i_p},
 \] 
 en donde $\mathfrak{H}^{\odot p}$ representa el \textit{producto tensorial} de orden $p$ del espacio de Hilbert $\mathfrak{H}$ consigo mismo (el cual nuevamente es un espacio de Hilbert). En el caso en que $p=1$ hablaremos simplemente de la \textit{derivada de Malliavin} de $F$ y la dentamos por $DF$.
 \end{dfn}
 \begin{obs}
   Nótese que para $F\in \S$ y $h\in \mathfrak{H}$ arbitrarios, al ser $DF\in L^2(\Omega;\mathfrak{H})$ una variable aleatoria con valores en $\mathfrak{H}$, 
   \[
   \langle DF,h\rangle_{\mathfrak{H}}=\lim_{\varepsilon\to 0} \frac{1}{\varepsilon}\left(f(X(h_1)+\varepsilon \langle h_1,h\rangle,...,X(h_n)+\varepsilon \langle h_n,h\rangle)-F\right),
   \]
   por lo que el producto interno entre $DF$ y $h$ se puede interpretar como \textit{una derivada direccional}. Por tal motivo, denotaremos tal producto interior como  
   \[
   D_hF:=\langle DF,h\rangle_{\mathfrak{H}}.
   \]
 \end{obs}
 La derivada de Malliavin, vista como operador entre espacios, también cumple la propiedad de ser cerrable, tal y como era de esperarse en concordancia con el caso unidimensional.
 \begin{teo} 
  Sea $q\in [1,\infty)$, y sea $p\geq 1$ un entero. Entonces el operador 
  \[
  D^{p}:\S\subseteq L^q(\Omega)\to L^q(\Omega;\mathfrak{H}^{\odot p})
  \]
  es cerrable.
  \end{teo}

En consecuencia, podemos replicar los pasos hechos para el caso unidimensional y obtener un espacio adecuado en donde dicho operador sea cerrado: para $q\in [1,\infty)$ y $p\geq1$ un entero, definimos la norma $\norm{\cdot}_{\D^{p,q}}:\S\to [0,\infty)$ dada por 
\[
\norm{F}_{\D^{p,q}}:= \left(\E\left[\abs{F}^{q}\right]+\E\left[\norm{DF}^{q}_{\mathfrak{H}}\right]+...+\E\left[\norm{D^pF}_{\mathfrak{H}^{\otimes p}}^q\right]\right)^{1/q}.
\]
y denotamos por $\D^{p,q}$ a la cerradura del espacio $\S$ con respecto a la norma anterior. 

Tal y como en el caso unidimensional, en el caso general se tiene un teorema análogo al teorema \ref{teocontencionessobolev}. Omitimos su enunciado y su prueba. También se tiene que para cualquier $p\geq1$, el espacio $\D^{p,2}$ es un espacio de Hilbert con producto interno análogo al del caso unidimensional.

Un resultado importante y que será usado en la posteridad es la \textit{regla de la cadena} para la derivada de Malliavin. Éste se encuentra en el siguiente teorema.

\begin{teo}[\textbf{Regla de la cadena}]\label{reglacadena} 
 Sea $\phi:\R^{m}\to \R$ una función $C^1$ con derivadas parciales acotadas. Supongamos que $F=(F_1,...,F_n)$ es un vector aleatorio cuyas componentes son elementos de $\D^{1,q}$ para algún $q\geq1$. Entonces $\phi(F)\in \D^{1,q}$ y 
 \[
D\phi(F)=\sum_{i=1}^n \frac{\partial \phi}{\partial x_i}(F)DF_i.
 \]
 \end{teo}
Pasamos ahora a estudiar el operador de divergencia. Fijamos $q=2$. Dado un entero $p\geq1$, definimos el operador de divergencia de orden $p$ como el operador adjunto del operador derivada $D^{p}:\D^{p,2}\to L^{2}(\Omega,\mathfrak{H}^{\odot p})$.

\begin{dfn} 
 Sea $p\geq1$ un entero. Denotamos por $\text{Dom}(\delta^p)$ al subconjunto de $L^2(\Omega;\mathfrak{H}^{\odot p})$ compuesto de aquellos elementos $u$ tales que existe una constante $C>0$ que satisface 
 \[
 \abs{\E\left[\langle D^pF,u\rangle_{\mathfrak{H}^{\otimes p}}\right]}\leq c\sqrt{\E\left[F^2\right]}, \qquad \forall F \in \S.
 \]
 \end{dfn}
 Argumentando de manera similar al caso unidimensional, por el teorema de representación de Riesz, existe un único elemento en $L^2(\Omega)$, denotado por $\delta^p(u)$ tal que cumple la relación 
 \[
 \E\left[\langle D^pF,u\rangle_{\mathfrak{H}^{\otimes p}}\right]=\E\left[F\delta^{p}(u)\right], \qquad \forall f\in \S.
 \]
 Llegamos así a la siguiente definición.

 \begin{dfn} 
  Si $u\in \text{Dom}(\delta^p)$, entonces $\delta^p(u)$ es el único elemento de $L^2(\Omega)$ caracterizado por la siguiente fórmula:
\[
 \E\left[\langle D^pF,u\rangle_{\mathfrak{H}^{\otimes p}}\right]=\E\left[F\delta^{p}(u)\right], \qquad \forall f\in \S.
 \]
  \end{dfn}
Este operador se denomina el operador de divergencia múltiple de orden $p$. Cuando $p=1$ simplemente lo llamamos el \textit{operador de divergencia},  escribiendo $\delta$ en lugar de $\delta^1$. La fórmula anterior se suele llamar \textit{fórmula de integración por partes}. Tal y como sucede en el caso unidimensional, al ser el operador de divergencia el adjunto del operador derivada $D^{p}$ y ser este último cerrado, se tiene que $\delta^{p}$ es un operador cerrado.

El siguiente resultado nos permite <<sacar>> una variable aleatoria escalar dentro del operador de divergencia $\delta$. Este resultado se puede consultar en \cite[proposición 2.5.4]{Nourdin_Peccati_2012}, sin embargo, enunciamos una versión general que se encuentra en \cite[lema 1]{Caballero1998-hz}.

\begin{teo}\label{factordeltafuera}
 Sean $p,p'>1$ tales que $\frac{1}{p}+\frac{1}{p'}=1$. Supongamos que $F\in \D^{1,p'}$, y sea $u \in \text{Dom}(\delta)$ tal que $u\in L^{p}(\Omega;\mathfrak{H})$ y $\delta(u)\in L^{p}(\Omega)$. Entonces $Fu\in \text{Dom}(\delta)$ y se cumple la igualdad 
 \begin{equation}\label{eqfactordeltafuera}
   \delta (Fu)=F\delta(u)-\langle DF,u\rangle_{\mathfrak{H}}.
\end{equation}
 
 \end{teo}

 Tenemos también la propiedad de conmutatividad de Heisenberg en este caso. Dicha propiedad es completamente análoga al caso unidimensional.

\begin{teo} 
Se cumple que 
\[
D\delta(u)-\delta(Du)=u.
\]
 \end{teo}

Múltiples propiedades con respecto al operador derivada y el operador de divergencia se pueden obtener, generalizando las propiedades del caso unidimensional. En particular, las versiones adecuadas del semigrupo de Ornstein-Uhlenbeck, y de su generador infinitesimal asociado, pueden ser enunciadas. No obstante, dado que para nuestros propósitos la teoría desarrollada hasta aquí es suficiente, remitimos \cite[capítulo 2]{Nourdin_Peccati_2012} para continuar el estudio de estos objetos. 

Cabe destacar que estaremos interesados en el caso en que el proceso gaussiano isonormal subyacente es aquél inducido por un proceso de ruido blanco gaussiano $(W_t(A))_{t\in [0\infty), \ A\in \B(\R)}$, el cual se puede construir a partir del espacio de Hilbert $\mathfrak{H}=L^{2}([0,\infty)\times \R)$. Este será nuestro contexto para el capítulo 4. Concluimos nuestro breve estudio de la disciplina con una propiedad muy interesante e importante del operador de divergencia: precisamente cuando estamos en el caso en que $\mathfrak{H}=L^2([0,\infty)\times\R)$, y el proceso isonormal subyacente es un proceso de ruido blanco gaussiano, el operador $\delta$ \textit{coincide} con la integral de Itô-Walsh en $[0,\infty)\times\R$.

\begin{teo} 
 Sea $u$ un proceso adaptado y cuadrado-integrable. Entonces $u\in \text{Dom}(\delta)$ y se cumple que 
 \[
 \delta(u)=\int_{[0,\infty)\times \R}^{}u(s,y)W(ds,dy).
 \]
 \end{teo}
 
 \chapter{Aplicaciones a Teoremas Límite y estudio de densidades}
En este capítulo se visita la aplicación de los conceptos del cálculo de Malliavin al estudio de Teoremas límite y al estudio de densidades. 
Una herramienta clave en esta tarea es el conocido como \textit{método de Stein}. 
Dicho método, desarrollado por Charles Stein en la década de 1970, es una herramienta bastante popular para evaluar la distancia entre las leyes de dos leyes de probabilidad. Lo anterior es posible lograrlo estudiando operadores diferenciales involucrados con las leyes de probabilidad en cuestión. La exposición de este capítulo está basado principalmente en \cite[capitulo 4]{Nourdin_Peccati_2012}. 

\section{Método de Stein}

Dentro del mundo de la probabilidad, la distribución normal juega un papel importante debido a la gran cantidad de propiedades que la misma presenta. Entre ellas, está la propiedad de ser una ley que está determinada por sus momentos. Esta propiedad, la cual está codificada en el conocido como \textit{lema de Stein}, forma parte importante de este texto. 

Comenzamos dando una prueba de que la ley de una variable gaussiana estándar está caracterizada por sus momentos. Recordemos que, dada una medida $\nu$ en $\R$, el momento de orden $n$ de la misma, cuando existe, está dado por 
\[
   m_n(\nu):=\int_{\R}x^{n}d\nu(x), \qquad n\geq0.
\]

\begin{lema} 
 Sea $\gamma$ la distribución gaussiana estándar. Dicha distribución está caracterizada por sus momentos.
 \end{lema}
 \begin{proof} 
    Debemos mostrar que, si existe otra medida definida en $\R$ con todos sus momentos definidos, digamos $\mu$, y  tales momentos coinciden con los de $\gamma$, entonces $\mu=\gamma$.

    Sea entonces $\mu$ una medida como antes. Es suficiente mostrar que la función característica de ambas medidas coincide en todo $\R$, esto es, que 
    \[
      \int_\R e^{itx}d\mu(x)=\int_\R e^{itx}d\gamma(x), \qquad \forall t\in \R.
    \]
    Utilizando la forma de Lagrange del residuo en el teorema de Taylor y la desigualdad de Cauchy-Schwarz, tenemos para cualquier $n\geq1$ que
    \begin{align*}
      \abs{\int_{\R}e^{itx}d\nu(x)-\int_\R e^{itx}d\gamma(x)}&\leq \int_\R \abs{e^{itx}-\sum_{k=0}^{n}\frac{(itx)^k}{k!}}d\nu(x)+\int_\R \abs{e^{itx}-\sum_{k=0}^{n}\frac{(itx)^k}{k!}}d\gamma(x)\\
      &=\int_\R \abs{\frac{(it)^{n+1}e^{it\xi_L}x^{n+1}}{(n+1)!}}d\nu(x)+\int_\R \abs{\frac{(it)^{n+1}e^{it\eta_L}x^{n+1}}{(n+1)!}}d\gamma(x)\\
      &\leq\int_\R \frac{\abs{tx}^{n+1}}{(n+1)!}d\nu(x)+\int_\R \frac{\abs{tx}^{n+1}}{(n+1)!}d\gamma(x)\\
      &\leq \left(\int_{\R}\frac{\abs{tx}^{2n+2}}{(n+1)!^2}d\nu(x)\right)^{1/2}+\left(\int_{\R}\frac{\abs{tx}^{2n+2}}{(n+1)!^2}d\gamma(x)\right)^{1/2}\\
      &=\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\nu)\right)^{1/2}+\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\gamma)\right)^{1/2}\\
      &=2\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\gamma)\right)^{1/2}.
    \end{align*}
   Recordando que para la distribución gaussiana se cumple que $m_{2n}(\gamma)=\frac{(2n)!}{2^{n}n!}$, y usando cotas superior e inferior de la fórmula de Stirling, tenemos que 
   \begin{align*}
      2\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\gamma)\right)^{1/2}&=2|t|^{n+1}\left(\frac{(2n+2)!}{2^{n+1}(n+1)!^3}\right)^{1/2}\\
      &\leq 2|t|^{n+1}\left(\frac{\sqrt{4\pi(n+1)}\left(\tfrac{2(n+1)}{e}\right)^{2n+2}e^{-12\cdot 2(n+1)}}{2^{n+1}\left(\sqrt{2\pi(n+1)}\left(\tfrac{n+1}{e}\right)^{n+1}e^{-12(n+1)+1}\right)^{3}}\right)^{1/2}\\
      &\leq 2|t|^{n+1}\left(\frac{2^ne^{13(n+1)+3}}{\pi(n+1)^{n+2}}\right)^{1/2}\\
      &=\frac{K_t^{\tfrac{13}{2}(n+1)}}{(n+1)^{n/2+1}},\\
   \end{align*} 
   donde $K_t$ es una constante dependiente sólo de $t$. Pero esta última expresión tiende a cero cuando $n$ tiende a infinito. Concluimos que las funciones características de ambas medidas coinciden y por lo tanto, son equivalentes.
  \end{proof}

Ahora enunciamos y demostramos el conocido como \textit{lema de Stein}.
\begin{teo}\textbf{(Lema de Stein)} \label{LemaStein} Sea $X$ una variable aleatoria. $X$ tiene una distribución normal estándar si y sólo sí, para cualquier función $f$ derivable y tal que $f'\in L^{1}(\gamma)$, se tiene que
   \begin{enumerate}
       \item $\E\left[Xf(X)\right]<\infty \ $ y $ \ \E\left[f'(X)\right]<\infty$.
       \item $\E\left[Xf(X)\right]=\E\left[f'(X)\right]$.
   \end{enumerate}
\end{teo}
\begin{proof} 
  Si directamente $X\sim$ Normal$(0,1)$, cualquier función que cumpla las condiciones del enunciado cumple las del lema \ref{lema1}, el cual precisamente estipula la relación anterior. 

  Supongamos ahora que $X$ satisface la relación $\E\left[Xf(X)\right]=\E\left[f'(X)\right]$ para cualquier función $f$ con las propiedades del enunciado. Si definimos $f_n(x)=x^n$ para cualquier $n\geq1$, tenemos que la variable $X$ tiene todos sus momentos definidos, y además 
  \[
   \E\left[X^{n+1}\right]=\E\left[Xf(X)\right]=\E\left[f'(X)\right]=n\E\left[X^{n-1}\right],
  \] 
  de tal forma que los momentos de la variable $X$ coinciden con los de una variable normal estándar. Dado que la distribución normal está caracterizada por sus momentos según vimos en el resultado previo, se sigue que $X\sim$ Normal$(0,1)$.
 \end{proof}
El resultado anterior es interesante por varios motivos. Uno de ellos es que nos permite caracterizar plenamente la distribución normal a partir de funciones y sus derivadas aplicadas a una variable aleatoria que siga dicha distribución. Por otro lado, un instante de reflexión nos puede llevar a la siguiente pregunta.

Supongamos que ahora $X$ es una variable aleatoria y que para una cierta clase suficientemente rica de funciones suaves, se cumple que 
\[
\E\left[Xf(X)-f'(X)\right]\approx0.    
\]
En virtud del lema de Stein, ¿es posible decir que la distribución de $X$ está cerca (en algún sentido) de la distribución normal estándar?

La pregunta anterior se conoce como la \textit{heurística de Stein}, y la respuesta es afirmativa en algunos casos. Para llegar a una formulación explícita de dicha respuesta, debemos introducir la idea de \textit{distancia entre dos medidas de probabilidad} y también la famosa \textit{ecuación de Stein} asociada a alguna función $h\in L^1(\gamma)$. 

Comenzamos definiendo la noción de distancia entre dos variables aleatorias. La definición de distancia se otorga para variables que tomen valores en $\R^d$. Recordamos la noción de \textit{clase separante de funciones}.

\begin{dfn} 
 En un espacio de probabilidad $(\Omega, \F, \P)$, sea $\mathscr{H}$ un conjunto de funciones borelianas complejas con valores en $\R^d$, para algún $d\geq1$. Decimos que $\mathscr{H}$ es una familia de \textit{funciones separantes} si para cualesquiera dos variables aleatorias $F$ y $G$ con valores en $\R^{d}$ tales que $h(F)$ y $h(G)$ son elementos de $L^1(\Omega)$ y $\E\left[h(F)\right]=\E\left[h(G)\right]$, para cualquier elemento $h\in \mathscr{H}$ implica que $F$ y $G$ tienen la misma ley.
 \end{dfn}
Ejemplos clásicos de familias $\mathscr{H}$ separantes son el conjunto de funciones borelianas continuas y acotadas, el conjunto de funciones indicadoras de los borelianos de $\R^{d}$, o el conjunto de funciones exponenciales de la forma $\mathscr{H}=\left\{e^{i \langle v,\cdot\rangle}: v\in \R^{d} \right\}$, donde el producto interno anterior es el producto interno usual de $\R^{d}$.

A continuación, hacemos rigurosa la idea de distancia entre las leyes de dos variables aleatorias con la siguiente definición.
\begin{dfn} 
 Sea $\mathscr{H}$ una clase separante de funciones, y sean $F$ y $G$ dos variables aleatorias con valores en $\R^{d}$ y tales que $h(F)$ y $h(g)$ están en $L^1(\Omega)$ para cualquier $h\in \mathscr{H}$. Definimos la distancia inducida por $\mathscr{H}$ entre las distribuciones de $F$ y $G$ como 
 \[
 d_\mathscr{H}(F,G):=\sup_{h\in \mathscr{H}}\left\{\abs{\E\left[h(F)\right]-\E\left[h(G)\right]}\right\}.
 \]
 \end{dfn}
Es sencillo demostrar que para una clase de funciones separantes $\mathscr{H}$, el mapeo $d_\mathscr{H}$ definido anteriormente en efecto define una métrica en cierto subconjunto de las medidas de probabilidad sobre $\R^{d}$. Es pertinente hacer hincapié en que la distancia inducida entre las leyes de variables depende de la clase $\mathscr{H}$. Ello resulta en una amplia variedad de opciones con las cuales considerar la distancia entre dos distribuciones de probabilidad. Algunas de las más usadas se encuentran a continuación.

\begin{itemize}
   \item \textbf{La distancia de Kolmogorov.} Esta distancia entre dos variables $F$ y $G$, denotada por $d_{\text{Kol}}(F,G)$, se obtiene utilizando a la familia \[\mathscr{H}= \left\{h:\R^d\to \R \ :\ h(x_1,...,x_d)=\1_{(-\infty,z_1]}(x_d)\cdot ... \cdot \1_{(-\infty,z_d]}(x_d), \ \  z_1,...,z_d\in \R\right\},\]
   de manera que dicha distancia está dada por la expresión
   \[
   d_{\text{Kol}}(F,G)=\sup_{z_1,...,z_d\in \R}\abs{\P(F\in (-\infty,z_1]\times...\times(-\infty,z_d])-\P(G\in (-\infty,z_1]\times...\times(-\infty,z_d])}.
   \]
   \item \textbf{La distancia de variación total.} Dicha distancia entre las leyes de las variables $F$ y $G$ se define utilizando la clase \[\mathscr{H}=\left\{h:\R^d\to \R \ :\ h(x_1,...,x_d)=\1_B(x_1,...,x_d), \ \ B\in \B(\R^{d})\right\},\] 
   y dicha distancia se denota por $d_{\text{TV}}(F,G).$ Su expresión explícita es 
   \[
      d_{\text{TV}}(F,G)= \sup_{B\in \B(\R^{d})}\abs{\P(F\in B)-\P(G\in B)}.
   \]
   Observamos directamente que $d_{\text{Kol}}(F,G)\leq d_{\text{TV}}(F,G)$.
   \item \textbf{La distancia de Wasserstein.} Usando la clase de funciones separantes \[\mathscr{H}=\left\{h:\R^{d}\to \R \ :\ \norm{h}_{\text{Lip}}\leq 1\right\},\] donde claramente $\norm{\cdot}_{Lip}$ denota la norma Lipschitz en $\R^{d}$. Esta distancia se denota como $d_{\text{W}}(F,G)$.
\end{itemize}

Armados con la noción de distancia entre dos leyes de probabilidad, pasamos ahora a estudiar brevemente las ecuaciones de Stein. Dichas ecuaciones nos permiten codificar la distancia entre la ley de una variable $F$ y una variable aleatoria normal $N$, en virtud del lema de Stein \ref{LemaStein}. De manera rigurosa, tenemos la siguiente definición.

\begin{dfn} 
 Sea $N$ una variable gaussiana estándar y sea $h:\R\to\R$ una función boreliana tal que $\E\left[h(N)\right]<\infty$. La \textit{ecuación de Stein} asociada con $h$ es la ecuación diferencial ordinaria 
 \begin{equation}\label{eqstein}
   f'(x)-xf(x)=h(x)-\E\left[h(N)\right].
 \end{equation}
 Diremos que $f$ es una solución a la ecuación \eqref{eqstein} si dicha función es absolutamente continua y tal que es igual Lebesgue casi en todas partes a la derivada $f'$ que satisface \eqref{eqstein} para cualquier $x\in \R$.
 \end{dfn}

 Es de notar que dicha ecuación diferencial es sencilla de resolver. Este es el contenido del siguiente resultado.

 \begin{teo}\label{soleqstein}
  Las soluciones a la ecuación \eqref{eqstein} son de la forma 
  \[
  f(x)=ce^{x^2/2}+e^{x^2/2}\int_{-\infty}^x(h(y)-\E\left[h(N)\right])e^{-y^2/2}dy, \qquad \forall x\in \R,
  \] 
  donde $c\in \R$ es constante. En particular, si escribimos 
  \[
   f_h(x):=e^{x^2/2}\int_{-\infty}^x(h(y)-\E\left[h(N)\right])e^{-y^2/2}dy,
  \]
  entonces $f_h$ es la única solución a la ecuación de Stein \eqref{eqstein} que satisface la condición 
  \[
  \lim_{x\to \infty}e^{-x^2/2}f(x)=0.
  \]
  \end{teo}
  \begin{proof} 
    Supongamos que $h$ cumple las hipótesis del enunciado. Partiendo de las igualdades 
    \[
    \frac{d}{dx}\left(e^{-x^2/2}f(x)\right)=-xe^{-x^2/2}f(x)+e^{-x^2/2}f'(x)=e^{-x^2/2}(f'(x)-xf(x)),
    \]
    la ecuación de Stein $\eqref{eqstein}$ se puede reformular como 
    \[
      e^{x^2/2}\frac{d}{dx}\left(e^{-x^2/2}f(x)\right)=h(x)-\E\left[h(N)\right].
    \] 
    Usando el método de factor integrante, es claro que las soluciones a la ecuación  \eqref{eqstein} tienen la forma 
    \begin{equation*}
      f(x)=ce^{x^2/2}+e^{x^2/2}\int_{-\infty}^{x}(h(y)-\E\left[h(N)\right])e^{-y^2/2}dy, \qquad \forall x\in \R.
    \end{equation*}
   Para el comportamiento en $\pm\infty$ de las solución $f_h$, usando la función $g(x)=\abs{h(y)-\E\left[h(N)\right]}e^{-y^2/2}$, la cual es integrable e independiente de $x\in \R$, por el teorema de convergencia dominada, tenemos para el límite en $+\infty$ que
   \begin{align*}
      \lim_{x\to \infty}\int_{-\infty}^x \left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}dy&=\int_\R\lim_{x\to\infty}\left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}\1_{(-\infty,x]}(y)dy\\
      &=\int_\R h(y)e^{-y^2/2}dy-\E\left[h(N)\right]\int_\R e^{-y^2/2}dy\\
      &=\sqrt{2\pi}\E\left[h(N)\right]-\sqrt{2\pi}\E\left[h(N)\right]\\
      &=0,
      \end{align*}
      mientras que para el límite en $-\infty$ tenemos que 
      \[
         \lim_{x\to -\infty}\int_{-\infty}^x \left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}dy=\int_\R\lim_{x\to-\infty}\left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}\1_{(-\infty,x]}(y)dy=0.
      \]
      Se sigue entonces que si $f$ es una solución como las anteriormente mencionadas, entonces $e^{-x^2/2}f(x)\xrightarrow[x\to\pm\infty]{}0$ si y solo si $c=0$.
   \end{proof}

 Contando ahora con estas herramientas, podemos evaluar de manera rigurosa la distancia que hay entre la ley de una variable $X$ y una variable normal estándar $N$. Para ello, consideremos una función $h:\R\to\R$ que cumple que $\E\left[\abs{h(X)}\right]<\infty$ y $\E\left[\abs{h(N)}\right]<\infty$. Sea $f_h$ la solución única del teorema \ref{soleqstein}. Tomando esperanza a ambos lados de la ecuación de Stein, tenemos que $f_h$ cumple que 
\begin{equation}\label{eqmetodostein}
   \E\left[h(X)\right]-\E\left[h(N)\right]=\E\left[f'_h(X)-Xf_h(X)\right].
 \end{equation}
 
 En particular, notamos que para una clase de funciones adecuada $\mathscr{H}$, 
  \begin{equation*}
   d_\mathscr{H}(X,N)=\sup_{h\in \mathscr{H}}\abs{\E\left[h(X)\right]-\E\left[h(N)\right]}=\sup_{h\in \mathscr{H}}\abs{\E\left[f_h'(X)-Xf_h(X)\right]}.
   \end{equation*}
 Es de destacar que en la expresión anterior, la distancia entre las leyes de las variables $X$ y $N$ están codificadas en la cantidad de la derecha, la cual no involucra la ley gaussiana estándar de manera directa. Por otro lado, es deseable desarrollar algunas cotas para la norma de las soluciones $f_h$, dependiendo del tipo de funciones $h\in\mathscr{H}$ que estemos considerando. Dependiendo de dicha clase, podremos obtener distintas cotas para las distancias en las que estemos interesados. A dichas cotas se les conoce como \textit{cotas de Stein}.
 
 En nuestro caso, nos vamos a enfocar en dos familias de funciones distintas. La primera de ellas es la familia $\mathscr{H}$ que origina la distancia de Kolmogorov, mientras que en el segundo caso consideramos una familia específica que nos será de utilidad en el capítulo 4.
 
En cuanto a la distancia de Kolmogorov, tenemos el siguiente teorema que nos da una cota universal explícita de las normas de las soluciones a la ecuación de Stein asociada a la familia $\mathscr{H}= \left\{h:\R\to \R \ :\ h(x)=\1_{(-\infty,z]}(x),\ \  z\in \R\right\}$.
  
 \begin{teo}\label{cotakolmogorovstein}
   Sea $z\in \R$ y consideremos a la función $h=\1_{(-\infty,z]}\in \mathscr{H}$ asociada. Denotemos por $f_z$ a la única solución de la ecuación de Stein asociada a la función $h$ anterior. Se tiene la siguiente cota.
  \[
      \norm{f_z}_{\infty}\leq \frac{\sqrt{2\pi}}{4} \qquad \text{y} \qquad \norm{f'_z}_\infty\leq 1.
  \]
En particular, para $N$ una variable aleatoria normal estándar, y para cualquier variable integrable $X$, se tiene que 
\[
   d_{\text{Kol}}(X,N)\leq \sup_{f\in \F_{Kol}}\abs{\E\left[f'(X)\right]-\E\left[Xf(X)\right]},
\]
en donde 
\[\F_{\text{Kol}}:=\left\{f:\R\to\R \text{ abs. cont.}: f \text{ derivable salvo un conjunto finito}, \norm{f}_\infty\leq \frac{\sqrt{2\pi}}{4}, \ \norm{f'}_\infty\leq 1\right\},\]
y donde el supremo anterior se entiende como la cantidad 
\[
\sup \abs{\E\left[g(X)\right]-\E\left[Xf(X)\right]},
\]
donde el supremo se toma sobre todos los pares de funciones borelianas $(f,g)$, con $f$ derivable salvo un conjunto finito de puntos, y $g$ es una versión de $f'$ tal que $\norm{g}_\infty\leq 1$.
\end{teo}

Una guía para la demostración de este resultado se puede encontrar en la sección 3.4 de \cite{Nourdin_Peccati_2012}. Concluimos esta sección con el siguiente resultado, el cual nos da cotas explícitas para una familia de funciones específicas.

\begin{teo}\label{metodosteinfamespecial}
 Consideremos la familia de funciones $\mathscr{H}=\left\{h:\R\to\R : h(z)=\1_{\{z>x\}}z, \ x\in \R\right\}$. Entonces la solución $f_h$ a la ecuación de Stein \eqref{eqstein} satisface 
 \[
 |f'_h(x)|\leq C(\abs{x}+1), \qquad \forall x\in \R,
 \]
 en donde $C$ es una constante.
 \end{teo}
Nuevamente este resultado se enuncia sin demostración. Una versión general del mismo, así como su demostración, se encuentran en el apéndice A de \cite{HU2014814}.

Los dos resultados anteriores se encuentran en el núcleo de las técnicas que se conocen en su conjunto como \textit{método de Stein.}, y que son ampliamente usadas para deducir resultados similares al teorema central del límite, además de deducir tasas de convergencia para los mismos. Ejemplo de lo anterior es el célebre teorema de Berry-Essen, el cual cuantifica la velocidad de convergencia de una sucesión i.i.d de variables con media cero, varianza 1 y tercer momento absoluto finito, a la distribución normal estándar. Es decir, es una versión cuantificable del clásico teorema del límite central.

Es posible otorgar una demostración del mismo utilizando el método de Stein, y más específicamente el teorema \ref{cotakolmogorovstein}. Para el enunciado y demostración del mismo, remitimos a \cite[sección 3.7]{Nourdin_Peccati_2012}.

\section{Existencia de densidades y estimación de la distancia uniforme}

En preparación para el capítulo 4, en esta sección estudiamos brevemente un par de resultados. El primero de ellos nos garantiza la existencia y unicidad de la densidad de una variable aleatoria unidimensional bajo ciertas hipótesis, además de proveer una fórmula para la misma. También enunciamos un resultado que consiste en una cota para la distancia uniforme entre la densidad de una variable aleatoria y la densidad de una variable normal estándar, siendo este último un resultado medular en el capítulo 4. Es de destacar que estos resultados son consecuencias del teorema \ref{factordeltafuera} y ambos, por orden de aparición, se pueden encontrar en \cite[proposición 1]{Caballero1998-hz} y en \cite[teorema 3.2]{KUZGUN202268}.

Comenzamos directamente con el primer resultado. La demostración del mismo se omite en este texto.
\begin{teo}\label{existdensidad}
 Sean $F$ una variable aleatoria en el espacio $\D^{1,1}$ y $\mathfrak{H}$ un espacio de Hilbert. Supongamos que $u$ es una variable en $L^{1}(\Omega;\mathfrak{H})$ tal que $D_uF\neq 0$ casi seguramente. Supongamos también que $\tfrac{u}{D_uF}\in \text{Dom}(\delta)$. Entonces la ley de $F$ tiene una densidad continua y acotada dada por 
   \begin{equation}\label{formuladensidad}
      f_F(x)=\E\left[\1_{\{F>x\}}\delta \left(\frac{u}{D_uF}\right)\right].
   \end{equation}
 \end{teo}
Es de nuestro interés saber en qué caso es cierto que $\tfrac{u}{D_uF}\in \text{Dom}(\delta)$. El siguiente lema nos da una respuesta.

\begin{lema}\label{lemaexistdensidad}
 Sean $p,p'>1$ tales que $\frac{1}{p}+\frac{1}{p'}=1$, y sea $F\in \D^{2,1}$. Supongamos que $u\in \text{Dom}(\delta)$. Si se cumplen las siguientes condiciones 
 \begin{enumerate}
   \item $u\in L^{p}(\Omega;\mathfrak{H})$,
   \item $\delta(u)\in L^{p}(\Omega)$,
   \item $(D_uF)^{-1}\in L^{p'}(\Omega)$,
   \item $(D_uF)^{-2}\left(\norm{D^2F}_{\mathfrak{H}^{\otimes 2}}\norm{u}_{\mathfrak{H}}+\norm{Du}_{\mathfrak{H}}\norm{DF}_{\mathfrak{H}}\right)\in L^{p'}(\Omega)$,
 \end{enumerate}
 entonces $\tfrac{u}{D_uF}\in \text{Dom}(\delta)$ y en particular, se cumple la ecuación \eqref{eqfactordeltafuera}. 
 \end{lema}
 Para una prueba de este lema, se puede consultar \cite[lema 3, lema 4]{Caballero1998-hz}.

Estamos interesados en aplicar tanto el lema anterior como el teorema \ref{existdensidad} en condiciones que aparecerán en el capítulo 4. Por lo tanto, y siguiendo de cerca a \cite[sección 3]{KUZGUN202268}, tenemos el siguiente resultado. 

\begin{teo}\label{teocotafundamental}
 Sean $u\in \D^{1,6}(\Omega;\mathcal{\mathfrak{H}})$ y definamos la variable $F:=\delta(u)$. Supongamos que $F\in \D^{2,6}$, con $\E\left[F\right]=0, \ \E\left[F^{2}\right]=1$ y $\left(D_uF\right)^{-1}\in L^4(\Omega)$. Entonces $u/D_uF \in \text{Dom}(\delta)$, la variable $F$ admite una densidad continua y acotada $f_F(x)$ y la misma cumple la siguiente desigualdad
 \begin{equation}\label{cotafundamental}
    \sup_{x\in \R} \abs{f_F(x)-\Phi(x)}\leq \left(\|F\|_4\|\left(D_vF\right)^{-1}\|_4+2\right)\|1-D_vF\|_2+\|\left(D_vF\right)^{-1}\|^{2}_4\|D_v \left(D_vF\right)\|_2,
 \end{equation}
donde $\Phi(x)$ es la función de densidad de una variable normal estándar.

\end{teo}
\begin{proof} 
   Demostramos primero que $\frac{u}{D_uF}\in \text{Dom}(\delta)$. Buscamos satisfacer las hipótesis del lema \ref{lemaexistdensidad}. Para ello usamos $p=p'=2$. Claramente $p,p'>1$ y se cumple que $\frac{1}{p}+\frac{1}{p'}=1$. Por otro lado, por hipótesis $u\in \text{Dom}(\delta)$ y además, dado que en nuestro caso la variable $F$ es por definición la divergencia de $u$, de la hipótesis y de la contención entre los espacios $\D^{k,q}$ se sigue que $F\in \D^{2,6}\subseteq\D^{2,1}$. Resta mostrar que las cuatro condiciones del lema \ref{lemaexistdensidad} se satisfacen.
   \begin{enumerate}
      \item Dado que $u\in \D^{1,6}(\Omega;\mathfrak{H})\subseteq\D^{1,2}(\Omega;\mathfrak{H})$, y que por construcción este último espacio es un subconjunto de $L^{2}(\Omega;\mathfrak{H})$, se cumple la primera condición.
      \item Por hipótesis $\delta(u)=F\in \D^{2,6}$, y este es subconjunto de $L^{2}(\Omega)$, por lo que la segunda condición también se cumple.
      \item El enunciado nos indica que $(D_uF)^{-1}\in L^4(\Omega)$. Como la medida del espacio es finita, se da la contención $L^4(\Omega)\subseteq L^2(\Omega)$ y la tercera condición se satisface.
      \item Denotemos $a:=\left(\norm{D^2F}_{\mathfrak{H}^{\otimes 2}}\norm{u}_{\mathfrak{H}}+\norm{Du}_{\mathfrak{H}}\norm{DF}_{\mathfrak{H}}\right)$. Obsérvese que dichas cantidades están bien definidas gracias a las hipótesis. Notamos que
      \[
      \norm{a(D_uF)^{-2}}_2^2=\int_\Omega (a(D_uF)^{-2})^2d\P=a^2\int_\Omega ((D_uF)^{-1})^4d\P=a^2\norm{(D_uF)^{-1}}_4^{4}<\infty.
      \] 
      Concluimos que la última condición se satisface.   
   \end{enumerate}
   Por lo tanto, $u/D_uF\in \text{Dom}(\delta)$. Ahora bien, el resto de hipótesis del teorema \ref{existdensidad} se satisfacen, pues se tiene que $F\in \D^{2,6}\subseteq \D^{1,1}$. Además, $u\in \D^{1,6}\subseteq \D^{1,1}$ y este último es subconjunto de $L^{1}(\Omega;\mathfrak{H})$. Finalmente si fuera el caso que $D_uF=0$ en un conjunto medible de probabilidad positiva $A\subseteq \Omega$, tendríamos que 
   \[
   \norm{(D_uF)^{-1}}_1=\int_\Omega \abs{(D_uF)}^{-1}d\P\geq\int_A\abs{(D_uF)}^{-1}d\P=+\infty,
   \]
   lo cual es una contradicción ya que $(D_uF)^{-1}\in L^{4}(\Omega)$, que es subconjunto de $L^{1}(\Omega)$.

   Satisfechas las hipótesis del teorema \ref{existdensidad}, tenemos garantizada la existencia de una densidad continua y acotada para la variable $F=\delta(v)$, la cual está dada por la fórmula \eqref{formuladensidad}. Ahora buscamos una cota uniforme sobre $x\in \R$ para las siguientes cantidades
   \begin{equation}\label{eqdiferencia1}
   \abs{f_F(x)-\phi(x)}=\abs{\E\left[\1_{\{F>x\}}\delta\left(\frac{u}{D_uF}\right)\right]-\E\left[\1_{\{N>x\}}N\right]}, 
   \end{equation}
  
  donde $N\sim$ Normal$(0,1)$.
   En virtud de que se satisfacen las hipótesis del lema \ref{existdensidad}, en la fórmula \eqref{formuladensidad} podemos usar la ecuación \eqref{eqfactordeltafuera} y obtenemos que, para $x\in \R$ arbitrario y omitiendo el subíndice $\mathfrak{H}$ de los productos interiores,
   \begin{align*}
   f_F(x)=\E\left[\1_{\{F>x\}}\delta\left(\frac{u}{D_uF}\right)\right]&=\E\left[\1_{\{F>x\}}\left(\frac{1}{D_{u}F}\delta(u)-\Big\langle D \left(\frac{1}{D_{u}F}\right),u \Big\rangle \right)\right]\\
   &=\E\left[\1_{\{F>x\}}\frac{F}{D_{u}F}\right]+\E\left[\1_{\{F>x\}}\left(\frac{1}{\left(D_{u}F\right)^2}\right)D_{u}D_{u}F\right],
      \end{align*}
      donde hemos hecho uso de que $F=\delta(u)$ y de la regla de la cadena en la segunda igualdad.
  Incrustando esta última igualdad en la fórmula \eqref{eqdiferencia1}, y subsecuentemente sumando y restando el término $\E\left[\1_{\{F>x\}}F\right]$,
  
\begin{align}\label{eqdiferencia2}
   \abs{f_F(x)-\Phi(x)}&=\abs{\E\left[\1_{\{F>x\}}\frac{F}{D_{u}F}\right]+\E\left[\1_{\{F>x\}}\frac{D_uD_uF}{\left(D_{u}F\right)^2}\right]-\E\left[\1_{\{N>x\}}N\right]}\notag\\
   &=+\Bigg|\E\left[\1_{\{F>x\}}\frac{F}{D_{u}F}\right]-\E\left[\1_{\{F>x\}}F\right]+\E\left[\1_{\{F>x\}}\frac{D_uD_uF}{\left(D_{u}F\right)^2}\right]\notag\\
   &\ \ \ +\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]\Bigg|\notag\\
   &\leq \abs{\E\left[\1_{\{F>x\}}\left(\frac{F}{D_uF}-F\frac{D_uF}{D_uF}\right)\right]}+\abs{\E\left[\1_{\{F>x\}}\frac{D_uD_uF}{\left(D_{u}F\right)^2}\right]}\notag\\
   &\ \ \ +\abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}\notag\\
   &\leq \E\left[\abs{\frac{F(1-D_uF)}{D_uF}}\right]+\E\left[\frac{\abs{D_uD_uF}}{(D_uF)^2}\right]+\abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}.
\end{align}
Ahora, en la desigualdad \eqref{eqdiferencia2} usamos sucesivamente la desigualdad de Hölder en el primer y segundo términos, de manera que
\begin{equation}\label{eqdiferencia3}
   \E\left[\abs{\frac{F(1-D_uF)}{D_uF}}\right]\leq \norm{F(D_uF)}_2\norm{1-D_uF}_2\leq \norm{F}_4\norm{(D_vF)^{-1}}_4\norm{1-D_uF}_2,
\end{equation}
y también
\begin{equation}\label{eqdiferencia4}
   \E\left[\frac{\abs{D_uD_uF}}{(D_uF)^2}\right]\leq \norm{D_uD_uF}_2\norm{(D_uF)^{-2}}_2=\norm{D_uD_uF}_2\norm{(D_uF)^{-1}}_4^{2}.
\end{equation}
Finalmente, estudiamos el tercer término de la desigualdad \eqref{eqdiferencia2}. Aquí es donde utilizamos el método de Stein. Observemos que, para $x\in \R$,
\[
   \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}=\abs{\E\left[h(F)\right]-\E\left[h(N)\right]},
\]
donde $h(z)=\1_{\{z>x\}}z$ es una función perteneciente a la familia $\mathscr{H}$ correspondiente al teorema \ref{metodosteinfamespecial}. En virtud de la ecuación \eqref{eqmetodostein}, la igualdad anterior se convierte en
\begin{equation}\label{eqdiferencia5}
   \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}=\abs{\E\left[f'_h(F)\right]-\E\left[Ff_h(F)\right]}.
\end{equation}
En este punto usamos nuevamente que $F=\delta(u)$, y como tal, cumple la fórmula de integración por partes. Por lo tanto, tomando como variable a $f_h(F)$ y usando regla de la cadena para la derivada de Malliavin, se tienen las siguientes igualdades
\begin{equation*}
   \E\left[Ff_h(F)\right]=\E\left[\delta(u)f_h(F)\right]=\E\left[\langle D(f_h(F)),u\rangle\right]=\E\left[f'_h(F)\langle DF,u\rangle\right]=\E\left[f'_h(F)D_uF\right].
\end{equation*}
Insertando la igualdad anterior en \eqref{eqdiferencia5} se sigue que 
   \begin{equation*}
      \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}=\abs{\E\left[f'_h(F)\right]-\E\left[f'_h(F)D_uF\right]}= \abs{\E\left[f_h'(F)(1-D_vF)\right]}.
   \end{equation*}
Usamos nuevamente la desigualdad de Hölder en el último termino de la ecuación anterior, obtenemos
\begin{equation}\label{eqdiferencia6}
   \abs{\E\left[f_h'(F)(1-D_vF)\right]}\leq \norm{f_h'(F)}_2\norm{1-D_vF}_2.
\end{equation}
Finalmente, usamos el teorema \ref{metodosteinfamespecial}, en \ref{eqdiferencia6}, por lo que 
\begin{equation}\label{eqdiferencia7}
      \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}\leq C(\abs{F}+1)\norm{1-D_vF}_2
\end{equation}
Finalmente, tenemos que $|F|\leq 1$ y $C\leq 1$, por lo que \eqref{eqdiferencia7} se convierte en 
\begin{equation}\label{eqdiferencia8}
   \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}\leq2\norm{1-D_vF}.
\end{equation}
Combinando las desigualdades \eqref{eqdiferencia3}, \eqref{eqdiferencia4} y \eqref{eqdiferencia7}, tenemos para $x\in \R$,
\begin{align*}
   \abs{f_F(x)-\Phi(x)}&\leq \norm{F}_4\norm{(D_vF)^{-1}}_4\norm{1-D_uF}_2+\norm{D_uD_uF}_2\norm{(D_uF)^{-1}}_4^{2}+2\norm{1-D_vF}\\
   &=\left(\|F\|_4\|\left(D_vF\right)^{-1}\|_4+2\right)\|1-D_vF\|_2+\|\left(D_vF\right)^{-1}\|_4^{2}\|D_v \left(D_vF\right)\|_2.   
\end{align*}

Tomando supremo obtenemos la desigualdad \eqref{cotafundamental}.
\end{proof}

Tal y como se comentó al inicio de la sección, el resultado anterior resulta ser la cota principal desde la cual se deducirá la tasa de convergencia de las densidades de los promedios espaciales de la solución al problema del calor \eqref{shedefinitiva}, a la densidad normal. 

Hasta aquí llega nuestro estudio del método de Stein y de la existencia de la densidad de una variable aleatoria bajo ciertas condiciones de la misma. Es de destacar que el estudio realizado es solamente una pequeñísima parte de la vanguardia de ambos temas. Por un lado las ecuaciones de Stein unidimensionales han sido ampliamente estudiadas, mientras que el estudio de existencia de densidades también se encuentra profundizado hoy en día, de acuerdo con \cite{Nourdin_Peccati_2012}. 

En lo que a este texto respecta, la herramienta necesaria para abordar el problema planteado al inicio ha sido cubierta en su totalidad, por lo que en lo que resta, nos enfocaremos en el mismo.


\chapter{Propiedades espaciales de la Ecuación estocástica del calor}
En este capítulo presentamos en su plenitud el problema planteado al inicio de este texto, el cual consiste en estudiar la convergencia de las densidades de los promedios espaciales de la solución a la ecuación estocástica del calor, bajo ciertas condiciones, a la densidad de una variable aleatoria normal estándar $\Phi$, usando la distancia uniforme entre funciones en $\R$.

Ahora que contamos con las herramientas necesarias para abordar nuestro problema, procedemos a enunciarlo de manera rigurosa. Este capítulo está completamente basado en \cite[pp. 68-71, 75-86]{KUZGUN202268}. Consideremos nuevamente el problema del calor \eqref{shedefinitiva},
\begin{equation*}
   \begin{cases}
      \partial_t u=\frac{1}{2}\partial_{xx}u+\sigma(u)\dot{W}, & \qquad \forall x\in \R, \ \forall t>0,\\
      u_0(x):=u(x,0)=1, & \qquad \forall x\in \R,
   \end{cases}
\end{equation*}
donde $\dot{W}$ es un ruido blanco en $[0,\infty)\times\R$, y $\sigma$ es una función Lipschitz tal que $\sigma(1)\neq 0$.

Nos referiremos a este como nuestro problema principal, y siempre que hagamos referencia a él, entenderemos que las hipótesis sobre $u_0$ y $\sigma$ anteriormente mencionadas se satisfacen. También a lo largo de este capítulo escribiremos a la densidad de una variable gaussiana de varianza $t>0$ como
\[
p_t(x):=\frac{1}{\sqrt{2\pi t}}e^{-x^2/2t}, \qquad \forall x\in \R.
\]
% Tal y como hemos mencionado, la existencia y la unicidad de la solución mild a la ecuación estocástica del calor ha sido ya demostrada desde la década de los ochentas en el caso en que la condición inicial $u_0$ es una función acotada y la función $\sigma$ es Lipschitz continua (ver, por ejemplo, \cite{Walsh_J.B_Introduction_to_SPDEs}). 

Del capítulo 1 tenemos garantizada la existencia y unicidad de un campo aleatorio  $u=\left\{u(t,x):(t,x)\in [0,\infty)\times\R\right\}$ que funge como solución al problema anterior. 

Como mencionamos en dicho capítulo, diremos que $u$ es la \textit{solución mild} al problema \eqref{shedefinitiva}. Con dicha solución al alcance, la cual es de naturaleza estocástica, nos interesamos ahora por en el comportamiento asintótico de un nuevo objeto construido a partir de la misma: los \textit{promedios espaciales}. Estos objetos, en nuestro contexto, se definen a continuación. 

\begin{dfn}\label{defpromediosespaciales}
 Sea $u=\left\{u(t,x):(t,x)\in [0,\infty)\times\R\right\}$ la solución mild al problema \eqref{shedefinitiva}. Para $R>0$ fijo, definimos los promedios espaciales centrados y normalizados de $u$ como 
 \begin{equation*}
   F_{R,t}:=\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}u(t,x)dx -2R\right), \qquad \text{donde} \ \ \sigma^2_{R,t}:=\text{Var}\left(\int_{[-R,R]}u(t,x)dx\right).
\end{equation*}
 \end{dfn}
Nos referiremos a las variables $F_{R,t}$ simplemente como los promedios espaciales de la solución $u$, entendiéndolos siempre en el sentido de la definición anterior. Tal y como se mencionó al inicio, en \cite{HUANG20207170}, para un $t>0$ fijo, Huang, Nualart y Viitasaari estudian el comportamiento límite de las variables $F_{R,t}$ conforme $R$ tiende a infinito, siendo capaces de probar resultados tipo teorema central del límite para los promedios espaciales. Más aún, en dicha publicación, utilizando método de Stein junto con cálculo de Malliavin, se obtuvieron cotas superiores para la tasa de convergencia de $F_{R,t}$ a la distribución normal estándar en distancia de variación total. De manera precisa, los autores obtuvieron el siguiente resultado.

\begin{teo} 
 Sean $u$ la solución mild al problema \eqref{shedefinitiva} y $Z\sim$ Normal(0,1). Supongamos que $\sigma_{R,t}>0$. Entonces existe una constante $C>0$ que depende solamente de $t$ tal que 
 \begin{equation}
    d_{\text{TV}}(F_{R,t},N)\leq \frac{C}{\sqrt{R}}, \qquad \text{para }t>0 \ \text{ y } \ R\geq1.  
 \end{equation}
 \end{teo}
En particular, los autores de \cite{HUANG20207170} resaltan que la condición $\sigma(1)\neq 0$ que aparece en el problema \eqref{shedefinitiva} implica que $\sigma_{R,t}>0$.

Con este trabajo como antecedente, el propósito final del capítulo es obtener cotas superiores pero ahora para la tasa de convergencia de la distancia uniforme entre las densidades de los promedios espaciales y la densidad de una variable aleatoria normal estándar. Concretamente, el resultado principal que se busca probar en este capítulo es el siguiente.
\begin{teo}\label{teoremaprincipal}
Sea $u=\left\{u(t,x):(t,x)\in [0,\infty)\times\R\right\}$ la solución mild al problema \eqref{shedefinitiva}. Supongamos que $\sigma:\R\to\R$, además de las hipótesis propias del problema del calor, cumple las siguientes tres condiciones.
\begin{itemize}
   \item $\sigma\in C^2(\R)$.
   \item $\sigma'$ es acotada.
   \item $\abs{\sigma''(x)}\leq C(1+\abs{x}^{m})$, \qquad para cualquier $x\in \R$ real y para alguna $m>0$.
\end{itemize}
Supongamos también que, para algún $q>10$, se cumple que 
\[
\E\left[\abs{\sigma(u(t,0))}^{-q}\right]<\infty.    
\]
Entonces para $t>0$ fijo, se tiene que para cualquier $R>0$, los promedios espaciales $F_{R,t}$ poseen una densidad continua y acotada $f_{F_{R,t}}$, y la distancia entre las densidades de los promedios espaciales y la densidad de la distribución normal satisface la siguiente cota.  
\[
\sup_{x\in \R} \abs{f_{F_{R,t}}(x)-\Phi(x)}\leq \frac{C_t}{\sqrt{R}},   
\]
donde $\Phi$ es la densidad de una variable aleatoria normal estándar y $C_t>0$ es una constante dependiente de $t$ y de $\sigma$.
\end{teo}

Este capítulo, dedicado a la prueba de este último teorema, consiste en dos secciones. La primera la dedicamos a mostrar dos resultados clave para la prueba. El primero de ellos tiene que ver con una expresión para la segunda derivada de Malliavin de la solución al problema del calor $\eqref{shedefinitiva}$, mientras que el segundo tiene que ver con la existencia de los momentos inversos de la derivada de la misma. La segunda sección consiste en el cuerpo de la prueba como tal, y al inicio de ésta se describe como está estructurada.

\section{Cotas para la primera y segunda derivada de la solución mild y existencia de momentos inversos}

Tal y como se mencionó, estudiamos primero el comportamiento de la segunda derivada de la solución mild $u$ a la ecuación del calor \eqref{shedefinitiva}. Cabe mencionar que para lograrlo debemos basarnos en un resultado análogo para la primera derivada de $u$. Esto debido a que a partir del resultado para la primera derivada es que se llega al de la segunda derivada, además de ser necesario para la prueba del resultado principal. La proposición correspondiente a la primera derivada la enunciamos a continuación, y es adaptada de \cite[proposición 5.1]{Nualart2007}, y de \cite[sección 4.1]{KUZGUN202268} aunque con la notación de este texto, y directamente suponiendo que el término de deriva $b$ en \cite{Nualart2007} es cero.

\begin{teo}\label{teocota1eraderivada}
   Sea $u$ la solución al problema \eqref{shedefinitiva}. Supongamos además que $\sigma:\R\to\R$ es de clase $C^1(\R,\R)$, y su derivada es una función Lipschitz y acotada. Entonces para cualquier $(t,x)\in [0,T]\times\R$, $u(t,x)\in\bigcap_{p\geq1}\D^{1,p}$ y además, la derivada de Malliavin cumple que 
   \begin{equation}\label{formula1eraderivada}
      D_{s,y}u(t,x)=p_{t-s}(x-y)\sigma(u(s,y))+ \int_{[s,t]\times\R} p_{t-\tau}(x-\xi)\sigma'(u(\tau,\xi))D_{s,y}u(\tau,\xi)W(d\tau,d\xi),
   \end{equation}
   
   para casi cualquier $s\in [0,t]$ y $y\in \R$. Más aún, se cumple que 
   \begin{equation}\label{cota1eraderivada} 
      \norm{D_{s,y}u(t,x)}_{p}\leq C_{T,p \ }p_{t-s}(x-y),
   \end{equation}
   para cualquier $0\leq s<t\leq T$, y para $x,y\in \R$.
 \end{teo}
Con el resultado anterior a la mano, procedemos a enunciar lo respectivo para la segunda derivada de $u$, tal y como se enuncia en \cite{KUZGUN202268}. Es preciso destacar que, de acuerdo con los autores del artículo, la cota de la segunda derivada de la solución mild resuelve un problema abierto en el cálculo de Malliavin y por lo tanto este resultado tiene su interés en sí mismo.
 \begin{teo}\label{teocota2daderivada}
   Sea $u$ la solución al problema \eqref{shedefinitiva}. Supongamos que $\sigma$ cumple las hipótesis del teorema \ref{teoremaprincipal}. Fijando $(t,x)\in [0,\infty)\times \R$, tenemos que $u(t,x)\in \bigcap_{p\geq2}\D^{2,p}$ y para casi todo $0<r<s<t$, $y,z\in \R$, la segunda derivada de $u$, que denotamos como $D_{r,z}D_{s,y}u(t,x)$, satisface la siguiente ecuación diferencial estocástica lineal:
   \begin{align}
      D_{r,z}D_{s,y}u(t,x)&=p_{t-s}(x-y)\sigma'(u(s,y))D_{r,z}u(s,y)\notag\\
      &+\int_{[s,t]\times \R}p_{t-\tau}(x-\xi)\sigma''(u(\tau,\xi))D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\notag\\
      &+\int_{[s,t]\times \R}p_{t-\tau}(x-\xi)\sigma'(u(\tau,\xi))D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\label{formula2daderivada}
   \end{align}
   Más aún, para cualesquiera $0\leq r<s<t\leq T$  y $x,y,z\in \R$, tenemos que 
   \begin{equation}\label{cota2daderivada}
         \|D_{r,z}D_{s,y}u(t,x)\|_p\leq C_{T,p \ }\Phi_{r,z,s,y}(t,x),
   \end{equation}
   donde $C_{T,p}$ es una constante que depende de $T,p$ y $\sigma$, mientras que
      \begin{align*}
          \Phi_{r,z,s,y}(t,x):= & \ p_{t-s}(x-y)\left(p_{s-r}(y-z)+\frac{p_{t-r}(z-y)+p_{t-r}(z-x)+\1_{\{\abs{y-x}>\abs{z-y}\}}}{(s-r)^{1/4}}\right).
      \end{align*} 
   \end{teo}
   Por ser de carácter fundamentalmente técnico, la demostración de este resultado se omite en esta tesis. No obstante, mencionamos que la idea de su prueba consiste en la técnica de las iteraciones de Picard, la desigualdad de Burkholder-Davis-Gundy, y lemas técnicos que se encuentran en \cite[lema A.1]{HUANG20207170} y \cite[lema A.1, lema A.2]{KUZGUN202268}. Es de destacar que las hipótesis del teorema anterior implican las hipótesis del teorema \ref{teocota1eraderivada}.

   Tornamos nuestra atención a la existencia de los momentos inversos de la derivada de la solución $u$ al problema del calor \eqref{shedefinitiva}.
\begin{teo}\label{teocotamomentosinversos}
   Sea $u$ la solución mild a la ecuación estocástica del calor. Supongamos que $\sigma$ es Lipschitz. Sea $p\geq2$ fijo, $t>0$ y supongamos que existe un $q>5p$ tal que $\E\left[\abs{\sigma(u(t,0))}^{-2q}\right]<\infty$. Entonces existe una constante $R_0>0$ tal que 
   \begin{equation}\label{cotamomentosinversos}
      \sup_{R\geq R_0}\E\left[\abs{D_{v_{R,t}}F_{R,t}}^{-p}\right]<\infty.
   \end{equation}
   
   \end{teo}
   La prueba de este resultado se omite en este texto, por lo que se remite a \cite[proposición 4.2]{KUZGUN202268} para su consulta. En la prueba del mismo se utilizan nuevamente la desigualdad de Burkholder-Davis-Gundy, propiedades de la solución mild $u$ y distintos lemas presentes en el apéndice del artículo citado.

\section{Prueba del resultado principal}
Con intención de desmenuzar con suficiente claridad la prueba, describimos brevemente su estructura.

La prueba parte de la cota \eqref{cotafundamental} aplicada a los promedios espaciales $F_{R,t}$. Es por lo tanto necesario verificar que dichas variables cumplen las hipótesis del teorema \ref{teocotafundamental}. Asimismo, serán usadas constantemente las desigualdades \eqref{cota1eraderivada}, \eqref{cota2daderivada} y \eqref{cotamomentosinversos}, por lo que debemos verificar que las hipótesis de los teoremas \ref{teocota1eraderivada}, \ref{teocota2daderivada} y \ref{teocotamomentosinversos} se satisfacen. Habilitados para usar dichos resultados, de la cota \eqref{cotafundamental} nos surgen dos sumandos. El primero de ellos se puede acotar con relativa rapidez apoyándonos de las cotas anteriores y usando la prueba de \cite[teorema 1.1]{HUANG20207170}, por lo que el grueso de la prueba consiste en acotar el segundo sumando, correspondiente a la norma de la segunda derivada de los promedios espaciales. Allí se descompondrá dicha derivada de segundo orden en dos términos integrales y se procede a acotar cada término por separado utilizando las propiedades de la integral de Itô-Walsh, las cotas mencionadas antes y algunos resultados técnicos presentes en el apéndice de este texto.

Por lo anterior, la prueba está dividida en tres partes. La primera de ellas dedicada a verificar la validez de las hipótesis de los resultados a usar, la segunda de ellas está dirigida a acotar el primero de los sumandos y la última parte se dedica a acotar la norma de la segunda derivada de los promedios espaciales. Dicho todo lo anterior, comenzamos la prueba del resultado principal.
\begin{proof}[Demostración (del teorema \ref{teoremaprincipal})]

\begin{proofpart}[Validez de las hipótesis para las cotas]
Sean $R$ y $t>0$ fijos. Consideremos a los promedios espaciales $F_{R,t}$ de la solución mild $u$ al problema \eqref{shedefinitiva}. Buscamos primero ver que $F_{R,t}$ satisface las hipótesis del teorema \ref{teocotafundamental}. Para ello, debemos ver que dicha variable puede verse como la divergencia de algún elemento $v\in \D^{1,6}(\mathfrak{H})$. Obsérvese que, de la definición de los promedios espaciales y de la relación en el teorema \ref{solucionmild}, tenemos las siguientes igualdades.
\begin{align}
   F_{R,t}&=\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}u(t,x)dx-2R\right)\notag\\
   &=\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}\left(1+\int_{[0,t]\times \R}p_{t-s}(x-y)\sigma(u(s,y))W(ds,dy)\right)dx-2R\right)\notag\\
   &=\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}\int_{[0,t]\times \R}p_{t-s}(x-y)\sigma(u(s,y))W(ds,dy)\right)dx\notag\\
   &=\int_{[0,t]\times \R}\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}p_{t-s}(x-y)\sigma(u(s,y))dx\right)W(ds,dy)\notag\\
   &=\int_{[0,\infty)\times \R}\1_{[0,t]}(s)\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}p_{t-s}(x-y)\sigma(u(s,y))dx\right)W(ds,dy)\label{promespacdiv1},
\end{align}
en donde en la penúltima igualdad hemos hecho uso del teorema de Fubini estocástico. Por otro lado, sabemos que la integral de Itô-Walsh coincide con el operador de divergencia $\delta$ en dos dimensiones, así que definiendo $v_{R,t}$ como 

\begin{equation}\label{vectordireccion}
      v_{R,t}:=\1_{[0,t]}(s)\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}p_{t-s}(x-y)\sigma(u(s,y))dx\right),
\end{equation}
 la relación \eqref{promespacdiv1} se convierte en
\begin{equation}\label{promespacialdiv2}
F_{R,t}=\int_{[0,\infty)\times \R}\1_{[0,t]}(s)\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}p_{t-s}(x-y)\sigma(u(s,y))dx\right)W(ds,dy)=\delta(v_{R,t}).
\end{equation}
Por lo tanto, los promedios espaciales se pueden ver como el operador divergencia aplicado a $v_{R,t}$. Observamos ahora que de las tres hipótesis sobre $\sigma$ que tenemos, estamos en condiciones de aplicar la cota $\eqref{cota2daderivada}$, por lo que para $(t,x)\in [0,\infty)\times\R$ fijo, $u(t,x)\in \bigcap_{p\geq2} \D^{2,p}$. Además, para cualquier $T>0$ y $p\geq2$, se tiene que 

\begin{equation}\label{cotaunif}
      \sup_{(t,x)\in [0,\infty)\times\R}\norm{u(t,x)}_{2,p}<\infty.
\end{equation}

Esto nos dice que para un $t>0$ fijo, $F_{R,t}\in \D^{2,6}$ y $v_{R,t}\in \D^{1,2}(\mathfrak{H})$. 

Por otro lado, por hipótesis existe algún $q>10$ tal que $\E\left[\abs{\sigma(u(t,0))}^{-q}\right]<\infty$, de tal forma que eligiendo $p=4$ en el teorema \ref{teocotamomentosinversos}, tenemos que para $q':=2q>2(10)=5(4)=5p$, al ser el caso que $2q'>q$ y ser $\Omega$ un espacio de medida finita, 
\[
\E\left[\abs{\sigma(u(t,0))}^{-2q'}\right]=\E\left[\abs{\frac{1}{\sigma(u(t,0))}}^{2q'}\right]\leq \E\left[\abs{\frac{1}{\sigma(u(t,0))}}^{q}\right]=\E\left[\abs{\sigma(u(t,0))}^{-q}\right]<\infty,
\]
por lo que las hipótesis del teorema $\ref{teocotamomentosinversos}$ se satisfacen para $p=4$ y por lo tanto, para $t>0$ fijo, existe una constante $R_0$ tal que la cota de los momentos inversos \eqref{cotamomentosinversos} es válida. En particular, para $t>0$ fijo y $R>R_0$ se sigue que $D_{v_R,t}F_{R,t}\in L^{4}(\Omega)$. Finalmente, por definición estamos tomando los promedios espaciales como variables centradas y con varianza 1. 

Hemos mostrado que para $R>R_0$ y $t>0$, las hipótesis del teorema \ref{teocotafundamental} se cumplen y por lo tanto, la variable $F_{R,t}/D_{v_R,t}F_{R,t}\in \text{Dom}(\delta)$, los promedios espaciales tienen una densidad continua y acotada, que denotamos como $f_{F_{R,t}}$, y la misma cumple  $\eqref{cotafundamental}$. Con precisión, sustituyendo en dicha cota obtenemos que 
\begin{align}\label{cotapromespacfundamental}
   \sup_{x\in \R} \abs{f_{F_{R,t}}(x)-\Phi(x)}&\leq \left(\|F_{R,t}\|_4\|\left(D_{v_{R,t}}F_{R,t}\right)^{-1}\|_4+2\right)\|1-D_{v_{R,t}}F_{R,t}\|_2\notag\\
   &\ \ \ +\|\left(D_{v_{R,t}}F_{R,t}\right)^{-1}\|^{2}_4\|D_{v_{R,t}} \left(D_{v_{R,t}}F_{R,t}\right)\|_2,
\end{align}
Observamos dos sumandos en la desigualdad anterior. Mostramos a continuación que cada uno de estos sumandos está acotada por una cantidad proporcional a $1/\sqrt{R}$.
\end{proofpart}
\begin{proofpart}[Cota del primer sumando]
Notemos primero que, de la relación \eqref{cotapromespacfundamental}, al ser $F_{R,t}\in \D^{2,6}$, y darse las contenciones $\D^{2,6}\subseteq \D^{2,4}\subseteq{L^{4}}(\Omega)$, se sigue que $\norm{F_{R,t}}_4<\infty$. Más aún, dicha cota es uniforme en $R$ gracias a \label{cotaunif}. Por otro lado, para $t>0$ y $R>R_0$ fijos, de \eqref{cotamomentosinversos} se sigue que 
\begin{equation}\label{cotapromespacmomentosinversos}
   \norm{(D_{v_{R,t}}F_{R,t})^{-1}}_4=\E\left[\abs{\left(D_{v_{R,t}}F_{R,t}\right)^{-1}}^{4}\right]^{1/4}=\E\left[\abs{D_{v_{R,t}}F_{R,t}}^{-4}\right]^{1/4}<\infty,
\end{equation}
y nuevamente gracias a \ref{cotamomentosinversos}, dicha cota se puede hacer uniforme en $R$. 

Definiendo $C_{t}^{(1)}:=\left(\|F_{R,t}\|_4\|\left(D_{v_{R,t}}F_{R,t}\right)^{-1}\|_4+2\right)$, de lo anterior tenemos que $0<C_{t}^{(1)}<\infty$, y dicha constante depende solamente de $t>0$ fijo. Resta estudiar el término $\norm{1-D_{v_{R,t}F_{R,t}}}_2$. Esta parte de la prueba está basada en la prueba de \cite[teorema 1.1]{HUANG20207170}. Dado que $F_{R,t}$ es una variable centrada y de varianza 1, se sigue que 
\begin{align*}
   \norm{1-D_{v_{R,t}}F_{R,t}}_2&=\E\left[(\E\left[F_{R,t}^2\right]-D_{v_{R,t}}F_{R,t})^2\right]^{1/2}\\
   &=\E\left[(\E\left[F_{R,t}\delta(v_{R,t})\right]-D_{v_{R,t}}F_{R,t})^2\right]^{1/2}\\
   &=\E\left[(\E\left[D_{v_{R,t}}F_{R,t}\right]-D_{v_{R,t}}F_{R,t})^2\right]^{1/2}\\
   &=\sqrt{\text{Var}\left(D_{v_{R,t}}F_{R,t}\right)},
\end{align*}
donde hemos hecho uso de la definición de $\delta$. Llegados a este punto, remitimos a la prueba del teorema 1.1 en el artículo \cite{HUANG20207170}, en donde el grueso de la prueba consiste en acotar $\sqrt{\text{Var}\left(D_{v_{R,t}}F_{R,t}\right)}$ por $C/\sqrt{R}$, donde $C$ es una constante que depende de $t>0$ y de $p$, que en nuestro caso está fijado en $p=4$. Por lo tanto, definiendo $C_{t}^{(2)}:=C_{t}^{(1)}\cdot C$, para $t>0$,
\begin{equation}\label{cotaprimersumando}
   \left(\|F_{R,t}\|_4\|\left(D_{v_{R,t}}F_{R,t}\right)^{-1}\|_4+2\right)\|1-D_{v_{R,t}}F_{R,t}\|_2\leq \frac{C_{t}^{(2)}}{\sqrt{R}},
\end{equation}
donde $C_{t}^{(2)}$ depende solamente de $t>0$.
\end{proofpart}
\begin{proofpart}[Cota del segundo sumando]
Directamente notamos que el término $\norm{(D_{v_{R,t}}F_{R,t})^{-1}}^2_4$ del segundo sumando está acotado, según vimos en \eqref{cotapromespacmomentosinversos}, por lo que nos enfocamos en acotar el término $\norm{D_{v_{R,t}}(D_{v_{R,t}}F_{R,t})}_2$. 
Para tal efecto, establecemos la siguiente notación que nos será útil. Definimos, para $t>0$ y $R>0$, 
\[
Q_R:=[-R,R], \qquad \Sigma_{t,x}:=\sigma(u(t,x)), \qquad \Sigma_{t,x}^{(1)}:=\sigma'(u(t,x)) \qquad \text{ y } \qquad \Sigma_{t,x}^{(2)}:=\sigma''(u(t,x)),
\]
y además, para $0<s<t$, y $y\in \R$, definimos  
\begin{equation}\label{defnotacionphi}
   \phi_{R,t}(s,y):=\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-s}(x-y)dx.
\end{equation}

Armados con la notación anterior, directamente de la definición de los promedios espaciales tenemos que la derivada de Malliavin de los mismos está dada por 

\[
   D_{r,z}F_{R,t}=D_{r,z}\left(\frac{1}{\sigma_{R,t}}\int_{-[R,R]}u(t,x)dx-2R\right)=\frac{1}{\sigma_{R,t}}\int_{Q_R}D_{r,z}u(t,x)dx,
\]
por lo que tomando la derivada en dirección de $v_{R,t}$ definido en \eqref{vectordireccion} y sustituyendo, obtenemos 
\begin{align*}
   D_{v_{R,t}}&F_{R,t}=\langle D_{r,z}F_{R,t},v_{R,t}\rangle_{L^{2}([0,\infty)\times\R)}\\
   &=\int_{[0,\infty)}\int_\R v_{R,t}D_{r,z}F_{R,t}dz dr\\
   &=\int_{[0,\infty)}\int_\R\left(\1_{[0,t]}(r)\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)\sigma(u(r,z))dx_1\right)\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}D_{r,z}u(t,x_2)dx_2\right) dz dr\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)dx_1\right)\sigma(u(r,z))D_{r,z}u(t,x_2) dx_2dz dr\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(r,z)\sigma(u(r,z))D_{r,z}u(t,x_2) dx_2dz dr\\
\end{align*}
Haciendo un cambio de variables en esta última igualdad para simplificar notación, llegamos a que
\begin{align*}
D_{v_{R,t}}F_{R,t}&=\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma(u(s,y))D_{s,y}u(t,x) dx dy ds,
\end{align*}
 Tomamos nuevamente la derivada de Malliavin de la expresión anterior para obtener 
\begin{align*}
   D_{r,z}(D_{v_{R,t}}F_{R,t})&=\frac{1}{\sigma_{R,t}}D_{r,z}\left(\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma(u(s,y))D_{s,y}u(t,x) dx dy ds\right)\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)D_{r,z}\left(\sigma(u(s,y))D_{s,y}u(t,x)\right) dx dy ds\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma'(u(s,y))D_{r,z}u(s,y)D_{s,y}u(t,x)dx dy ds\\
   & \ \ \ +\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma(u(s,y))D_{r,z}D_{s,y}u(t,x)dx dy ds
\end{align*}
Sacando nuevamente la derivada direccional, la expresión anterior nos lleva a las siguientes igualdades.
\begin{align}
   D_{v_{R,t}}&(D_{v_{R,t}}F_{R,t})=\langle D_{r,z}\left(D_{v_{R,t}F_{R,t}}\right),v_{R,t}\rangle_{L^{2}([0,\infty)\times\R)}\notag\\
   &=\int_{[0,\infty)}\int_\R \left(\1_{[0,t]}(r)\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)\sigma(u(r,z))dx_1\right)\notag\\
   &\ \ \ \ \cdot \left(\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma'(u(s,y))D_{r,z}u(s,y)D_{s,y}u(t,x)dx dy ds\right)dz dr\notag\\
   &\ \ \ +\int_{[0,\infty)}\int_\R \left(\1_{[0,t]}(r)\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)\sigma(u(r,z))dx_1\right)\notag\\
   &\ \ \ \ \cdot\left(\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma(u(s,y))D_{r,z}D_{s,y}u(t,x)dx dy ds\right)dz dr\notag\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)dx_1\right)\notag\\
   &\ \ \ \ \cdot \left(\Sigma_{r,z}\phi_{R,t}(s,y)\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)D_{s,y}u(t,x)\right) dx dy dz ds dr\notag\\
   &\ \ \ +\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R} \left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)dx_1\right)\notag\\
   &\ \ \ \ \cdot\left(\Sigma_{r,z}\phi_{R,t}(s,y)\Sigma_{s,y}D_{r,z}D_{s,y}u(t,x)\right)dx dy dz ds dr\notag
   \end{align}
   y reconociendo las respectivas funciones $\phi_{R,t}$, llegamos a que 
   \begin{align}
      D_{v_{R,t}}&(D_{v_{R,t}}F_{R,t})\notag\\
      &=\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)D_{s,y}u(t,x)dx dy dz ds dr\label{pruebasumando1}\\ 
   &\ \ \ +\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}D_{r,z}D_{s,y}u(t,x)dx dy dz ds dr,\label{pruebasumando2}
\end{align}
   donde hemos hecho uso múltiples veces del teorema de Fubini y de la notación previamente establecida. A continuación, utilizamos las fórmulas \eqref{formula1eraderivada} y \eqref{formula2daderivada} en los dos sumandos de la última igualdad anterior. Así, para el sumando \eqref{pruebasumando1}, sustituyendo la derivada $D_{s,y}u(t,x)$ se tiene la expresión
   \begin{align*}
      \frac{1}{\sigma_{R,t}}&\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)D_{s,y}u(t,x)dx dy dz ds dr\\
      &=\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\\
      & \ \ \ \ \cdot\left(p_{t-s}(x-y)\Sigma_{s,y}+ \int_{[s,t]\times\R} p_{t-\tau}(x-\xi)\Sigma^{(1)}_{\tau,\xi}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dx dy dz ds dr\\
   \end{align*}
   Distribuimos la suma anterior en la integral, por lo que 
   \begin{align}
      \frac{1}{\sigma_{R,t}}&\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)D_{s,y}u(t,x)dx dy dz ds dr\notag\\
      &=\int_{[0,t]^2}\int_{\R^2}\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-s}(x-y)dx\right)\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dy dz ds dr\notag\\
      & \ \ \ + \int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\notag\\
      &\ \ \ \ \cdot\left(\int_{[s,t]\times\R} p_{t-\tau}(x-\xi)\Sigma^{(1)}_{\tau,\xi}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dx dy dz ds dr\notag\\
      &=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dy dz ds dr\notag\\
      & \ \ \ + \int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\notag\\
      &\ \ \ \ \cdot\left(\int_{[s,t]\times\R} \left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-\tau}(x-\xi)dx\right)\Sigma^{(1)}_{\tau,\xi}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dy dz ds dr\notag\\
      &=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dy dz ds dr\label{pruebasumando3}\\
      &\ \ \ + \int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\notag\\
      &\ \ \ \ \cdot\left(\int_{[s,t]\times\R} \phi_{R,t}(\tau,\xi)\Sigma^{(1)}_{\tau,\xi}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dy dz ds dr\label{pruebasumando4}.
   \end{align}
Haciendo lo respectivo para el sumando \eqref{pruebasumando2}, obtenemos las siguientes expresiones.
\begin{align}
   &\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}D_{r,z}D_{s,y}u(t,x)dx dy dz ds dr\notag\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\left( p_{t-s}(x-y)\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\notag\\
   &\ \ \ +\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\left(\int_{[s,t]\times \R} p_{t-\tau}(x-\xi)\Sigma^{(2)}_{\tau,\xi}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)\notag\\
   &\ \ \ +\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y} \left(\int_{[s,t]\times \R}p_{t-\tau}(x-\xi)\Sigma^{(1)}_{\tau,\xi}D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right) dx dy dz ds dr\notag\\
   &=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\left(\frac{1}{\sigma_{R,t}}\int_{Q_R} p_{t-s}(x-y)dx\right)\notag\\
   &\ \ \ +\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   & \ \ \ \ \cdot\left(\int_{[s,t]\times \R} \left(\frac{1}{\sigma_{R,t}}\int_{Q_R} p_{t-\tau}(x-\xi)dx\right)\Sigma^{(2)}_{\tau,\xi}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)\notag\\
   &\ \ \ +\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   &\ \ \ \ \cdot \left(\int_{[s,t]\times \R}\left(\frac{1}{\sigma_{R,t}}\int_{Q_R} p_{t-\tau}(x-\xi)dx\right)\Sigma^{(1)}_{\tau,\xi}D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right) dy dz ds dr\notag
\end{align}
Y nuevamente reconociendo los términos $\phi_{R,t}$, la expresión anterior es igual a
\begin{align}
   &\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dy dz ds dr\label{pruebasumando5}\\
   &\ \ \ +\int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   &\ \ \ \ \cdot\left(\int_{[s,t]\times \R} \phi_{R,t}(\tau,\xi)\Sigma^{(2)}_{\tau,\xi}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right) dy dz ds dr\label{pruebasumando6}\\
   &\ \ \ +\int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   &\ \ \ \ \cdot\left(\int_{[s,t]\times \R}\phi_{R,t}(\tau,\xi)\Sigma^{(1)}_{\tau,\xi}D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right) dy dz ds dr.\label{pruebasumando7}
\end{align}
Observamos que los términos \eqref{pruebasumando3} y \eqref{pruebasumando5} coinciden, mientras que el término $\phi_{R,t}(r,z)\phi_{R,t}(s,y)$ es común dentro de las integrales en las expresiones \eqref{pruebasumando4}, \eqref{pruebasumando6} y \eqref{pruebasumando7}. Luego, definiendo 
\begin{align*}
   Z_{r,z,s,y}(\tau,\xi)&:=\Sigma_{r,z}\Sigma_{s,y}^{(1)}\Sigma_{\tau,\xi}^{(1)}D_{r,z}u(s,y)D_{s,y}u(\tau,\xi)\\
   &\ \ \ +\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(2)}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)\\
   &\ \ \ +\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(1)}D_{r,z}D_{s,y}u(\tau,\xi),
\end{align*}
y combinando las expresiones \eqref{pruebasumando3} a \eqref{pruebasumando7}, hallamos que la segunda derivada direccional de $F_{R,t}$ cumple
\begin{align}
   &D_{v_{R,t}}(D_{v_{R,t}}F_{R,t})=2\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dy dz ds dr\notag\\
   &\ \ \ \ + \int _{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}^{(1)}D_{r,z}u(s,y)\right)\notag\\
   & \ \ \ \ \cdot \left(\int_{[s,t]\times\R}\phi_{R,t}(\tau,\xi)\Sigma_{\tau,\xi}^{(1)}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dy dz ds dr\notag\\
   &\ \ \ +\int _{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   &\ \ \ \ \cdot \left(\int_{[s,t]\times\R}\phi_{R,t}(\tau,\xi)\Sigma_{\tau,\xi}^{(2)}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dy dz ds dr\notag\\
   &\ \ \ +\int _{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   &\ \ \ \ \cdot \left(\int_{[s,t]\times\R}\phi_{R,t}(\tau,\xi)\Sigma_{\tau,\xi}^{(21)}D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dy dz ds dr,\notag
\end{align}
y denotando por comodidad la primera integral de la suma anterior como $\mathcal{Y}_{R,t}^{(1)}$,
\begin{align}
   &D_{v_{R,t}}(D_{v_{R,t}}F_{R,t})\notag\\
   &=\mathcal{Y}_{R,t}^{(1)}+\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\notag\\
   & \ \ \ \ \cdot \left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\left[\Sigma_{r,z}\Sigma_{s,y}^{(1)}\Sigma_{\tau,\xi}^{(1)}D_{r,z}u(s,y)D_{s,y}u(\tau,\xi)\right]dy dz ds dr\right) W(d\tau,d\xi)\notag\\
   &\ \ \ +\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\notag\\
   &\ \ \ \ \cdot\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\left[\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(2)}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)\right]dy dz ds dr\right) W(d\tau,d\xi)\notag\\
   &\ \ \ +\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\notag\\
   &\ \ \ \ \cdot\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\left[\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(1)}D_{r,z}D_{s,y}u(\tau,\xi)\right]dy dz ds dr\right) W(d\tau,d\xi)\notag\\
   &=\mathcal{Y}_{R,t}^{(1)}+\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dy dz ds dr\right)W(d\tau,d\xi)\notag\\
   &=2\mathcal{Y}_{R,t}^{(1)}+\mathcal{Y}_{R,t}^{(2)},\label{pruebasumando8}
\end{align}
en donde convenimos que
\begin{align*}
   &\mathcal{Y}_{R,t}^{(1)}:=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dy dz ds dr\\
   &\mathcal{Y}_{R,t}^{(2)}:=\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dy dz ds dr\right)W(d\tau,d\xi),
\end{align*}
y donde nuevamente se ha usado múltiples veces del teorema de Fubini y la notación previamente establecida. Una vez que llegamos a este punto, buscamos acotar cada uno de los sumandos de \eqref{pruebasumando8}. 

Comenzamos con el término $\mathcal{Y}_{R,t}^{(1)}$. Primero hallamos una cota para la norma $L^2(\Omega)$ de la expresión $\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}D_{r,z}u(s,y)$. Usando la desigualdad de Cauchy-Schwarz, 
\[
   \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}D_{r,z}u(s,y)}_2\leq \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}}_4\norm{D_{r,z}u(s,y)}_4, 
\]
y usando ahora la desigualdad \eqref{cota1eraderivada}, para $0\leq r<s\leq T$, \ $y,z\in \R$ y $p=4$, hallamos que 
\begin{equation}\label{pruebacotasigma1}
      \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}}_4\norm{D_{r,z}u(s,y)}_4\leq \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}}_4C_{T}^{(3)}p_{s-r}(y-z),   
\end{equation}
donde $C_{T}^{(3)}$ es una constante que solo depende de $T$. Por otro lado, por hipótesis $\sigma'$ es una función acotada, digamos, por una constante $K_1>0$, de forma que 
\begin{equation}\label{pruebacotasigma2}
   \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}}_4\leq K_1\norm{\Sigma_{r,z}\Sigma_{s,y}}_4\leq K_1\norm{\Sigma_{r,z}}_8\norm{\Sigma_{s,y}}_{8}.
\end{equation}
   
Ahora bien, como la condición inicial en el problema del calor que estamos tratando es $u_0(w)=u(0,w)=1$, para cualquier $w\in \R$, y dado que $\sigma$ es Lipschitz,  se tienen las siguientes relaciones para $\Sigma_{r,z}$:
\begin{align}
      \norm{\Sigma_{r,z}}_8&\leq\norm{\sigma(u(r,z))-\sigma(u(0,z))}_8+\norm{\sigma(u(0,z))}_8\notag\\
      &\leq K_2\norm{u(r,z)-u(0,z)}+\norm{\sigma(1)}_8\notag\\
      &\leq K_2(\norm{u(r,z)}_8+\norm{1}_8)+|\sigma(1)|\notag\\
      &\leq K_2(C_{T}^{(4)}+1)+|\sigma(1)|\label{pruebacotasigma3},
\end{align}
en donde $K_2>0$ es la constante de Lipschitz de $\sigma$, y en la cuarta desigualdad hemos hecho uso de la igualdad \eqref{cotasolucionmild} con $T>0$ y $p=8$. Obsérvese que la cota obtenida antes depende solamente de $\sigma$ y de $T>0$. De manera análoga se puede acotar el término $\norm{\Sigma_{s,y}}_8$. Por lo tanto, combinando \eqref{pruebacotasigma2}, \eqref{pruebacotasigma3} y la cota \eqref{pruebacotasigma3} análoga para $\Sigma_{s,y}$, hallamos que 
\begin{equation}\label{pruebacotasigma4.5}
   \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}}_4\leq C_{T}^{(5)},
\end{equation} 
en donde $C^{(5)}_T>0$ es una constante que solo depende de $T>0$ y $\sigma$. Insertando esta cota en \eqref{pruebacotasigma1}, obtenemos que 
\begin{equation}\label{pruebacotasigma4}
   \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}}_4\norm{D_{r,z}u(s,y)}_4\leq C_{T}^{(6)}p_{s-r}(y-z),
\end{equation}
 donde $C_T^{(6)}:=C_T^{(5)}\cdot C_T^{(3)}$. Obtenida esta cota, procedemos a extraer la norma $L^{2}(\Omega)$ de $\mathcal{Y}_{R,t}^{(1)}$. Obtenemos así las siguientes relaciones.
 \begin{align*}
   &\norm{\mathcal{Y}_{R,t}^{(1)}}_2=\left(\int_\Omega \left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dy dz ds dr\right)^2d\P\right)^{1/2}\\
   &=\Bigg(\int_\Omega\int_{[0,t]^2}\int_{\R^2}\int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\\
   &\ \ \ \ \cdot \left(\phi_{R,t}(r',z')\phi_{R,t}^2(s',y')\Sigma_{r',z'}\Sigma_{s',y'}\Sigma^{(1)}_{s',y'}D_{r',z'}u(s',y')\right)dy dz ds dr dy' dz'ds'dr'd\P\Bigg)^{1/2}\\
   &=\Bigg(\int_{[0,t]^2}\int_{\R^2}\int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\phi_{R,t}(r',z')\phi_{R,t}^2(s',y')\right)\\
   &\ \ \ \ \cdot \left(\int_\Omega\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\Sigma_{r',z'}\Sigma_{s',y'}\Sigma^{(1)}_{s',y'}D_{r',z'}u(s',y')\right) dy dz ds dr dy'dz'ds'dr'd\P\Bigg)^{1/2}.
 \end{align*}
 Usando ahora la desigualdad de Cauchy-Schwarz, 
 \begin{align*}
   \norm{\mathcal{Y}_{R,t}^{(1)}}_2&\leq \Bigg(\int_{[0,t]^2}\int_{\R^2}\int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\phi_{R,t}(r',z')\phi_{R,t}^2(s',y')\right)\\
   &\ \ \ \ \cdot \left(\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)}_2\norm{\Sigma_{r',z'}\Sigma_{s',y'}\Sigma^{(1)}_{s',y'}D_{r',z'}u(s',y')}_2\right) dy dz ds dr dy' dz' ds' dr'\Bigg)^{1/2}\\
   &=\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)}_2dy dz ds dr\right)^{1/2}\\
   & \ \ \ \ \ \cdot \left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r',z')\phi_{R,t}^2(s',y')\norm{\Sigma_{r',z'}\Sigma_{s',y'}\Sigma^{(1)}_{s',y'}D_{r',z'}u(s',y')}_2dy'dz'ds'dr'\right)^{1/2}\\
   &=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)}_2dy dz ds dr.
 \end{align*}
 Usamos ahora \eqref{pruebacotasigma4} en la última desigualdad anterior para obtener que
 \begin{align*}
   \norm{\mathcal{Y}_{R,t}^{(1)}}_2&\leq\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)}_2dy dz ds dr\\
   &\leq C^{(6)}_T\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)p_{s-r}(y-z)dy dz ds dr.
\end{align*}
En esta última expresión integramos sobre $z$ y expandimos el término $\phi_{R,t}(r,z)$ para hallar que 
\begin{align*}
   \norm{\mathcal{Y}_{R,t}^{(1)}}_2&\leq C^{(6)}_T\int_{[0,t]^2}\int_{\R}\phi_{R,t}^2(s,y)\left(\int_\R\phi_{R,t}(r,z)p_{s-r}(z-y)dz\right)dy ds dr\\
   &=C^{(6)}_T\int_{[0,t]^2}\int_{\R}\phi_{R,t}^2(s,y)\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}\int_\R p_{t-r}(x-z)p_{s-r}(z-y)dz dx\right)dy ds dr\\
   &=C^{(6)}_T\int_{[0,t]^2}\int_{\R}\phi_{R,t}^2(s,y)\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t+s-2r}(x-y)dx\right)dy ds dr,
\end{align*}
donde hemos hecho uso de la propiedad de semigrupo del núcleo del calor $(p_t)_{t\geq0}$ en la última igualdad. Claramente se tiene que 
\[
   \frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t+s-2r}(x-y)dx\leq \frac{1}{\sigma_{R,t}}\int_{\R}p_{t+s-2r}(x-y)dx\leq \frac{1}{\sigma_{R,t}},
\]
por lo que insertando esta última desigualdad en la anterior, llegamos a que 
\[
\norm{\mathcal{Y}_{R,t}^{(1)}}_2\leq \frac{C_T^{(6)}}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R}\phi_{R,t}^2(s,y).
\]
Utilizando el lema \ref{LemaA.4a}, obtenemos que existe un $R_0\geq1$ dependiente de $t>0$ tal que para cualquier $R\geq R_0$,
\[
   \frac{C_T^{(6)}}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R}\phi_{R,t}^2(s,y)\leq \frac{C_T^{(6)}}{\sigma_{R,t}}\int_{[0,t]^2}C_t ds dr\leq \frac{C_t^{(7)}}{\sigma_{R,t}},
\]
por lo que tenemos la siguiente cota que es uniforme en $R$,
\[
\norm{\mathcal{Y}_{R,t}^{(1)}}_2\leq \frac{C_t^{(7)}}{\sigma_{R,t}}.
\]
Finalmente, utilizamos el lema \ref{lemaA.3a}, para argumentar que, como $\tfrac{\sigma^2_{R,t}}{R} \xrightarrow[n\to\infty]{}2\int_{0}^{t}\xi(s)ds$, donde $\xi(s)=\E\left[\sigma^2(u(s,y))\right]$, y esta última función no depende de $y\in \R$ (ver el enunciado del lema mencionado) entonces existe un $R_1>0$ tal que para cualquier $R>R_1$,
\begin{equation}\label{cotafinal1}
      \norm{\mathcal{Y}_{R,t}^{(1)}}_2\leq \frac{C_t^{(7)}}{\sigma_{R,t}}=\frac{C_t^{(7)}}{\sqrt{\frac{\sigma_{R,t}^2}{R}}\sqrt{R}}\leq \frac{C_t^{(8)}}{\sqrt{R}},  
\end{equation}
en donde $C_t^{(8)}$ es una constante que depende de $C_t^{(7)}$ y de la integral en $[0,t]$ de $\xi$, función que es independiente de $y$, y por ende tanto la integral como la constante $C_t^{(8)}$ sólo dependen de $t$.

Procedemos ahora a estimar el término $\mathcal{Y}_{R,t}^{(2)}$. Extrayendo el cuadrado de la norma $L^{2}(\Omega)$ de dicho término, y utilizando la isometría de Itô-Walsh, tenemos las siguientes igualdades.
\begin{align*}
   &\norm{\mathcal{Y}_{R,t}^{(2)}}_2^{2}=\norm{\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dy dz ds dr\right)W(d\tau,d\xi)}_2^2\\
   &=\E\left[\left(\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dy dz ds dr\right)W(d\tau,d\xi)\right)^2\right]\\
   &=\E\left[\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dy dz ds dr\right)^2d\xi d\tau\right]\\
   &=\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\E\left[\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dy dz ds dr\right)^2\right]d\xi d\tau.
\end{align*}
Reescribiendo el cuadrado de la integral anterior, tenemos que 
\begin{align*}
   &\norm{\mathcal{Y}_{R,t}^{(2)}}_2^{2}=\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\E\left[\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dy dz ds dr\right)^2\right]d\xi d\tau\\
   &=\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\E\Bigg[\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r_1,z_1)\phi_{R,t}(s_1,y_1)Z_{r_1,z_1,s_1,y_1}(\tau,\xi)dy_1dz_1ds_1dr_1\right)\\
   &\ \ \ \ \cdot \left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r_2,z_2)\phi_{R,t}(s_2,y_2)Z_{r_2,z_2,s_2,y_2}(\tau,\xi)dy_2dz_2ds_2dr_2\right)\Bigg]d\xi d\tau\\
   &=\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\int_{0\leq r_1\leq s_1\leq \tau}\int_{0\leq r_2\leq s_2\leq \tau}\int_{\R^4}\phi_{R,t}(r_1,z_1)\phi_{R,t}(s_1,y_1)\phi_{R,t}(r_2,z_2)\phi_{R,t}(s_2,y_2)\\
   &\ \ \ \ \cdot \E\left[Z_{r_1,z_1,s_1,y_1}(\tau,\xi)Z_{r_2,z_2,s_2,y_2}(\tau,\xi)\right]dy_1dz_1ds_1dr_1dy_2dz_2ds_2dr_2d\xi d\tau,
\end{align*}
en donde $Z_{r_1,z_1,s_1,y_1}(\xi,\tau)$ y $Z_{r_2,z_2,s_2,y_2}(\xi,\tau)$ son copias i.i.d. de $Z_{r,z,s,y}(\xi,\tau)$. A continuación, utilizamos la desigualdad de Cauchy-Schwarz justo en estos términos para obtener que 
\begin{align}
   \norm{\mathcal{Y}_{R,t}^{(2)}}_2^{2}&\leq \int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\int_{0\leq r_1\leq s_1\leq \tau}\int_{0\leq r_2\leq s_2\leq \tau}\int_{\R^4}\phi_{R,t}(r_1,z_1)\phi_{R,t}(s_1,y_1)\phi_{R,t}(r_2,z_2)\phi_{R,t}(s_2,y_2)\notag\\
   &\ \ \ \ \cdot \norm{Z_{r_1,z_1,s_1,y_1}(\tau,\xi)}_2 \norm{Z_{r_2,z_2,s_2,y_2}(\tau,\xi)}_2 dy_1dz_1ds_1dr_1dy_2dz_2ds_2dr_2d\xi d\tau\notag\\
   &=\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\int_{0\leq r_1\leq s_1\leq \tau}\int_{0\leq r_2\leq s_2\leq \tau}\int_{\R^4}\prod_{i=1,2}\phi_{R,t}(r_i,z_i)\phi_{R,t}(s_i,y_i)\notag\\
   &\ \ \ \ \cdot \norm{Z_{r_i,z_i,s_i,y_i}(\tau,\xi)}_2  dy_1dz_1ds_1dr_1dy_2dz_2ds_2dr_2d\xi d\tau \label{pruebacotazeta1}
\end{align}
En este punto, nos interesamos por acotar el término $\norm{Z_{r,z,s,y,}(\xi,\tau)}_2$. Usando la definición de $Z_{r,z,s,y}$, la desigualdad de Minkowski, y la desigualdad de Hölder, 
\begin{align*}
   &\norm{Z_{r,z,s,y,}(\xi,\tau)}_2\\
   &\leq \norm{\Sigma_{r,z}\Sigma_{s,y}^{(1)}\Sigma_{\tau,\xi}^{(1)}D_{r,z}u(s,y)D_{s,y}u(\tau,\xi)}_2+\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(2)}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)}_2\\
   &\ \ \ +\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(1)}D_{r,z}D_{s,y}u(\tau,\xi)}_2\\
   &\leq \norm{\Sigma_{r,z}\Sigma_{s,y}^{(1)}\Sigma_{\tau,\xi}^{(1)}}_4\norm{D_{r,z}u(s,y)}_8\norm{D_{s,y}u(\tau,\xi)}_8+\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(2)}}_4 \norm{D_{r,z}u(\tau,\xi)}_8 \norm{D_{s,y}u(\tau,\xi)}_8\\
   &\ \ \ +\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(1)}}_4\norm{D_{r,z}D_{s,y}u(\tau,\xi)}_4.
\end{align*}
En esta última desigualdad utilizamos cuatro veces la relación \eqref{cota1eraderivada} con $p=8$ y la desigualdad \eqref{cota2daderivada} con $p=4$ para obtener que 
\begin{align*}
   \norm{Z_{r,z,s,y,}(\xi,\tau)}_2&\leq C_{t}^{(9)} \Big(\norm{\Sigma_{r,z}\Sigma_{s,y}^{(1)}\Sigma_{\tau,\xi}^{(1)}}_4p_{s-r}(y-z)p_{\tau-s}(\xi-y)\\
   &\ \ \ +\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(2)}}_4p_{\tau-r}(\xi-z)p_{\tau-s}(\xi-y)+\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(1)}}_4\phi_{r,z,s,y}(\tau,\xi)\Big),
\end{align*}
en donde $C_t^{(9)}$ es una constante que solo depende de $t>0$ y de $\sigma$. Ahora bien, por hipótesis, $|\sigma'|\leq K_1$ en concordancia con la notación de la desigualdad \eqref{pruebacotasigma2}. Por lo tanto si en la última cota usamos la desigualdad de Hölder, así como las desigualdades \eqref{pruebacotasigma3} y \eqref{pruebacotasigma4.5}, hallamos que 
\begin{align*}
   \norm{Z_{r,z,s,y,}(\xi,\tau)}_2&\leq C_t^{(9)}\Big(C^{(10)}_tp_{s-r}(y-z)p_{\tau-s}(\xi-y)\\
   &\ \ \ +C_t^{(11)}\norm{\Sigma_{\tau,\xi}^{(2)}}_8p_{\tau-r}(\xi-z)p_{\tau-s}(\xi-y)+C_t^{(12)}\phi_{r,z,s,y}(\tau,\xi)\Big).
\end{align*}
Finalmente, recordemos que por hipótesis $|\Sigma_{\tau,\xi}^{(2)}|=|\sigma''(u(\tau,\xi))|\leq C(1+|u(\tau,\xi)|)^{m}$, para alguna $m>0$, y $C>0$. Usando esto y la igualdad \eqref{cotasolucionmild} para $p=8$ deducimos que
\[
\norm{\Sigma_{\tau,\xi}^{(2)}}_8\leq C\norm{1+\abs{u(\tau,\xi)}}_8\leq C(1+\E\left[\abs{u(\tau,\xi)^8}\right]^{1/8})=C_t^{(13)},
\]
donde $C_t^{(13)}$ es una cota que solo depende de $t>0$ y $\sigma$. Insertando esta cota en la desigualdad previa, hallamos finalmente que 
\begin{align}
   \norm{Z_{r,z,s,y,}(\xi,\tau)}_2&\leq C_t^{(9)}\Big(C^{(10)}_tp_{s-r}(y-z)p_{\tau-s}(\xi-y)\notag\\
   &\ \ \ +C_t^{(13)}p_{\tau-r}(\xi-z)p_{\tau-s}(\xi-y)+C_t^{(12)}\phi_{r,z,s,y}(\tau,\xi)\Big)\label{pruebacotazeta2}.
\end{align}
Ahora, si insertamos la cota \eqref{pruebacotazeta2} en \eqref{pruebacotazeta1} llegamos a que 
\begin{align*}
   \norm{\mathcal{Y}_{R,t}^{(2)}}_2^{2}&\leq C_t^{(9)}\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\int_{0\leq r_1\leq s_1\leq \tau}\int_{0\leq r_2\leq s_2\leq \tau}\int_{\R^4}\prod_{i=1,2}\phi_{R,t}(r_i,z_i)\phi_{R,t}(s_i,y_i)\\
   &\ \ \ \ \cdot \Big(C^{(10)}_tp_{s_i-r_i}(y_i-z_i)p_{\tau-s_i}(\xi-y_i)+C_t^{(13)}p_{\tau-r_i}(\xi-z_i)p_{\tau-s_i}(\xi-y_i)\\
   &\ \ \ +C_t^{(12)}\phi_{r_i,z_i,s_i,y_i}(\tau,\xi)\Big)  dy_1dz_1ds_1dr_1dy_2dz_2ds_2dr_2d\xi d\tau.
\end{align*}
Utilizando nuevamente el lema \ref{LemaA.4a}, para $R>R_0\geq1$ y $0<s_i,r_i<t$,
\begin{align}
   &\norm{\mathcal{Y}_{R,t}^{(2)}}_2^{2}\notag\\
   &\leq \frac{C_t^{(14)}}{\sigma_{R,t}^2}\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\int_{0\leq r_1\leq s_1\leq \tau}\int_{0\leq r_2\leq s_2\leq \tau}\int_{\R^4}\prod_{i=1,2}\Big(C^{(10)}_tp_{s_i-r_i}(y_i-z_i)p_{\tau-s_i}(\xi-y_i)\notag\\
   &\ \ \ +C_t^{(13)}p_{\tau-r_i}(\xi-z_i)p_{\tau-s_i}(\xi-y_i)\notag\\
   &\ \ \ +C_t^{(12)}\phi_{r_i,z_i,s_i,y_i}(\tau,\xi)\Big)  dy_1dz_1ds_1dr_1dy_2dz_2ds_2dr_2d\xi d\tau.\label{pruebacotaphi1}
\end{align}
Aquí separamos primero la integral en $\R^4$ del producto como dos integrales separadas, 
\begin{align}
   \int_{\R^{4}}&\prod_{i=1,2}\Big(C^{(10)}_tp_{s_i-r_i}(y_i-z_i)p_{\tau-s_i}(\xi-y_i)\notag\\
   &+C_t^{(13)}p_{\tau-r_i}(\xi-z_i)p_{\tau-s_i}(\xi-y_i)+C_t^{(12)}\phi_{r_i,z_i,s_i,y_i}(\tau,\xi)\Big)  dy_1dz_1dy_2dz_2\notag\\
   &=\Bigg(\int_{\R^{2}}C^{(10)}_tp_{s_1-r_1}(y_1-z_1)p_{\tau-s_1}(\xi-y_1)+C_t^{(13)}p_{\tau-r_1}(\xi-z_1)p_{\tau-s_1}(\xi-y_1)\notag\\
   &\ \ \ +C_t^{(12)}\phi_{r_1,z_1,s_1,y_1}(\tau,\xi)dy_1dz_1\Bigg) \Bigg(\int_{\R^{2}}C^{(10)}_tp_{s_2-r_2}(y_2-z_2)p_{\tau-s_2}(\xi-y_2)\notag\\
   &\ \ \ +C_t^{(13)}p_{\tau-r_2}(\xi-z_2)p_{\tau-s_2}(\xi-y_2)+C_t^{(12)}\phi_{r_2,z_2,s_2,y_2}(\tau,\xi)dy_2dz_2\Bigg),\notag
\end{align}
y ahora integramos primero los núcleos gaussianos $p_t(x)$ en las variables $y_1,y_2,z_1$ y $z_2$, y posteriormente utilizamos el lema \ref{lemaA.2}, hallando que la expresión anterior es igual a
\begin{align}
   &=\Bigg(C^{(10)}_t\int_{\R}p_{s_1-r_1}(y_1-z_1)\int_{\R}p_{\tau-s_1}(\xi-y_1)dy_1dz_1\notag\\
   &\ \ \ +C_t^{(13)}\int_{\R}p_{\tau-r_1}(\xi-z_1)\int_{\R}p_{\tau-s_1}(\xi-y_1)dy_1dz_1+C_t^{(12)}\int_{\R^2}\phi_{r_1,z_1,s_1,y_1}(\tau,\xi)dy_1dz_1\Bigg)\notag\\
   &\ \ \ \ \cdot\Bigg(C^{(10)}_t\int_{\R}p_{s_2-r_2}(y_2-z_2)\int_{\R}p_{\tau-s_2}(\xi-y_2)dy_2dz_2\notag\\
   &\ \ \ +C_t^{(13)}\int_{\R}p_{\tau-r_2}(\xi-z_2)\int_{\R}p_{\tau-s_2}(\xi-y_2)dy_2dz_2+C_t^{(12)}\int_{\R^2}\phi_{r_2,z_2,s_2,y_2}(\tau,\xi)dy_2dz_2\Bigg)\notag\\
   &=\left(C_t^{(10)}+C_t^{(13)}+\int_{\R^2}\phi_{r_1,z_1,s_1,y_1}(\tau,\xi)dy_1dz_1\right)\left(C_t^{(10)}+C_t^{(13)}+\int_{\R^2}\phi_{r_2,z_2,s_2,y_2}(\tau,\xi)dy_2dz_2\right)\notag\\
   &\leq\left(C_t^{(15)}+C_t^{(16)}\left(1+\frac{1}{(s_1-r_1)^{1/4}}\right)\right)\left(C_t^{(15)}+C_t^{(16)}\left(1+\frac{1}{(s_2-r_2)^{1/4}}\right)\right),\label{pruebacotaphi2}                      
\end{align}
Combinando las desigualdades \eqref{pruebacotaphi1} y \eqref{pruebacotaphi2} se sigue que 
\begin{align*}
   \norm{\mathcal{Y}_{R,t}^{(2)}}_2^2&\leq  \frac{C_t^{(14)}}{\sigma_{R,t}^2}\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\int_{0\leq r_1\leq s_1\leq \tau}\int_{0\leq r_2\leq s_2\leq \tau}\left(C_t^{(15)}+C_t^{(16)}\left(1+\frac{1}{(s_1-r_1)^{1/4}}\right)\right)\\
   &\ \ \ \ \cdot\left(C_t^{(15)}+C_t^{(16)}\left(1+\frac{1}{(s_2-r_2)^{1/4}}\right)\right)ds_1dr_1ds_2dr_2d\xi d\tau\\
   &=\frac{C_t^{(14)}}{\sigma_{R,t}^2}\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\left(\int_{0\leq r\leq s\leq \tau}\left(C_t^{(15)}+C_t^{(16)}\left(1+\frac{1}{(s-r)^{1/4}}\right)\right)ds dr\right)^2d\xi d\tau\\
   &=\frac{C_t^{(17)}}{\sigma_{R,t}^2}\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\left(\int_{0\leq r\leq s\leq \tau}\left(1+\frac{C_t^{(18)}}{(s-r)^{1/4}}\right)ds dr\right)^2d\xi d\tau.
\end{align*}
Al realizar la integral en $s$ y $r$, obtenemos que 
\[
   \left(\int_{0\leq r\leq s\leq \tau}\left(1+\frac{C_t^{(18)}}{(s-r)^{1/4}}\right)ds dr\right)^2=\left(\frac{\tau^2}{2}-C^{(19)}_t\tau^{7/4}\right)^2,
\]
donde $C^{(19)}>0$ es una constante que solamente depende de $t$ y de $\sigma$. Pero dado que estamos integrando a $\tau$ en el intervalo $[0,t]$, el resultado de la integral anterior lo podemos acotar por una constante $C_t^{(20)}$ que depende solamente de $t>0$ y de $\sigma$. Deducimos así que
\[
   \norm{\mathcal{Y}_{R,t}^{(2)}}_2^2\leq \frac{C^{(20)}_t}{\sigma_{R,t}^2}\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)d\xi d\tau.
\]
Aplicamos ahora el lema \ref{LemaA.4a} e integramos sobre $\tau$ para obtener que 
\[
   \norm{\mathcal{Y}_{R,t}^{(2)}}_2^2\leq \frac{C^{(21)}_t}{\sigma_{R,t}^2}.
\] 
Por último, extraemos raíz y aplicamos nuevamente el lema \ref{lemaA.3a} para deducir que  
\begin{equation}\label{cotafinal2}
   \norm{\mathcal{Y}_{R,t}^{(2)}}_2^2\leq \frac{C^{(22)}_t}{\sqrt{R}},
\end{equation}
en donde $C_t^{(22)}$ depende solamente de $t>0$ y $\sigma$. Combinando las desigualdades \eqref{cotaprimersumando}, \eqref{cotafinal1} y \eqref{cotafinal2} hallamos que 
\[
   \sup_{x\in \R} \abs{f_{F_{R,t}}(x)-\Phi(x)}\leq \frac{C_t}{\sqrt{R}},
\]
donde $C_t$ es una constante positiva que depende solo de $t>0$ y de $\sigma$, completando la prueba.
\end{proofpart}
\qedhere
\end{proof}



% % % primero se demuestra una cota superior elemental para la distancia uniforme entre la densidad de una variable aleatoria $F$ que está dada coom un funcional de un proceso gaussiano isonormal, y la densidad de una variable aleatoria normal estándar.

% % % Para lograr dicha estimación, es necesario ver a la función de distribución de la variable aleatoria $F$ como la integral de Skorokhod (o como divergencia en el sentido del cálculo de Malliavin). Esto es, $F=\delta(v)$ para algún $v$.

% % % La idea es aplicar las técnicas de estimación anteriores a los promedios espaciales $F_{R,t}$ de la solución mild a la ecuación del calor. 

% % % Tenemos el siguiente resultado, el cual se encuentra en \cite{Caballero1998-hz}, y del cual haremos uso sin ahondar en su demostración.
% % % \begin{teo} 
% % % Sea $F\in \D^{1,1}$ y  sea $v\in L^{1}(\Omega;\mathcal{H})$ tal que $D_vF\neq 0$ c.s. Supongamos que $v/D_vF\in \text{Dom}(\delta)$. Entonces la distribución de $F$ tiene una densidad continua y acotada dada por 
% % % \[
% % % f_F(x)=\E\left[\1_{\{F>x\}}\delta \left(\frac{v}{D_vF}\right)\right]   
% % % \]
% % % \end{teo}$

% % % El resultado anterior es válido para cualquier espacio de Hilbert $\mathfrak{H}$. Aplicando el resultado anterior al contexto de una variable aleatoria adecuada, se tiene lo siguiente:

% % % \begin{teo} 
% % %  Sea $v\in \D^{1,6}(\Omega;\mathcal{H})$ y $F=\delta(v)\in \D^{2,6}$ con $\E\left[F\right]=0, \E\left[F^{2}\right]=1$ y $\left(D_vF\right)^{-1}\in L^4(\Omega)$. Entonces $v/D_vF \in \text{Dom}(\delta)$, $F$ admite una densidad $f_F(x)$ y se tiene la siguiente desigualdad
% % %  \[
% % %    \sup_{x\in \R} \abs{f_F(x)-\Phi(x)}\leq \left(\|F\|_4\|\left(D_vF\right)^{-1}\|_4+2\right)\|1-D_vF\|_2+\|\left(D_vF\right)^{-1}\|^{2}\|D_v \left(D_vF\right)\|_2,
% % %    \]
% % % donde $\Phi(x)$ es la función de densidad de una variable normal estándar.

% % % \end{teo}
% % % \begin{proof} 
% % %   Vamos a denotar por $F_R$ a los promedios espaciales de la ecuación estocástica del calor, y directamente por $\sigma$ a la varianza de los promedios espaciales.

% % %   Buscamos hallar una cota uniforme para $x\in \R$ para las siguientes cantidades
% % %   \[
% % %   \abs{f_F(x)-\phi(x)}. 
% % %   \]
% % %   Gracias a que los promedios espaciales admite una densidad dada por 
% % %   \[
% % %   f_F(x)=\E\left[\1_{\{F_R>x\}}\delta(\frac{v}{D_vF_R})\right], 
% % %   \]
% % %   podemos operar con estas cantidades de la siguiente forma:
% % %    \begin{align*}
% % %    f_F(x)=\E\left[\1_{\{F_R>x\}}\delta(\frac{v}{D_vF_R})\right]&=\E\left[\1_{\{F_R>x\}}\frac{1}{D_{v_R}}\delta(v_R)\right]-\E\left[\1_{\{F_R>x\}}\langle D \left(\frac{1}{D_{v_R}F_R}\right),v_R\rangle_{\mathcal{h}}\right]\\
% % %    &=\E\left[\1_{\{F_R>x\}}\frac{F_R}{D_{v_R}F_R}\right]+\E\left[\1_{\{F_R>x\}}\left(\frac{1}{\left(D_{v_R}F_R\right)^2}\right)D_{v_R}D_{v_R}F_R\right].
% % %    \end{align*}
% % %   Ahora bien, en el término de la derecha de antes, podemos acotar la cantidad de la siguiente forma:
% % %   \[
% % %    \E\left[\1_{\{F_R>x\}}\left(\frac{1}{\left(D_{v_R}F_R\right)^2}\right)D_{v_R}D_{v_R}F_R\right]\leq \sqrt{\E\left[\abs{D_{v_R}F_R}^{-4}\right]\E\left[\abs{D_{v_R}D_{v_R}F_R}^2\right]},
% % %   \]
% % %   de tal forma que el problema simplificado consiste en analizar el término de la izquierda, es decir, analizar.
% % %   \[
% % %    \E\left[\1_{\{F_R>x\}}\frac{F_R}{D_{v_R}F_R}\right] 
% % %   \]
% % %    Pero en cuanto a este término, notamos lo siguiente:
% % %   \begin{align*}
% % %    \E\left[\1_{[x,\infty)}(F_R)\frac{F_R}{D_vF_R}\right]&=\E\left[\1_{[x,\infty)}(F_R)F_R \left(\frac{1}{D_vF_R}-\frac{1}{\sigma^2}\right)\right] + \frac{1}{\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right]\\
% % %   \end{align*}
% % %   Ahora bien, nosotros tenemos la siguiente estimación 
% % %   \[
% % %   \sigma^2\approx \E\left[D_{V_R}F_R\right]\approx 1,  
% % %   \]
% % %   por lo que insertando dicha aproximación en la igualdad anterior, tenemos que 
% % %   \begin{align*}
% % %    \E\left[\1_{[x,\infty)}(F_R)F_R \left(\frac{1}{D_vF_R}-\frac{1}{\sigma^2}\right)\right] &+ \frac{1}{\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right]\\
% % %    &=\frac{1}{(D_vF_R)\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right] \left(\E\left[D_vF_R\right]-D_vF_R\right)\\
% % %    &+\frac{1}{\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right]\\
% % %   \end{align*}
% % %   Tenemos aquí dos términos nuevamente. El primero de ellos lo podemos acotar de la siguiente forma:
% % %   \[
% % %     \|F_R\|_{L^4(\Omega)}\|\left(D_{v_R}F_R\right)^{-1}\|_{L^{4}(\Omega)}\text{Var}\abs{D_{v_R}F_R}^{1/2}\le C\|DD_{v_R}F_R\|^{1/2}_{L^{2}(\Omega,\mathcal{h})},
% % %   \] 
% % %   donde $C$ es una constante adecuada.

% % %   Resta entonces analizar qué sucede con el siguiente término:
% % %   \[
% % %    \E\left[\1_{[x,\infty)}(F_R)F_R\right].
% % %   \]
% % %   Para ello, utilizamos la herramienta de Malliavin-Stein. Concretamente notemos que, sumando y restando el término $\1_{\{N>x\}}N$, donde $N$ es una variable aleatoria normal estándar, tenemos que, denotando por $g_x(F_R):=\1_{\{F_R>x\}}F_R$ y lo correspondiente para $g_x(N)$, 
% % %   \[
% % %    \E\left[\1_{\{F_R>x\}}\right]=\E\left[g_x(F_R)-g_x(N)\right]+\E\left[g_x(N)\right].
% % %   \]
% % %   El término de la izquierda en la ecuación anterior es sencillo de controlar, por lo que resta analizar la diferencia entre las esperanzas de $g_x$ evaluada en $F_R$ y en $N$. Para ello, de acuerdo al capítulo de método de Stein, notamos que 
% % %   \[
% % %   \E\left[g_x(F_R)-g_x(N)\right]=\E\left[F_R\psi_x(F_R)-\psi_x'(F_R)\right], 
% % %   \]
% % %   donde $y\psi_x(y)-\psi_x'(y)=g_x(y)-\E\left[g_x(N)\right]$.

% % %   Ahora bien, analizando la esperanza en términos de $\psi_x$, tenemos que 
  
% % %   \begin{align*}
% % %    \E\left[F_R\psi_x(F_R)\right]&=\E\left[\delta(v_R)\psi_x(F_R)\right]\\
% % %    &=\E\left[\abs{\langle v_R,DF_R\rangle}\psi_x'(F_R)\right]\\
% % %    &=\E\left[D_{V_R}F_R\psi_x'(F_R)\right]\\
% % %    &=\sigma^2 \E\left[\psi_x(F_R)\right]+\E\left[\left(D_{v_R}F_R-\sigma^2\right)\psi_x'(F_R)\right]\\
% % %    &\le \|\psi_x'\|_\infty \text{Var}\abs{D_{v_R}F_R}^{-1/2}\\
% % %    &\leq \|\psi_x'\|_\infty \|DD_{v_R}F_R\|_{L^2(\Omega,\mathfrak{H})}.
% % %   \end{align*}
% % %   Finalmente, para la prueba de la última cota, si nosotros vemos a la variable aleatoria normal $N$ como una divergencia, esto es, $N=\delta(h)$ para algún $h$ tal que $\|h\|=1$, entonces 
% % %   \[
% % %    f_N(x)=\E\left[\1_{\{N>x\}}\delta(h)\right]=\E\left[\1_{\{N>x\}N}\right]=\E\left[g_x(N)\right],
% % %   \]
% % %   por lo que concluimos la cota.
% % % \end{proof}

% % % Y para la prueba del resultado principal, es esencial el uso de dos resultados distintos: uno de ellos consiste en una cotas de momentos para la segunda derivada de Malliavin de la solución, y los momentos negativos de la proyección $DF_{R,t}$ en $v_{R,t}$, donde $F_{R,t}=\delta(v_{R,t})$. El primero de ellos se encuentra en el siguiente resultado:






%\section{Formulación Mild en el caso sin deriva}
%\section{Una ecuación estocástica para $Du$ y $D^2u$}
%\section{Densidad explícita de los promedios espaciales}
%Disclaimer: no se van a manejar los momentos inversos.
%\textit{Contribución del Kernel de Stein}
%\textit{Contribución de las derivadas de orden superior}

%\section{Aproximación uniforme de las densidades} %La sección integradora de los conceptos

\chapter{Conclusiones y trabajo futuro}
dfasdf
\appendix
\chapter{Resultados técnicos}
En este apéndice colocamos los tres resultados técnicos utilizados en la tercera parte de la prueba del resultado principal \ref{teoremaprincipal} de este texto.  Específicamente, los lemas A.1, A.2 y A.3 de este apéndice corresponden respectivamente a los lemas A.2 parte (a), A.3 parte (a) y A.4 parte (a), del apéndice presente en el artículo de \c{S}efika Kuzgun y David Nualart \cite{KUZGUN202268}. Las pruebas del lema A.1 y A.3 se pueden encontrar en el artículo antes mencionado, mientras que la prueba del lema A.2 se puede encontrar en \cite[proposición 3.1]{HUANG20207170}.

\begin{lema}\label{lemaA.2}
 Sea $\phi_{r,z,s,y}(t,x)$ de acuerdo al enunciado del teorema \ref{teocota2daderivada} y sea $(t,x)\in [0,\infty)\times\R$. Entonces para $0<r<s<t$,
 \[
 \int_{\R^2}\phi_{r,z,s,y}(t,x)dydz\leq C_t \left(1+\frac{1}{(s-r)^{1/4}}\right).
 \]
 \end{lema}
 \begin{lema}\label{lemaA.3a} 
  Sea $\sigma_{R,t}$ según la definición \ref{defpromediosespaciales}. Entonces se cumple que 
  \[
      \lim_{R\to\infty}\frac{\sigma_{R,t}^2}{R}=2\int_{0}^{t}\xi(s)ds,
  \] 
  en donde $\xi(s)=\E\left[\sigma^2(u(s,y))\right]$, donde tal y como la notación indica, la función $\xi$ es independiente del punto $y\in \R$ y además es acotada en intervalos compactos.
  \end{lema}
  \begin{lema}\label{LemaA.4a}
   Sean $t>0$ y $\phi_{R,t}$ según se define en \eqref{defnotacionphi}. Entonces existe un $R_0\geq1$, dependiente de $t$, tal que para cualesquiera $0<s<t$ y $R\geq R_0$,
   \[
      c_t\leq \int_\R\phi_{R,t}^2(s,y)dy\leq C_t,
   \]
   en donde la cota inferior es válida siempre que $t/2<s<t$.
  \end{lema}
\nocite{*}
\backmatter
\bibliography{build/Referencias_tesis}
\bibliographystyle{amsplain}

\end{document}