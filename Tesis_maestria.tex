\documentclass[letterpaper,twoside,12pt]{book} 
%\usepackage[left = 0.5in, right = 0.5in, top = 0.9in, bottom = 0.9in]{geometry}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage[spanish,es-nodecimaldot,es-tabla]{babel}
\usepackage[utf8]{inputenc}
\usepackage[svgnames,table]{xcolor}  
\usepackage{CIMATpreamble}  
\usepackage[dvipsnames]{xcolor}
%\usepackage[12pt]{extsizes}
\linespread{1.1}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathrsfs}
\usepackage{tikz-cd}
\usepackage[bbgreekl]{mathbbol}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage[colorlinks=true,linkcolor=red,citecolor=red]{hyperref}
\usepackage{apptools}
\AtAppendix{\counterwithin{lema}{chapter}}
\graphicspath{{img/}}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\B}{\mathcal{B}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\W}{\dot{W}}
\newcommand{\1}{\mathds{1}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\inv}{^{-1}}
\renewcommand{\to}{\rightarrow}
\newcommand{\ent}{\Longrightarrow}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\renewcommand{\qedsymbol}{$\blacksquare$}

\theoremstyle{definition}
\newtheorem{dfn}{Definición}
\theoremstyle{definition}
\newtheorem{teo}{Teorema}
\theoremstyle{remark}
\newtheorem{proofpart}{Parte}
\theoremstyle{definition}
\newtheorem{cor}{Corolario}
\theoremstyle{definition}
\newtheorem{prop}{Proposición}
\theoremstyle{definition}
\newtheorem{obs}{Observación}
\theoremstyle{definition}
\newtheorem{ejem}{Ejemplo}
\theoremstyle{definition}
\newtheorem{lema}{Lema}



\title{\textbf{}}
\author{Iván Irving Rosas Domínguez}
\date{\today}

\DeclareSymbolFontAlphabet{\mathbbm}{bbold}
\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
\DeclareMathSymbol\bbDelta  \mathord{bbold}{"01}


\author{Ivan Irving Rosas Domínguez}
\documentType{T E S I S}  
\title{Sobre convergencia de densidades de promedios espaciales para la ecuación estocástica del calor}
\degree{Maestría en Ciencias con Orientación en Probabilidad y Estadística}
\supervisor{Dr. Arturo Jaramillo Gil}
%\supervisorSecond{} 
\cityandyear{Guanajuato, Gto., ?? de ?? de 2024}


\begin{document}

\maketitle 

\thispagestyle{empty}  

\frontmatter

% % Dedication
\chapter*{}
\begin{flushright}%
 \emph{Dedicatoria \dots}
  \thispagestyle{empty}
\end{flushright}

% % Abstract
\chapter*{Introducción}
\addcontentsline{toc}{chapter}{Introducción}

Este trabajo está basado principalmente en el artículo de Sefika Kuzgun y David Nualart \cite{KUZGUN202268} publicado en el año 2022. El objetivo principal es estudiar la convergencia de las densidades de los promedios espaciales de la solución de la ecuación estocástica del calor (SHE, por sus siglas en inglés), en distancia uniforme, a la densidad de una distribución normal estándar. 

Trabajo previo se puede hallar desde el artículo de 1998 de Maria Emilia Caballero, Begoña Fernández Fernández y David Nualart \cite{Caballero1998-hz}, en donde se demuestra la existencia y unicidad de una densidad continua y acotada para una variable aleatoria $X$ que cumpla ciertas condiciones. Con la llegada del nuevo milenio, técnicas que mezclan al cálculo de Malliavin junto con el método de Stein son propuestas por Nourdin y Peccati en su monografía \cite{Nourdin_Peccati_2012}. La fusión de ambas disciplinas conlleva a una amalgama que resulta muy útil a la hora de establecer teoremas tipo límite central, así como dar tasas de convergencia explícitas para los mismos. 

Dentro de esta explosión de resultados, podemos citar el trabajo de Yaozhong Hu, Fei Lu y David Nualart \cite{HU2014814}, en donde se estudia la convergencia de las densidades de algunos procesos gaussianos. Posteriormente, en 2020, Jingyu Huang, Lauri Viitasaari y David Nualart en \cite{HUANG20207170} logran demostrar un teorema central del límite para la solución a la ecuación estocástica del calor. Más aún, ellos son capaces de demostrar cotas explícitas para la velocidad de convergencia de las distribuciones de la solución a la ecuación estocástica del calor a la distribución normal, utilizando para ello la distancia en variación total. 

Sefika Kuzgun y David Nualart, \cite{KUZGUN202268} logran, a partir de estos resultados, exhibir la existencia y unicidad de las densidades de los promedios espaciales de la solución a la ecuación estocástica del calor, y mostrar la convergencia de estas densidades, en distancia uniforme, a la densidad de una variable aleatoria normal, otorgando una tasa de convergencia explícita para la misma.

Para lograr este objetivo, es menester estudiar los conceptos utilizados en el trabajo de Kuzgun y Nualart. Es de destacar desde ahora que en este texto la intención no es, en lo más mínimo, dar un estudio amplio y completo de cada una de las disciplinas anteriores, sino cubrir de manera suficiente pero sucinta la herramienta técnica necesaria para abordar el problema. Así, en esta tesis se da un breve recorrido por las ideas que yacen en el núcleo de la teoría de ecuaciones diferenciales parciales estocásticas (SPDE, por sus siglas en inglés), del cálculo de Malliavin, del método de Stein, así como la amalgama que de estas últimas dos disciplinas surge con la intención de estudiar la existencia de densidades de variables aleatorias bajo ciertas hipótesis, y la deducción de teoremas límite. 

De lo anterior se desprende la estructura del presente texto. En el primer capítulo se estudiarán los elementos esenciales de ecuaciones diferenciales parciales estocásticas. Específicamente, se enuncia brevemente la heurística que da origen a la formulación de las mismas. Se realiza la construcción de la integral de Itô-Walsh, y con dicha integral se formula la solución mild de una ecuación diferencial parcial estocástica. Posteriormente nos enfocamos en el contexto de la ecuación estocástica del calor, así como en su solución. Finalmente se enuncian algunos resultados necesarios para abordar el problema principal.

El capítulo dos está dedicado al estudio de los elementos básicos del cálculo de Malliavin. Este capítulo se divide a su vez en dos partes. En la primera se hace un estudio del caso unidimensional, siendo siendo este caso estudiado con mediano detalle, aunque siempre manteniéndose al mínimo la teoría requerida para nuestro propósito. La segunda parte del mismo se compone del caso general. En esta segunda parte sólo se estudia en mediano detalle el aparato necesario para enunciar los resultados análogos al caso unidimensional, mientras que dichos resultados en múltiples ocasiones serán solo postulados, pero no demostrados. 

La composición del capítulo tres es similar a la del capítulo anterior. La primera parte del mismo consiste en un estudio breve del método de Stein, mencionando tangencialmente las aplicaciones que esta disciplina puede otorgar. La segunda parte del capítulo consiste principalmente en estudiar un resultado que garantiza la existencia y unicidad de la densidad de una variable aleatoria, así como en estudiar la existencia de una cota para la distancia uniforme entre la densidad de una variable adecuada, y la densidad de una variable normal estándar. Este último resultado resulta ser la columna vertebral sobre la que se sostiene el resultado principal, y es demostrada con detalle, y además es en esta parte en donde se hace uso de la amalgama que el cálculo de Malliavin y el método de Stein conforman. El estudio de las herramientas necesarias para abortar el problema principal culmina en esta parte.

El enunciado y prueba del resultado principal se encuentran hasta el capítulo cuarto. Es aquí en donde se hace rigurosa la formulación del problema que se plantea. Se enuncian dos lemas de carácter técnico que son esenciales en la prueba, y finalmente se estudia y concluye el problema principal. Se agrega un último capítulo en el cual se hace una breve reflexión sobre lo realizado, y se echa una mirada al panorama que este problema nos presenta.

Los primeros tres capítulos comienzan con un muy breve resumen histórico de la disciplina a estudiar, así como de la relevancia de la misma en la actualidad. El cuarto capítulo repasa el trabajo previo que ha sido mencionado al inicio de esta introducción, pero con mayor rigurosidad. 

Es importante mencionar que ninguno de los resultados enunciados y tampoco la gran mayoría de heurísticas sobre algún tema en particular, que están presentes en este trabajo, pertenecen al autor del mismo, por lo que siempre que se usen proposiciones o ideas ajenas, se procurará hacer la correspondiente mención de su origen. De esta forma, el aporte del autor está en explicar de la manera más simple que le ha sido posible los resultados, así como rellenar algunos detalles (y no todos) en las pruebas, o bien unificar algunas ideas sueltas de las fuentes consultadas. 


\textbf{Palabras clave:} \textit{Ecuación del calor estocástica, promedios espaciales, cálculo de Malliavin, método de Stein, existencia de densidades, convergencia uniforme, teoremas tipo límite, tasas de convergencia.}
% % Acknowledgements
\chapter*{Agradecimientos}
 \addcontentsline{toc}{chapter}{Agradecimientos}

A mis padres \ldots

% Table of contents and list of figures
\tableofcontents

%\listoffigures

% Chapters





\mainmatter

\chapter{Elementos de ecuaciones diferenciales parciales estocásticas}

En este capítulo hablaremos acerca del concepto de ecuación diferencial parcial estocástica.
Dichos objetos comenzaron a ser estudiados a mediados del siglo pasado con la llegada de la integral de Itô.
El trabajo de Itô y demás colegas para tratar ecuaciones diferenciales estocásticas inmediatamente sugirió el estudio de la posible generalización a ecuaciones diferenciales en varias variables.
Durante las décadas de los 50, 60 y 70, varios artículos hacían alusión a estos objetos, aunque no fue sino hasta finales de la década de 1970 cuando finalmente estas ecuaciones comenzaron a ser estudiadas como objetos matemáticos en sí.
Finalmente, en la década de 1980, varias monografías surgieron con el propósito de construir una teoría que cohesionara las ideas existentes en el área. Destaca en particular la monografía escrita por Walsh \cite{Walsh_J.B_Introduction_to_SPDEs} y publicada en 1986, en donde el autor aborda el estudio de éstas ecuaciones, dando un significado preciso a lo que significa plantear y resolver una ecuación diferencial que involucre procesos estocásticos cuyas trayectorias generalmente no son diferenciables, y que tengan variables tanto espaciales como una temporal.

Desde esta década múltiples trabajos en esta área han sido publicados. Esto, junto con el uso de herramientas como cálculo de Malliavin en el estudio de las mismas, han hecho de las Ecuaciones diferenciales parciales estocásticas una área bastante activa en la probabilidad moderna. 
\section{Introducción a SPDEs}
Comenzamos en este capítulo con la pregunta clave: ¿qué es una ecuación diferencial parcial estocástica? Para responder a esta pregunta, podemos repasar cada uno de los conceptos que nos llevan a esta idea. Partimos del mundo clásico en $\R^{N}:$ sean $x\in \R^{N}$, $t\in [0,\infty)$ y $u:\R^{N}\times[0,\infty)\to \R$ una función en las variables $x$ y $t$. Una ecuación diferencial parcial es una ecuación del tipo 

\begin{equation}\label{ec_dif_parc}
    F(t,x,u,Du,D^2u,...)=0,    
\end{equation}

donde $x$ y $t$ se interpretan como las variables espacial y temporal respectivamente, y $F$ es una función arbitraria que depende tanto de las variables espaciales como de la temporal, así como de la misma función $u$ y de sus derivadas de orden $\alpha=(\alpha_1,...,\alpha_n)$, donde $\alpha_i$ representa la derivada parcial en la $i$-ésima coordenada.

Si existe una función $u$ tal que $u\in C^{\alpha}\left(\R^{N}\times [0,\infty)\right)$ y además $u$ satisface la ecuación \eqref{ec_dif_parc}, donde $\alpha$ es el multi-índice más grande tal que el orden de todas las derivadas que aparecen en la ecuación son cubiertas por el mismo, entonces decimos que $u$ es una solución clásica a la ecuación diferencial parcial anterior. De esta manera, y en particular para la ecuación estocástica del calor homogénea clásica, la cual está dada por 
\[
    \partial_tu(x,t)-\frac{1}{2}\Delta_x u(x,t)=0,
\]
se entiende por solución clásica a una función $u:\R^{N}\times[0,\infty)\to \R$ tal que $u\in C^{2,1}\left(\R^{N}\times [0,\infty)\right)$, esto es, al menos todas las segundas derivadas en las variables espaciales existen y son continuas, y al menos la derivada en la variable temporal existe y es continua. 

Cabe destacar que la noción de solución de un ecuación diferencial parcial en el sentido clásico coincide con lo esperable: una función $u$ será solución de una ecuación diferencial si al menos tiene tantas derivadas como aquellas involucradas en la igualdad \eqref{ec_dif_parc}.

No obstante, en algunas ocasiones los problemas planteados requieren soluciones que no necesariamente cumplan tales características de regularidad, pero que intuitivamente deberían tener sentido.
 Un ejemplo muy conocido es aquél de la ecuación de onda con una condición inicial que no necesariamente es regular. Por ejemplo, si hablamos del problema homogéneo de la ecuación de onda unidimensional dado por:
\begin{equation}\label{wave_eq_sobolev}
    \begin{cases}
        \partial_{tt}u(x,t)-\kappa^2\partial_{xx}u(x,t)=0 & (x,t)\in [-\pi,\pi]\times[0,\infty),\\
        u(-\pi,t)=u(\pi,t)=0, & t\in [0,\infty),\\
        u(x,0)=u_0(x)=\pi-\abs{x} & x\in [-\pi,\pi],\\
        u_t(x,0)=0 & (x,t) \in [-\pi,\pi]\times[0,\infty),\\
    \end{cases}
\end{equation}
es claro que tenemos una inconsistencia al momento de colocar la condición inicial dada por el valor absoluto $\pi-|x|$: dicha función no es derivable en $x=0$. No obstante, desde un punto de vista intuitivo, claramente el problema tiene sentido: estamos modelando el comportamiento de una cuerda sujeta en sus extremos a los puntos $-\pi$ y $\pi$, la cual al soltarla tiene una posición inicial dada por $\pi-|x|$ y cuya velocidad inicial es constante $0$ en cualquier punto de la cuerda.\newline

La idea para hallar una solución al problema anterior a grandes rasgos consiste en reformular el problema de forma que se conserve la esencia del problema, y posteriormente obtener una solución a este problema reformulado. Una manera estándar es utilizar la teoría de distribuciones. En el caso particular de la ecuación de onda anterior, se procede formalmente de la siguiente manera:

Supongamos que $u$ es una solución al problema $\eqref{wave_eq_sobolev}$. En particular se debería cumplir que 
\[
\partial_{tt}u(x,t)=\kappa^2\partial_{xx}u(x,t),
\]
por lo que para cualquier función $\phi\in C^{\infty}_c\left([-\pi,\pi]\times [0,\infty)\right)$ tal que se anule en la frontera,, se tiene que al multiplicar e integrar, la ecuación anterior se convierte en:
\[
    \int_{[-\pi,\pi]\times [0,\infty)}\partial_{tt}u(x,t)\phi(x,t)dx dt=\kappa^2\int_{[-\pi,\pi]\times [0,\infty)}\partial_{xx}u(x,t)\phi(x,t)dx dt,
\]
y suponiendo que es válido utilizar integración por partes, la igualdad anterior equivale a
\[
\int_{[-\pi,\pi]\times [0,\infty)}u(x,t)\partial_{tt}\phi(x,t)dx dt=\kappa^2\int_{[-\pi,\pi]\times [0,\infty)}u(x,t)\partial_{xx}\phi(x,t)dx dt,
\]
en donde los términos evaluados en la frontera se anulan gracias a que las funciones $\phi$ (a menudo denominadas funciones de prueba) se anulan en la misma. Reordenando los términos anteriores, se obtiene una reformulación de la ecuación diferencial del problema \eqref{wave_eq_sobolev} la cual no necesita que la función $u$ sea derivable. A saber, decimos que $u$ es una solución débil de la ecuación diferencial asociada al problema \eqref{wave_eq_sobolev} si se cumple que 
\[
\int_{[-\pi,\pi]\times [0,\infty)}u(x,t)(\partial_{tt}\phi(x,t)-\kappa^2\partial_{xx}\phi(x,t))dx dt=0    
\]
para cualquier función $\phi\in C^{\infty}_c([-\pi,\pi]\times [0,\infty))$ que se anule en la frontera.

Claramente toda solución clásica (o fuerte) es a su vez una solución débil, ya que precisamente la regularidad de dichas soluciones permiten hacer de manera rigurosa los cálculos formales anteriores.
No obstante, la existencia de una solución débil en el sentido anterior no garantiza que dicha solución sea regular, por lo que en principio es más sencillo demostrar la existencia o unicidad de las soluciones débiles, a pesar de que estas gocen de menos propiedades que las soluciones fuertes o clásicas, las cuales no siempre existirán, como en el caso del problema $\eqref{wave_eq_sobolev}$.

Llegados a este punto, reflexionamos ahora en dirección de la probabilidad. Supongamos nuevamente que tenemos la ecuación de onda unidimensional, pero considerando el caso no homogéneo. Tenemos entonces la siguiente ecuación
\begin{equation}\label{wave_spde}
\partial_{tt}u(x,t)=\kappa^2\partial_{xx}u(x,t)+F(x,t), \qquad (x,t)\in [-\pi,\pi]\times[0,\infty)
\end{equation}
donde ahora $F$ es una función que representa la cantidad de presión por unidad de longitud que se aplica a la cuerda. Bajo condiciones de regularidad en la función $F$, podemos resolver el problema de manera clásica utilizando el método de separación de variables. Sin embargo, nos preguntamos ¿qué sucede si ahora $F$ representa una función que no sea diferenciable? Más aún, ¿qué pasa si consideramos a $F$ una \textit{perturbación aleatoria}?

Para dar un sentido más preciso a las ideas anteriores podemos pensar, para $x$ y $t$ fijos, a $F(x,t)$ como una variable aleatoria. Dicha variable depende tanto del espacio como del tiempo, y por ende podemos interpretar a $F$ como un proceso estocástico que evoluciona de manera espacio-temporal. Tal y como se comentó al inicio, es conocido que existen procesos estocásticos cuyas trayectorias son no diferenciables, por lo que en principio pensar en resolver una ecuación diferencial de este estilo de manera clásica se vislumbra una tarea complicada.

Más aún, ¿en qué sentido se debe interpretar una ecuación como \eqref{wave_spde}?, o bien, considerando la siguiente ecuación del calor no homogénea 
\begin{equation}\label{heat_spde}
    \partial_t{u(x,t)}=\frac{1}{2}\Delta_{x}u(x,t)+F(x,t), \qquad (x,t)\in \R^{N}\times[0,\infty),
\end{equation}
en donde el término $F$ sea aleatorio, ¿bajo qué significado hablamos de una solución de dicha ecuación?

Obsérvese que, si bien la teoría de distribuciones nos permite lidiar con la parte de la no regularidad de las condiciones del problema, para replicar el mecanismo que fue utilizado para reformular\eqref{wave_eq_sobolev} en un sentido débil, nos encontramos con una dificultad, ya que a no ser que el proceso $F(x,t)$ sea de variación finita, no será posible realizar de manera rigurosa la transición al momento de integrar por partes.

\subsection{Heurística de la formulación Mild}

Un vistazo más a detalle de lo anterior puede arrojarnos luz sobre cómo sortear el problema. Suponiendo que lo anterior es válido, una manera de proceder con la solución de la ecuación del calor no homogénea será la siguiente. Supongamos que tenemos la siguiente ecuación diferencial del calor unidimensional no lineal:
\[
\partial_tu(x,t)=\frac{1}{2}\partial_{xx}u(x,t)+W(x,t)u(x,t), \qquad (x,t)\in [0,L]\times [0,\infty) 
\]
donde $W(x,t)$ representa nuestra fuente de aleatoriedad (aunque bien puede pensarse como una función suficientemente suave en principio). Utilizando la técnica presentada en el ejemplo de la ecuación de onda, podríamos tomar una función $\phi \in C^{\infty}([0,L])$ cuya derivada se anule en la frontera, multiplicarla por la ecuación anterior e integrar con respecto al espacio y al tiempo, obteniendo que 
\[
\int_{0}^{L}u(x,t)\phi(x)dx-\int_{0}^{L}u(x,0)=\int_{0}^{L}\int_{0}^{t}\partial_{xx}u(x,s)\phi(x)dx ds+\int_{0}^{L}\int_{0}^{t}\partial_{xx}f(x,s)\phi(x)W(dx,ds)
\]
 
Resta entonces estudiar en qué sentido podemos integrar la ecuación \eqref{wave_spde} o \eqref{heat_spde} para sortear las dificultades que la aleatoriedad añade al problema.


\section{Integral de Itô-Walsh}
En esta sección presentamos la construcción de la integral de Itô-Walsh. Dicha integral nos permite construir procesos estocásticas que más adelante y en cierto sentido pueden verse como la solución de una ecuación diferencial parcial estocástica. Para lograr dicho objetivo, presentamos primeramente la teoría necesaria para construir la integral.
\subsection{Variables Gaussianas y Procesos Gaussianos}
Recordamos aquí las propiedades fundamentales de los procesos gaussianos. Salvo algunos casos especiales, la mayoría de las propiedades se enuncian sin demostración, por lo que nos referiremos a \cite{gall2016brownian} para revisar las pruebas de las mismas. Comenzamos definiendo la distribución Gaussiana.

\begin{dfn}
Sea $(\Omega, \F, \P)$ un espacio de probabilidad y consideremos a los reales dotados de la $\sigma-$álgebra de Borel, la cual denotamos por $\B(\R)$. Decimos que una variable aleatoria $X:(\Omega, \F)\to (\R,\B(\R))$ tiene distribución normal estándar (o bien, es una variable Gaussiana estándar), si la función de distribución de la misma está dada por: 
\[
F_X(t)=\int_{-\infty}^{t}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}dx.    
\]
En otras palabras, si la variable aleatoria $X$ tiene densidad (con respecto a la medida de Lebesgue en $\R$) dada por la función 
\[
f_X(t)=\frac{1}{\sqrt{2\pi}}e^{-t^2/2}.
\]
En tal caso, denotamos dicha situación por $X\sim$ Normal(0,1).
\end{dfn}

Directamente de la definición, tenemos la caracterización de la variable Gaussiana en términos de su transformada de Laplace y de su transformada de Fourier.
\begin{prop} 
 Sea $X$ una variable aleatoria con distribución normal estándar. Su transformada de Laplace en todo $\C$ está dada por 
 \[
 \mathcal{L}_X(\lambda):=\E\left[e^{\lambda X}\right]=e^{\lambda^2/2}, \qquad \lambda\in \C,
 \]
mientras que su transformada de Fourier (o función característica) en $\R$ está dada por 
\[
\mathcal{F}_X(\xi):=\E\left[e^{-i\xi X}\right]=e^{-\xi^2/2}, \qquad \xi \in \R. 
\]
 \end{prop}
Se puede ver usando alguna de las transformadas anteriores y usando un argumento inductivo que una variable normal estándar tiene momentos $n$-ésimos de cualquier orden, y que los mismos están dados por 
 \begin{itemize}
   \item $\E\left[X^{2n}\right]=(n-1)!!=\frac{(2n)!}{2^nn!}=1\cdot3\cdot5\cdot_...\cdot (2n-1), \qquad n\geq0$.
   \item $\E\left[X^{2n+1}\right]=0, \qquad n\geq0$.
 \end{itemize}
 En particular, se cumple que $\E\left[X^{n+1}\right]=n\cdot \E\left[X^{n-1}\right]$, para $n\geq1$.

Diremos también que una variable aleatoria Gaussiana tiene media $\mu\in \R$ y varianza $\sigma^2>0$ si se cumple que $Y=\sigma X+\mu$ para $X$ una variable normal estándar. En tal caso, denotaremos la situación por $Y\sim$ Normal$(\mu,\sigma^2)$. En vista de la proposición anterior y de la definición, se tiene la siguiente proposición.
\begin{prop} 
 Sea $Y$ una variable aleatoria normal con media $\mu\in \R$ y varianza $\sigma^2>0$. Entonces las siguientes tres proposiciones son equivalentes a este hecho.
 \begin{itemize}
    \item $\mathcal{L}_Y(\lambda)=e^{\mu\lambda+\sigma^2\lambda^2/2}, \qquad \lambda \in \C,$
    \item $\F_X(\xi)=e^{i\mu\xi-\sigma^2\xi^2/2}, \qquad \xi \in \R$,
    \item La distribución de $Y$ tiene densidad con respecto a la medida de Lebesgue en $\R$ dada por 
    \[
        f_Y(y)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(y-\mu)/2\sigma^2}, \qquad t\in \R.    
    \]
 \end{itemize}
 \end{prop}
 Por convención, diremos que una variable $Y$ tiene distribución normal con media $\mu\in \R$ y varianza $\sigma^2=0$ (denotado por $Y\sim$ Normal$(\mu,0)$) si se cumple que $Y=\mu$, \  $\lambda$-casi seguramente, donde $\lambda$ denota la medida de Lebesgue en $\R$.

 Pasamos ahora a definir los vectores Gaussianos o vectores normales conjuntos. 
 \begin{dfn} 
  Sea $(\R^d, \langle\cdot,\cdot\rangle)$ el espacio $\R^{d}$ dotado de su producto interno usual. Decimos que una variable aleatoria $X:(\Omega,\F)\to (\R^{d},\B(\R^d))$ es un vector gaussiano, o un vector normal conjunto, si para cualquier $u\in \R^d$, se tiene que $\langle u,X\rangle$ es una variable aleatoria Gaussiana (en $\R$).
  \end{dfn}
Presentamos un primer resultado sobre vectores aleatorios Gaussianos.
\begin{prop} 
Sea $X$ un vector aleatorio Gaussiano en $\R^{d}$. Entonces existe una función $\mu_X\in\R^d$ y una forma cuadrática no negativa $q_X:\R^d\to \R$ tales que para cualquier $u\in \R^d$,
\begin{itemize}
    \item $\E\left[\langle u,X\rangle\right]=\langle u,\mu_X\rangle$
    \item $\text{Var}\left(\langle u,X\rangle\right)=q_X(u)$.
\end{itemize}
De hecho, si tenemos una base ortonormal $(e_{j})_{j\geq1}$ de $\R^{d}$, entonces podemos escribir a $X$ de la siguiente manera:
\[
X=\sum_{j=1}^{d}\langle X,e_j\rangle e_j,  
\]
donde $X_j:=\langle X,e_j\rangle$, por definición, es una variable aleatoria Gaussiana, para cualquier $j\geq1$. De lo anterior, se sigue que, escribiendo $u\in \R^d$ como $u=\sum_{j=1}^{d}u_je_j$, se tiene que 
\begin{itemize}
    \item $\mu_X=\E\left[X_j\right]e_j$.
    \item $q_X(u)=\sum_{j=1}^{d}\sum_{i=1}^{d}u_iu_j \text{Cov}\left(X_i,X_j\right)$.
\end{itemize}
 \end{prop}

Se sigue de la proposición anterior la fórmula de la función característica de un vector gaussiano.
\begin{prop} 
 Sea $X$ un vector aleatorio gaussiano y $(e_j)_{j\geq1}$ una base ortonormal de $\R^{d}$. Entonces, al ser $\langle u,X\rangle\sim$ Normal$(\langle u,\mu_X\rangle,q_X(u))$, se tiene que %Si denotamos por $\E\left[X\right]:=\sum_{j=1}^{d}\E\left[X_j\right]e_j$, entonces 
 \[
 \E\left[e^{i\langle u,X\rangle}\right]=e^{i \langle u,\mu_X\rangle-\frac{1}{2}q_X(u)}, \qquad u\in \R^d.
 \]
 \end{prop}
Y en vista de las dos proposiciones anteriores, está la siguiente caracterización de variables aleatorias Gaussianas independientes.
\begin{prop} 
 Sea $X$ un vector aleatorio gaussiano en $\R^{d}$, y sea $(e_j)_{j\geq1}$ una base ortonormal de $\R^{d}$. Entonces $X_1,...,X_d$ son variables aleatorias independientes si y solo sí la matriz de covarianzas $\Sigma=(\text{Cov}\left(X_i,X_j\right))_{1\leq i,j\leq d}$ es diagonal. Equivalentemente, $X_1,...,X_d$ son independientes si y solo si la forma cuadrática $q_X$ está en forma diagonal en la base $(e_j)_{j\geq1}$.
 \end{prop}

 Pasamos ahora a estudiar procesos estocásticos formados por variables aleatorias gaussianas.
\begin{dfn} 
 Sea $T$ un conjunto arbitrario y sea $G=(G(t))_{t\in T}$ una colección de variables aleatorias indexadas por $T$. Decimos que $G$ es un proceso Gaussiano, o un campo aleatorio Gaussiano si para cualesquiera $t_1,...,t_k\in T$, se tiene que $X=(G(t_1),...,G(t_k))$ es un vector gaussiano, $k\geq1$. 
\end{dfn}

Equivalentemente, un proceso estocástico $(G(t))_{t\in T}$ es Gaussiano si cualquier combinación lineal finita de variables $G(t)$, $t\in T$ es Gaussiana, lo cual de acuerdo a nuestra exposición, también equivale a decir que las distribuciones finito-dimensionales de un proceso Gaussiano $(G(t))_{t\in T}$ están dictadas por medio de variables aleatorias gaussianas.

A menudo es importante saber cuando un proceso gaussiano es independiente de otro proceso gaussiano. Para ello, recordamos la definición de $\sigma$-álgebra generada por un conjunto de variables.

\begin{dfn} 
 Sea $H$ una familia de variables aleatorias definidas en un espacio de probabilidad $(\Omega, \F, \P)$. La $\sigma$-álgebra generada por $H$ es la $\sigma$-álgebra más pequeña en $\Omega$ tal que todas las variables $X\in H$ son medibles con respecto a dicha $\sigma$-álgebra. A tal estructura la denotamos como $\sigma(H)$.  
 \end{dfn}
Enunciamos ahora un importante resultado con respecto a la independencia de dos conjuntos de variables aleatorias gaussianas. Este resultado nos dice que, en cierto sentido, dos conjuntos de variables aleatorias Gaussianas son ortogonales si y solo si son independientes entre sí. 

\begin{prop}\label{Gaussi_indep} 
 Sean $G_1$, $G_2$ dos familias de variables aleatorias Gaussianas centradas (i.e. con media cero). Denotemos por $H_1$ y $H_2$ al subespacio lineal cerrado de $L^{2}(\P)$ generado por dichas variables. Entonces dichos espacios están formados por variables aleatorias Gaussianas centradas.

 Más aún, son equivalentes 
 \begin{itemize}
    \item Los subespacios $H_1$ y $H_2$ son ortogonales entre sí en $L^{2}(\P)$.
    \item Las $\sigma$-álgebras $\sigma(H_1)$ y $\sigma(H_2)$ son independientes.
 \end{itemize}
 Es importante recordar que esta propiedad es muy particular del contexto Gaussiano.
 \end{prop}
Ahora bien, dado un proceso Gaussiano $G$, tenemos dos funciones asociadas al mismo.
\begin{dfn} 
 Sea $G=(G(t))_{t\in T}$ un proceso gaussiano. Definimos las funciones de media y covarianza del proceso $G$ respectivamente como sigue.
 \begin{enumerate}
    \item $\mu(t):=\E\left[G(t)\right]$, \qquad para cualquier $t\in T$,
    \item $\Gamma(s,t):= \text{Cov}\left(G(s),G(t)\right)$, \qquad para cualesquiera $s,t\in T$.
 \end{enumerate}
 \end{dfn}
Dichas funciones con valores en $\R$ determina la colección de distribuciones finito-dimensionales del proceso. 
De hecho, gracias a las proposiciones anteriores es claro que para cualesquiera subíndices $(t_1,...,t_k)\subseteq T$, la ley del vector Gaussiano $X=(G(t_1),...,G(t_k))$ está determinada de manera única. 

Lo anterior pues $\mu_X:=(\E\left[G(t_1)\right],...,\E\left[G(t_k)\right])=(\mu(t_1),...,\mu(t_k))$ y $q_X$ la forma cuadrática asociada que está determinada por la matriz de covarianzas de $X$, está en términos de la función $\Gamma$ definida antes, a saber, $\Sigma_X=\left(\text{Cov}\left(G(t_i),G(t_j)\right)\right)_{1\leq i,j\le k}=\left(\Gamma(t_i,t_j)\right)_{1\le i,j\le k }$.

Es consecuencia de que la forma cuadrática $q_X$ para cualquier vector $X$ que componga una distribución finito-dimensional de $G$, sea no negativa definida y simétrica, el hecho de que la función de covarianzas $\Gamma$ sea no negativa definida. A saber, si $c:T\to \R$ es una función con soporte finito, entonces 
\[  
    \sum_{T\times T}^{}c(s)c(t)\Gamma(s,t)\geq0
\]
Lo valioso de conocer las funciones anteriores es que, al caracterizar completamente las distribuciones finito-dimensionales del proceso Gaussiano $G$, todo el proceso está caracterizado por medio de dichas funciones, por lo que basta conocer dichas funciones para construir un proceso Gaussiano. Este es el contenido del siguiente teorema.

\begin{prop} 
 Sean $\Gamma:T\times T\to \R$ y $\mu:T\to \R$, tales que $\Gamma$ es simétrica y no negativa definida. Entonces existe un proceso gaussiano $(G(t))_{t\in T}$ en un espacio de probabilidad $(\Omega, \F,\P)$ apropiado, tal que para cualesquiera $t\in T$, $G(t)\sim$ Normal$(\mu(t),\Gamma(t,t))$, con $\mu$ y $\Gamma$ las respectivas funciones de media y covarianza del proceso $G$.
 \end{prop}
 La demostración de lo anterior es una consecuencia del Teorema de Consistencia (o extensión) de Kolmogorov (ver Teorema 6.3 en \cite{gall2016brownian} ). Para terminar esta sección, a continuación presentamos un par de ejemplos de procesos Gaussianos construidos de esta forma.
\begin{ejem}[\textbf{El movimiento Browniano}]
Consideremos el caso en el que $T=[0,\infty)$, $\mu(t)=0$ para cualquier $t\geq0$ y $\Gamma(s,t)=s\wedge t=\min\{s,t\}$, para $s,t\geq0$. Claramente $\Gamma$ es simétrica, y además nótese que para cualesquiera $c:T\to\R$ función de soporte finito, se tiene que
\begin{align*}
\sum_{T\times T}^{}c(s)c(t)\Gamma(s,t)&=\sum_{s\in T}^{}\sum_{t\in T}c(s)c(t)(s\wedge t)\\
&=\sum_{s\in T}^{}\sum_{t\in T}c(s)c(t)\int_{0}^{\infty}\1_{[0,s]}(x)\1_{[0,t]}(x)dx\\
&=\int_{0}^{\infty}\sum_{s\in T}\sum_{t\in T}\1_{[0,s]}(x)c(s)\1_{[0,t]}(x)c(t)dx\\
&=\int_{0}^{\infty}\abs{\sum_{t\in T}\1_{[0,t]}(x)c(t)}^2dx\\
&\geq0,
\end{align*}
por lo que por la proposición anterior, existe en un espacio de probabilidad adecuado un proceso gaussiano que denotaremos por $(B(t))_{t\geq0}$, con función de medias 0 y función de covarianza $\Gamma(s,t)=s\wedge t$. Dicho proceso es el movimiento Browniano estándar.
 \end{ejem}
El siguiente ejemplo es de suma importancia y es parte esencial en el resto del texto.

\begin{ejem}[\textbf{El ruido blanco en $\R^{d}$}] 
Sea $T=\B(\R^{d})$ el conjunto de los borelianos en $\R^d$.
Dado que estamos hablando de subconjuntos de $\R^{d}$, cambiaremos la notación al indicar con letras mayúsculas a los elementos de $T$.
Consideremos nuevamente la función de medias $\mu(A):=0$, para cualquier $A\in \B(\R^{d})$ y ahora consideremos la función $\Gamma(A,B):=\lambda^{d}(A\cap B)$, donde $\lambda^{d}:\B(\R^{d})\to [0,1]$ es la medida de Lebesgue en $\R^{d}$.

 Claramente $\Gamma$ es una función simétrica, y además, para $c:T\to\R$ función de soporte finito, se tiene que 
 \begin{align*}
    \sum_{T\times T}^{}c(A)c(B)\Gamma(A,B)&=\sum_{A\in T}^{}\sum_{B\in T}c(A)c(B)\lambda^{d}(A\cap B)\\
    &=\sum_{A\in T}^{}\sum_{B\in T}c(A)c(B)\int_{\R^d}\1_{A}(x)\1_{B}(x)\lambda^{d}(dx)\\
    &=\int_{\R^{d}}\sum_{A\in T}\sum_{B\in T}\1_{A}(x)c(A)\1_{B}(x)c(B)\lambda^{d}(dx)\\
    &=\int_{\R^{d}}\abs{\sum_{A\in T}\1_{A}(x)c(A)}^2\lambda^{d}(dx)\\
    &\geq0,
    \end{align*}
por lo que existe un proceso Gaussiano que denotaremos por $(\dot{W}(A))_{A\in \B(\R^{d})}$ en un espacio de probabilidad adecuado, tal que su función de medias es $0$ y su función de covarianzas es $\Gamma(A,B)=\lambda^{d}(A\cap B)$. Dicho proceso estocástico es conocido como \textit{ruido blanco} en $\R^{d}$.

Observemos que, si $A\cap B=\varnothing$, entonces $\Gamma(A,B)=\text{Cov}\left(\W(A),\W(B)\right)=0$, por lo que al ser variables Gaussianas, estas son independientes según lo visto antes.

Se sigue que si $A,B\in \B(\R^d)$, entonces aprovechando que el proceso es Gaussiano centrado, tenemos que
\begin{align*}
    &\text{Cov}\left(\W(A\cup B)-\W(A)-\W(B)+\W(A\cap B),\W(A\cup B)-\W(A)-\W(B)+\W(A\cap B)\right)\\
    &=\text{Cov}\left(\W(A\cup B),\W(A\cup B)\right)+\text{Cov}\left(\W(A),\W(A)\right)+\text{Cov}\left(\W(B),\W(B)\right)\\
    &\quad +\text{Cov}\left(\W(A\cap B),\W(A\cap B)\right)-2 \text{Cov}\left(\W(A\cup B),\W(A)\right)-2 \text{Cov}\left(\W(A\cup B), \W(B)\right)\\
    &\quad +2 \text{Cov}\left(\W(A\cup B), \W(A\cap B)\right)+2 \text{Cov}\left(\W(A),\W(B)\right)-2 \text{Cov}\left(\W(A),\W(A\cap B)\right)\\
    &\quad -2 \text{Cov}\left(\W(B),\W(A\cap B)\right)\\
    &=\lambda^{d}(A\cup B)+\lambda^{d}(A)+\lambda^{d}(B)+\lambda^{d}(A\cap B)-2\lambda^{d}((A\cup B)\cap A)-2\lambda^{d}((A\cup B)\cap B)\\
    &\quad+2\lambda^{d}((A\cup B)\cap (A\cap B))+2\lambda^{d}(A\cap B)-2\lambda^{d}(A\cap (A\cap B))-2\lambda(B\cap(A\cap B))\\
    &=\lambda^{d}(A\cup B)+\lambda^d(A)+\lambda^{d}(B)+\lambda^{d}(A\cap B)-2\lambda^{d}(A)-2\lambda^{d}(B)+2\lambda^{d}(A\cap B)\\
    &\quad +2\lambda^{d}(A\cap B)-2\lambda^{d}(A\cap B)-2\lambda^{d}(A\cap B)\\
    &=\lambda^{d}(A\cup B)-\lambda^d(A)-\lambda^{d}(B)+\lambda^{d}(A\cap B)\\
    &=\lambda^{d}(A\cup B)-\lambda^{d}(A\cup B)\\
    &=0.
\end{align*}

por lo que la variable $\W(A\cup B)-(\W(A)+\W(B)-\W(A\cap B))$ tiene varianza cero. Luego,
\begin{equation}\label{Wdotmeasure}    
    \W(A\cup B)=\W(A)+\W(B)-\W(A\cap B) \qquad \P-\text{casi seguramente}.
\end{equation}


\end{ejem}
Lo anterior nos podría hacer pensar que el ruido blanco es una medida signada. Sin embargo, esto es falso (ver ejemplo 3.16 de \cite{Khoshnevisan2009}). No obstante, podemos utilizar dicho proceso como base para construir una integral.

\begin{ejem}[\textbf{El proceso isonormal}] 
Dado un ruido blanco $\left(\W(A)\right)_{A\in \B(\R^d)}$, nos gustaría definir $\W(h)$ para alguna función $h$ adecuada. Es bien conocido en teoría de la medida un mecanismo estándar para definir nuevos objetos a partir de objetos más sencillos, y luego extender por aproximación. Seguiremos esta maquinaria estándar a continuación.

Sea $A\in \B(\R^d)$. Definimos $\W(\1_A):=\W(A)$, y para cualesquiera $A_1,...,A_n\in \B(\R^{d})$ disjuntos, y constantes $c_1,...,c_n\in \R$,  definimos
\[
\W \left(\sum_{k=1}^{n}c_k\1_{A_k}\right):=\sum_{k=1}^{n}c_k\W(A_k)    
\]
Nótese que si existen dos representaciones distintas de una función simple, de acuerdo a \eqref{Wdotmeasure}, la variable de la definición anterior es consistente ya que habrá una igualdad $\P-$ casi seguramente. Por otro lado, dado que los conjuntos $A_1,...,A_n$ son disjuntos, las variables $\W(A_1),...,\W(A_n)$ son independientes entre sí. Luego, observamos que al ser centradas,
\begin{align*}
    \norm{\W \left(\sum_{k=1}^{n}c_k\1_{A_k}\right)}_{L^{2}(\P)}^2&=\text{Var}\left(\sum_{k=1}^{n}c_k\W(A_k)\right)\\
    &=\sum_{k=1}^{n}c_k^2 \E\left[\W^2(A_k)\right]\\
    &=\sum_{k=1}^{n}c_k^2 \Gamma(A_k,A_k)\\
    &=\sum_{k=1}^{n}c_k^2\lambda^{d}(A_k)\\
    &=\int_{\R^d}\sum_{k=1}^{n}c_k^2\1_{A_k}\lambda^d(dx)\\
    &=\norm{\sum_{k=1}^{n}c_k\1_{A_k}}^2_{L^{2}(\R^{d})},
\end{align*}
donde en la última igualdad utilizamos que, al ser $A_i\cap A_j=\varnothing$, se tiene que $\1_{A_i}\1_{A_j}=0$ para $i\neq j$. De lo anterior deducimos que la aplicación $f\longmapsto \W(f)$, que envía funciones simples en variables Gaussianas, preserva la norma de los respectivos espacios.

Es un resultado conocido en teoría de la medida que, si tenemos ahora una función $f\in L^{2}(\R^{d})$, existe una sucesión de variables aleatorias simples en $L^{2}(\R^d)$ tales que $f_n\to f$ en la norma de dicho espacio. Luego, la sucesión $(f_n)_{n\ge 1}$ forma una sucesión de Cauchy de funciones simples en $L^2(\R^{d})$. Dado que la norma se preserva para estas funciones entre los espacios $L^2$ según lo visto antes, la sucesión de variables $(\W(f_n))_{n\ge 1}$ es de Cauchy en $L^{2}(\P)$, por lo que al ser un espacio completo, existe una única variable en $L^{2}(\P)$, que denotaremos por $\W(f)$, tal que $\W(f_n)\longrightarrow \W(f)$ en norma $L^{2}(\P)$.

Consideramos así al espacio $L^{2}(\R^{d})$, y a la colección de variables aleatorias $(\W(f))_{f\in L^{2}(\R^{d})}$. A dicho proceso estocástico se le conoce como el \textit{proceso isonormal}. Una característica fundamental de dicho proceso es que la aplicación $A\mapsto\W(A)$ es una isometría, conocida como \textit{isometría de Wiener}. Al elemento $\W(f)$ para $f\in L^2(\R^{d})$ se le conoce como \textit{integral de Wiener de f}, y a menudo se denota como
\[
\W(f)=\int f W(dx).    
\] 
El proceso isonormal se puede también obtener vía su función de medias y su función de covarianzas. En efecto, tomando $T=L^{2}(\R^{d})$, haciendo nuevamente $\mu(f)=0$ para cualquier $f\in L^2(\R)$, y definiendo la función de covarianzas $\Gamma$ como
\[
\Gamma(f,g)=\int_{\R^d}fg \lambda^d(dx)=\langle f,g\rangle_{L^{2}(\R^{d})}, \qquad f,g\in L^2(\R^{d}),
\]
entonces el proceso Gaussiano resultante, que denotamos por $(\W(f))_{f\in L^{2}(\R^{d})}$ es el proceso isonormal que construimos antes vía un ruido blanco. 
 \end{ejem}

\subsection{Medidas martingala}

En esta parte del texto, estudiamos el concepto de medidas martingala. Dichos procesos estocásticos son una pierda angular en la construcción de la integral de Itô-Walsh, como veremos a continuación.

Recordemos que si tenemos un ruido blanco $(W(A))_{A\in \B(R^{d})}$, entonces para cualesquiera $A,B \in \B(\R^{d})$, se tiene la siguiente igualdad
\[
\W(A\cup B)=\W(A)+\W(B)-\W(A\cap B), \qquad \P-\text{casi seguramente.}   
\]
Sin embargo, como también se mencionó en la subsección pasada, a pesar de que esto nos puede hacer pensar en que el ruido blanco constituye una medida aleatoria signada, lo anterior es falso. No obstante, se tiene el siguiente resulado con respecto al mismo.
\begin{prop}\label{Finito_aditiv_ruido_blanco}
 El proceso $(\W(A))_{A\in \R^{d}}$ cumple las siguientes propiedades
 \begin{itemize}
    \item $\W(\varnothing)=0$, \qquad $\P-\text{casi seguramente,}$
    \item Para cualquier sucesión de conjuntos ajenos $(A_n)_{n\geq 1}\subseteq \B(\R^d)$, se cumple que 
    \[
    \W \left(\bigcup_{k=1}^\infty A_k\right)=\sum_{k=1}^{\infty}\W(A_k), \qquad \P-\text{casi seguramente,} 
    \]
 \end{itemize}
 en donde la suma infinita anterior converge en $L^2(\P)$. En otras palabras, el 
 ruido blanco es una medida signada, $\sigma$-finita, $L^2(\P)$-valuada.
\end{prop}
\begin{proof} 
   Es un resultado conocido de teoría de la medida que, si queremos probar que la aplicación anterior es una medida signada, y ya hemos probado que es finito-aditiva, entonces basta con tomarnos una sucesión decreciente de conjuntos $(A_n)_{n\geq1}\subseteq\B(\R^{d})$ tal que al intersecar todos los elementos obtengamos el conjunto vacío, y probar que $\W(A_n)\to 0$ en $L^{2}(\P)$ para concluir.

   En efecto, sea $(A_n)_{n\geq1}$ una sucesión como la descrita antes. Directamente por definición del ruido blanco, 
   \[
   \norm{\W(A_n)}_{L^2(\P)}^2=\E\left[\W^2(A_n)\right]=\Gamma(A_n,A_n)=\lambda^{d}(A_n)\xrightarrow[n\to\infty]{}0,    
   \]
   ya que al ser $\lambda^d$ una medida, se cumple la propiedad de continuidad con la sucesión $(A_n)_{n\geq1}$.

   Para concluir que la medida es $\sigma-$finita, dado que $\R^{d}$ puede ser cubierto con una sucesión numerable de conjuntos compactos, basta ver que para cualquier conjunto compacto $K\subseteq\R^{d}$, se tiene que $\norm{\W(K)}_{L^ {2}(\R^{d})}^2<\infty$. Pero esto es claro, ya que 
   \[
       \norm{\W(K)}_{L^ {2}(\R^{d})}^2=\E\left[\W^2(K)\right]=\lambda^{d}(K)<\infty,
   \]
   ya que los compactos son acotados en $\R^{d}$.
 \end{proof}  
 \begin{obs}
   Es importante distinguir el que el ruido blanco forme una medida $L^{2}(\P)$-valuada a que forme una medida signada aleatoria en los borelianos de $\R^d$. Esto es, nótese que la segunda propiedad de la proposición posee el cuantificador fuera del conjunto de probabilidad 1. Más específicamente, la proposición anterior \textit{no} está asegurando que 
   \[
   \P\left(\left\{\forall \ (A_n)_{n\geq1}\subseteq\B(\R^{d}), \ \ \W \left(\bigcup_{k=1}^\infty A_k\right)=\sum_{k=1}^{\infty}\W(A_k)\right\}\right)=1  
   \]
\end{obs}
Ahora, a menudo es posible considerar una dimensión como la dimensión temporal, de forma que se puede estudiar un ruido blanco definido en el espacio $[0,\infty)\times\R^{d}$. El hecho de pensar al intervalo $[0,\infty)$ como una dimensión temporal nos permite definir un nuevo proceso conocido como el \textit{proceso de ruido blanco}, $(W_t)_{t\geq0}$, definido como 
\[
W_t(A):=\W \left([0,t]\times A\right), \qquad A\in \B(\R^{d}).    
\]
Este es un proceso estocástico que 'evoluciona en el tiempo', pero a su vez, es un ruido blanco en $A$. Recordemos también que la noción de filtración como sucesión de $\sigma$-álgebras ordenadas con respecto a la contención concreta la idea de proceso que evoluciona en el tiempo. Definimos entonces la filtración del proceso de ruido blanco como sigue
\[
\F_t:=\sigma \left(\W([0,t]\times A):0\leq s\leq t, A\in \B(\R^{d})\right), \qquad t\geq0.    
\]
Claramente la familia $(\F_t)_{t\geq0}$ forma una filtración, y además el proceso $(W_t)_{t\geq0}$ es adaptado a dicha filtración.

Una propiedad muy interesante que posee el proceso $(W_t)_{t\geq0}$, visto como un proceso que evoluciona en el tiempo, es que es una medida aleatoria (en el sentido visto antes), y también una martingala. 

\begin{prop} 
Sea $(W_t(A))_{t\geq0, A\in \B(\R)}$ un ruido blanco en $[0,\infty)\times \R^{d}$. Entonces dicho proceso forma una 'medida martingala' en el siguiente sentido:
\begin{itemize}
   \item Para cualquier $A\in \B(\R^{d})$, se cumple $W_0(A)=0, \ \P-$casi seguramente.
   \item Si $t>0$, entonces $W_t$ es una medida signada $\sigma$-finita, $L^{2}(\P)$-valuada.
   \item Para cualquier $A\in \R^{d}$, el proceso $(W_t(A))_{t\geq0, A\in \B(\R)}$ es una martingala de media cero.
\end{itemize}
\end{prop}
\begin{proof} 
 Nótese que por definición, $W_0(A)=\W(\{0\}\times A)$, y dicha variable es tal que 
 \[
 \E\left[\W^2(\{0\}\times A)\right]=\Gamma(0\times A,0\times A)=\lambda{0}\cdot\lambda^{d}(A)=0,  
 \]
 por lo que dicha variable aleatoria es la variable 0 con probabilidad 1.

 Por otro lado, para $t>0$, el que $W_t:\B(\R^{d})\to L^{2}(\P)$ sea una medida signada $\sigma$-finita se sigue de la propiedad siguiente: para cualquier $A\in \B(\R^{d})$, 
 \[
 \E\left[\W([0,t]\times A)\right]=\Gamma([0,t]\times A, [0,t]\times A)=\lambda([0,t])\cdot\lambda^{d}(A)=t\lambda^{d}(A). 
 \] 
 Por lo tanto, si $A=\varnothing$, claramente $\W_t(\varnothing)=0$ con probabilidad 1, ya que $\Gamma([0,t]\times\varnothing,[0,t]\times \varnothing)=0$. Ahora, para mostrar la $\sigma$-aditividad y la $\sigma$-finitud se procede de manera análoga a lo hecho en la proposición \ref{Finito_aditiv_ruido_blanco}: primero, se prueba que la aplicación $W_t:\B(\R)\to L^{2}(\P)$ es finito aditiva, y posteriormente se generaliza. Para la parte de la $\sigma$-finitud se procede igual.

 Finalmente, para mostrar que para cualquier $A\in \B(\R^{d})$ el proceso $t\to W_t(A)$ es una martingala con respecto a la filtración $(\F_t)_{t\geq0}$, observamos que el proceso claramente es integrable (lo componen variables Gaussianas) y es adaptado a la filtración por definición. Resta probar que para $A\in \B(\R)$ fijo,
 \[
 \E\left[W_{s+t}(A)\lvert \F_t\right]=W_t(A), \qquad s,t\geq0  
 \]
 Para ello haremos uso de la proposición \ref{Gaussi_indep}. Consideramos a la familia de variables aleatorias $C_1=\left\{W_u(A):u\leq t\right\}$ y
 $C_2=\left\{W_{t+s}-W_t(A):s>0 \right\}$, para $t\geq0$. Notemos que para $0\le u\le t$,
 \begin{align*}
   \E\left[(W_{t+s}(A)-W_t(A))W_u(A)\right]&=\Gamma([0,t+s]\times A, [0,u]\times A)-\Gamma([0,t]\times A,[0,u]\times A)\\
   &=\lambda([0,t+s]\cap[0,u])\cdot\lambda^{d}(A)-\lambda([0,t]\cap[0,u])\cdot\lambda^{d}(A)\\
   &=\lambda([0,u])\cdot\lambda^{d}(A)-\lambda([0,u])\cdot\lambda^{d}(A)\\
   &=0.
 \end{align*}
 Por lo tanto, para cualesquiera dos variables $\xi_1$ y $\xi_2$ en $C_1$ y $C_2$ respectivamente, se tiene que $\langle \xi_1,\xi_2\rangle_{L^2(\P)}= \E\left[\xi_1\xi_2\right]=0$, esto es, son ortogonales. Por lo tanto, si $H_1$ y $H_2$ representan la cerradura del subespacio lineal generado por las familias $C_1$ y $C_2$ respectivamente, dichos subespacios son ortogonales.
 
 Por lo tanto, por la proposición mencionada antes, las $\sigma$-álgebras generadas por las familias $C_1$ y $C_2$ son independientes. En particular, para $s,t\geq0$, $\F_t\subseteq \sigma(C_1)$, por lo que al ser $W_{t+s}-W(t)$ una variable $\sigma(C_2)$ medible, se sigue que 
 \[
     \E\left[W_{t+s}(A)|\F_t\right]=\E\left[W_{t+s}(A)-W_{t}(A)+W_t(A)|\F_t\right]=\E\left[W_{t+s}(A)-W_t(A)\right]+W_t(A)=W_t(A),
     \]
     por lo que la propiedad de martingala se satisface.  
\end{proof}

El proceso de ruido blanco es entonces un ejemplo de medida martingala, no obstante, este concepto se puede definir de manera mucho más general, como vemos a continuación.

\begin{dfn} 
Sea $(\F_t)_{t\geq0}$ una sucesión de $\sigma$-álgebras continuas por derecha. Un proceso $(M_t(A))_{t\geq0, A\in \B(\R^{d})}$ es una medida martingala con respecto a dicha filtración si 
\begin{itemize}
   \item $M_0(A)=0$ $\P$- casi seguramente.
   \item Si $t>0$, entonces $M_t$ es una medida signada, $\sigma$-finita, $L^{2}(\P)$-valuada.
   \item Para cualquier $A\in \B(\R^{d})$, se tiene que $(M_t(A))_{t\geq0}$ es una martingala con respecto a la filtración $(\F_t)_{t\geq0}$ de media 0.
\end{itemize}
\end{dfn}
Por construcción, el proceso del ruido blanco es un primer ejemplo de medida martingala. La razón de introducir este concepto, es que este conjunto de procesos forman una clase adecuada de integradores. A continuación, definimos un concepto relacionado íntimamente con las medidas martingala, y que es en cierto sentido una generalización del proceso de covariación para dos martingalas en el caso unidimensional.

\begin{dfn} 
 Sea $M$ una medida martingala. El funcional de covarianza de $M$ se define como
 \[
 \overline{Q}_t(A,B):= \langle M_\cdot(A),M_\cdot (B)\rangle_t, \qquad t\geq 0, A, B \in \B(\R^{d}). 
 \]
 \end{dfn}
 Dado que para $M$ una medida martingala, si $A$ y $B \in \B(\R^{d})$ se tiene que $(M_t(A))_{t\geq0}$ y $(M_t(B))_{t\geq0}$ son martingalas, entonces el proceso de covariación de ambas martingalas evaluado en $t\geq0$ coincide con el funcional de covarianza evaluado en los borelianos $A$ y $B$, y en $t\geq0$.

 Gracias a lo anterior, se deducen directamente de las propiedades de la covariación entre dos martingalas las siguientes propiedades del funcional de covarianza de una medida martingala.

 \begin{prop} 
  Sea $M$ una medida martingala y denotemos por $\overline{Q}$ a su funcional de covarianza. Entonces $\overline{Q}$ cumple que
  \begin{itemize}
   \item $\overline{Q}_t(A,B)=\overline{Q}_t(B,A)$, \ $\P$- casi seguramente.
   \item Si $B\cap C=\varnothing$, entonces $\overline{Q}_t(A,B\cup C)=\overline{Q}_t(A,B)+\overline{Q}_t(A,C)$, \ $\P$- casi seguramente.
   \item $\abs{\overline{Q}_t(A,B)}^2\leq \overline{Q}_t(A,A)\overline{Q}_t(B,B)$ \ $\P$-casi seguramente, y 
   \item $t\mapsto\overline{Q}_t(A,A)$ es $\P$- casi seguramente no decreciente.
  \end{itemize}
  \end{prop}

Con el concepto de funcional de covarianza, podemos definir una función aleatoria de conjuntos en $\B(\R^{d})\times \B(\R^{d})\times \B([0,\infty))$, que denotaremos $Q$, como sigue:
\[
   Q(A\times B\times(s,t]):=\overline{Q}_t(A,B)-\overline{Q}_s(A,B), \qquad A,B\in \B(\R^{d}), \ t\geq s\geq 0
   \]
Y extendemos dicha función a uniones disjuntas de elementos (rectángulos)
$(A_i\times B_i\times(s_i,t_i])$ para $1\leq i \leq m$ y $A_i,B_i\in \B(\R^{d})$, $(s_i,t_i]\in \B(\R^{d})$ como 
\[
Q \left(\bigcup_{i=1}^{n}(A_i\times B_i\times(s_i,t_i])\right):=\sum_{i=1}^{n}Q(A_i\times B_i\times (s_i,t_i]).
\]
  
Nuestro objetivo es extender la función aleatoria de conjuntos $Q$ a conjuntos más generales. El problema es que en general no es posible hacerlo. No obstante, con estas dos herramientas teóricas, podemos definir el concepto de \textit{worthiness} de una medida martingala, introducido por Walsh en $\cite{Walsh_J.B_Introduction_to_SPDEs}$.
Dicho concepto es de carácter técnico, pero será fundamental en la construcción de la integral de Itô-Walsh al permitirnos extender la definición de $Q$ a conjuntos más generales.


\begin{dfn} 
Sea $M$ una medida martingala. Decimos que $M$ es \textit{worthy} (digna) si existe una medida $\sigma$-finita $K:\B(\R^{d})\times \B(\R^{d})\times \B([0,\infty))\times \Omega \to [0,1]$ tal que para cualesquiera $A,B\in \B(\R^{d})$, $C\in \B([0,\infty))$ y $\omega \in \Omega$, 
\begin{itemize}
   \item $A\times B\mapsto K(A\times B \times C,\omega)$ es no negativa definida y simétrica,
   \item $\{K(A\times B \times (0,t])\}_{t\geq0}$ es un proceso predecible para cualesquiera $A, B \in \B(\R^{d})$,
   \item Para cualesquiera dos conjuntos compactos $A,B \in \B(R^{d})$, y $t>0$, 
   \[
   \E\left[K(A\times B \times (0,t])\right]<\infty,    
   \]
   \item Para cualesquiera $A,B\in \B(\R^{d})$ y $t>0$,
   \[
   \abs{Q(A\times B \times (0,t])}\leq K(A\times B \times (0,t]) \qquad \P-\text{ casi seguramente.}   
   \]
\end{itemize}
Como es usual en probabilidad, la dependencia en $\omega$ es omitida en la notación. Cuando dicha medida $K$ existe, decimos que es una \textit{medida dominante} para $M$.
\end{dfn}


\subsection{Definición de la Integral}
En este punto tenemos suficiente teoría para definir la integral de Itô-Walsh con respecto a medidas martingala. Conforme vayamos desarrollando la construcción, vamos a ir notando cierta familiaridad con la construcción de la integral de Lebesgue y la integral de Itô.

\begin{dfn} 
Una función $f:\R^{d}\times [0,\infty)\times \Omega\to \R$ se dice que es \textit{elemental} si es de la forma 
\[
   f(x,t,\omega)=X(\omega)\1_{(a,b]}(t)\1_A(x) 
\]
donde $X$ es una variable aleatoria acotada y $\F_a$-medible, y además $A\in \B(\R^{d})$. 

Definimos la integral de Itô-Walsh para una función elemental de la forma anterior como 

\[
(f\cdot M)_t(B)(\omega):= X(\omega)\left[M_{t\wedge b}(A\cap B)-M_{t\wedge a}(A\cap B)\right](\omega)  
\]
\end{dfn}

Como una primera propiedad, tenemos la siguiente proposición
\begin{prop} 
Sea $f$ una función elemental y $M$ una medida martingala. Entonces $(f\cdot M)$ es nuevamente una medida martingala. En particular, la integral de funciones elementales con respecto a martingalas se vuelve una forma de construir nuevas medidas martingala.
\end{prop}
\begin{proof} 
 Pendiente 
\end{proof}
\begin{ejem} 
Sea $\W$ un ruido blanco en $\R^{d}\times[0,\infty)$ y consideremos a $(W_t(A))_{t\geq0,A\in \B(\R^d)}$ el proceso de ruido blanco. Ya vimos que dicho proceso forma una medida martingala. Sea $f$ una función elemental. Entonces $(f\cdot W)$ es una medida martingala.
\end{ejem}

El siguiente paso en la construcción de la integral es extender la definición a funciones simples. 

\begin{dfn} 
   Decimos que una función $f$ es una función simple si existen $c_1,...,c_n$ constantes reales, y $f_1,...,f_n$ funciones elementales, tales que 
   \[
     f=\sum_{k=1}^{n}c_kf_k.  
   \] Denotaremos por $\mathscr{S}$ el conjunto de las funciones simples. Definimos la integral de Itô-Walsh para funciones simples como 
   \[
   (f\cdot M)_t(B):=\sum_{j=1}^{k}c_j(f_j\cdot M)_t(B)    
   \]
 \end{dfn}
 \begin{prop} 
  La definición anterior es consistente, esto es, no depende de la representación de $f$ en términos de funciones simples.
  \end{prop}
Resulta ser que una clase adecuada de funciones integradoras son las funciones $f$ que son predecibles. Esto es, aquella $\sigma$-álgebra que es generada por todas las funciones simples. Dicha $\sigma$-álgebra la denotamos por $\mathscr{P}$

En este punto, deseamos extender la definición de la integral para funciones más generales, definidas sobre conjuntos no necesariamente rectangulares. Es en este punto en donde entra en juego la función $Q$.  Dicha función es crucial, según se ve en el siguiente 

\begin{prop} 
Sea $f\in \mathscr{S}$ y supongamos que $M$ es una medida martingala digna. Entonces 
\[
\E\left[((f\cdot M)_t(B))^2\right]=\E\left[\int_{B\times B\times (0,t]}f(x,t)f(y,t)Q(dx,dy,dt)\right].   
\]
\end{prop}

\begin{proof} 
   Primero debemos de hacer la demostración para funciones elementales, y posteriormente para funciones simples.
 \end{proof}
Nótese en la proposición anterior que estamos integrando sobre un rectángulo del estilo $B\times B\times (0,t]$, y la idea es extender la integral a regiones más generales. Para ello, necesitamos extender la definición de $Q$ a dichas regiones, tarea que como anticipamos antes, es posible gracias al concepto de martingala digna.

De hecho, si $M$ es una martingala digna, $Q_M$ puede ser extendida a una medida en todo $\B(\R^{d})\times \B^{\R^{d}}\times [0,\infty)$. 

\begin{ejem} 
Sea $\W$ un ruido blanco en $\R^{d}\times[0,\infty)$ y consideremos al proceso de ruido blanco asociado al mismo. Entonces dicho proceso forma una medida martingala digna con medida dominante $K(A\times B \times C):=\lambda^{d}(A\cap B)\lambda(C)$.
\end{ejem}

Tenemos a continuación una proposición con respecto a las medidas martingala dignas y la integral de Itô-Walsh de funciones simples.

\begin{prop} 
Sea $M$ una medida martingala digna y $f$ una función simple. Entonces $(f\cdot M)$ es una medida martingala digna. 
Mas aún, si $Q_N$ y $K_N$ representan al funcional de covarianza y a la medida dominante de una medida martingala digna $N$, entonces 
\begin{align*}
   &Q_{f\cdot M}(dx \ dy \ dz)=f(x,t)f(y,t)Q_M(dx \ dy \ dt)\\
   &K_{f\cdot M}(dx \ dy \ dt)=\abs{f(x,t)f(y,t)}K_M(dx \ dy \ dt). 
\end{align*}
\end{prop}
\begin{proof} 
  La prueba nuevamente va primero por funciones elementales y luego por funciones simples. 
 \end{proof}

A partir de ahora, solo estaremos interesados en el caso cuando la variable temporal está en algún intervalo finito $(0,T]$. Siguiendo la maquinaria estándar para definir nuevos objetos a partir de aproximaciones, definimos ahora una norma apropiada para dicho propósito.

\begin{dfn} 
Sea $M$ una medida martingala digna. Supongamos que $K_M$ es su medida dominante. Sea $f\in \mathscr{P}$ una función en el conjunto de las funciones que son predecibles (es decir, que son medibles con respecto a la $\sigma$-álgebra $\mathscr{P}$ generada por las funciones simples). Definimos la norma $\norm{\cdot}_M$ como sigue 

\end{dfn}
\[
   \norm{f}_M^2:=\E\left[\int_{\R^{d}\times\R^{d}\times (0,T]} \abs{f(x,t)f(y,t)}K_M(dx \ dy \ dt)\right]
\]

Observamos que dicha norma está bien definida. Denotamos por $\mathscr{P}_M$ al conjunto de todas las funciones $f$ predecibles tales que $\E\left[\norm{f}_M\right]<\infty$.

\begin{prop} 
La norma anteriormente definida en efecto es una norma, y el conjunto $\mathscr{P}_M$ es completo con esta norma.
\end{prop}
\begin{proof} 
  pendiente 
 \end{proof}
A continuación un resultado esencial de aproximación, el cual se remite directamente a \cite{Walsh_J.B_Introduction_to_SPDEs} para su demostración.

\begin{teo} 
El conjunto $\mathscr{S}$ es denso en $\mathscr{P}_M$.
\end{teo}
Lo anterior junto con la proposición 5.18 (referenciar) nos dice que 
\[
\E\left[(f\cdot M)_t^{2}(B)\right]\leq \norm{f}_M^2,\qquad \text{ para } t\in (0,T], f\in \mathscr{S}, B\in \B(\R^d).
\]
Luego, gracias a la cota anterior, si $(f_m)_{m\geq1}$ es una sucesión de Cauchy en $(\mathscr{S},\norm{\cdot}_M)$, entonces la sucesión $\left((f_m\cdot M)_t(B)\right)_{m\geq1}$ es una sucesión de Cauchy en $L^{2}(\P)$. Dado que ambos espacios son completos, se sigue la existencia de objetos en $\mathscr{P}$ y $L^{2}(\P)$, que denotamos por $f$ y $(f\cdot M)_t(B)$ respectivamente, de tal forma que 
\[
f_m\xrightarrow[m\to\infty]{\norm{\cdot}_M}f \quad \ent \quad (f_m\cdot M)_t(B) \xrightarrow[m\to\infty]{L^{2}(\P)} (f\dot M)(B).   
\]

De lo anterior, concluimos con el siguiente teorema.

\begin{teo} 
 Sea $M$ una medida martingala digna. Entonces para cualquier $f\in \mathscr{P}_M$, la integral $(f\cdot M)$ es una medida martingala digna qe satisface la proposición 5.23 (referenciar). Más aún, para cualquier $t\in (0,T]$ y $A,B\in \B(\R^{d})$, 
 \[
 \left\langle (f\cdot M)(A),(f\cdot M)(B)\right\rangle _t= \int_{A\times B\times (0,t]}f(x,s)f(y,s)Q_M(dx \ dy \ ds),
 \]
 y además se tiene la siguiente cota en $L^{2}(\P)$.
 \[
 \E\left[(f\cdot M)^{2}_t(B)\right] \leq \norm{f}_M^{2}.
 \]
 \end{teo}
 De hecho, es posible extender la cota en $L^2(\P)$ a una cota en $L^{p}(\P)$, creando así una desigualdad del tipo Burkholder.
 \begin{teo}[\textbf{Desiguadades de Burkholder}]
  Sea $M$ una medida martingala digna. Entonces para cualquier $p\geq2$ existe una constante $c_p\in (0,\infty)$ tal que para cualquier $f$ predecible y cualquier $t>0$, 
  \[
  \E\left[\abs{(f\cdot M)_t(B)}^p\right]\leq c_p \E\left[\left(\int_{\R^{d}\times \R^{d}\times (0,T]}\abs{f(x,t)f(y,t)}K_M(dx \ dy \ dt)\right)^{p/2}\right] 
  \]
  \end{teo}
  Con esto terminamos la construcción de la integral de Itô-Walsh. Ya nos es posible hablar de integrales con respecto a un ruido blanco $\W$, pues sabemos que este proceso en $\R^{d}\times [0,\infty)$ induce una medida martingala digna, y por lo tanto la noción de integral está bien definida para funciones aleatorias adecuadas. 

\section{Formulación Mild de una SPDE. Existencia y unicidad de las soluciones}
Una vez que tenemos construida la teoría de la integral de Itô-Walsh, podemos pasar darle un significado riguroso a una ecuación diferencial parcial estocástica, así como definir con claridad qué significa resolverla. Como vimos en la introducción del capítulo, la heurística para llegar a un problema en cierto sentido equivalente a aquél que involucra derivadas sigue de cerca las ideas de las soluciones débiles en ecuaciones diferenciales parciales clásicas. Concretamente, el uso de integración por partes para lograr el objetivo es clave. 

Consideremos la ecuación diferencial estocástica formal dada por 
\[
\begin{cases}
   \partial_tu(x,t)=\partial_{xx}u(x,t)+f(u(x,t))\W, & \text{ si } t>0, x\in [0,L]\\
   \partial_xu(0,t)=\partial_xu(L,t)=0, & \text{ si } t>0,\\
   u(x,0)=u_0(x), & \text{ si } x\in [0,L].\\
\end{cases}
\]
donde $\W$ es un ruido blanco con respecto a alguna friltración $(\F_t)_{t\geq0}$, y $u_0:[0,L]\to\R$ es una función determinista, medible y acotada, mientras que $f:\R\to\R$ es una función globalmente Lipschitz y acotada.

Si formalmente multiplicamos el problema por una función $\phi \in C^{\infty}(\R)$ tal que $\phi(0)=\phi(L)=0$ e integramos con respecto al tiempo y posteriormente con respecto al espacio, obtendremos que el problema anterior se puede reformular como 

\begin{align*}
   \int_{0}^{L}u(x,t)\phi(x)dx-\int_{0}^{L}u_0(x)\phi(x)dx&=\int_0^{t}\int_0^{L}\partial_{xx}u(x,s)\phi(x)dxds\\
   &+\int_{0}^{t}\int_{0}^{L}f(u(x,s))\phi(x)W(dx \ ds).    
   \end{align*}
Es destacable que ahora tenemos una ecuación integral estocástica en donde la integral con respecto al ruido blanco cobra total sentido. No obstante, lo anterior aún no puede ser tratado de manera rigurosa, ya que $u$ al ser aleatorio no necesariamente será una función derivable. Por lo tanto, si de manera formal utilizamos integración por partes, al ser $\phi$ una función que se anula en la frontera, la integral de la parcial de $u$ multiplicado por $\phi$ se transforma en
\[
   \int_{0}^{t}\int_{0}^{L}\partial_{xx}u(x,s)\phi(x)dx ds=\int_{0}^{t}\int_{0}^{L}u(x,s)\phi''(x)dx ds,
\]
por lo que sustituyendo la expresión anterior en la reormulación de nuestro problema, obtenemos una ecuación integral estocástica. Tal ecuación integral tiene completo sentido, y dado que es una reformulación de nuestro problema inicial pero que puede tratarse de manera rigurosa, podemos considerar que una manera de resolver nuestro problema inicial, es resolver la reformulación integral anterior. Esta es conocida como la solución mild de nuestro problema anterior.

\begin{dfn} 
Decimos que $u$ es una solución \textit{mild} del problema inicial anterior, si existe un proceso estocástico $u(x,t)$ tal que para cualquier función $\phi\in C^{\infty}([0,L])$, donde $\phi'(0)=\phi'(L)=0$, se tiene la igualdad 

\begin{align*}
   \int_{0}^{L}u(x,t)\phi(x)dx-\int_{0}^{L}u_0(x)\phi(x)dx=\int_{0}^{t}\int_{0}^{L}u(x,s)\phi''(x)dx ds+\int_{0}^{t}\int_{0}^{L}f(u(x,s))\phi(x)W(dx \ ds).  
\end{align*}
\end{dfn}

Es conocida teoría que resuelve la ecuación diferencial estocástica del calor. En particular en el caso en que la condición inicial y $\sigma$ cumplen hipótesis sencillas. Este será nuestro caso de estudio. 
En particular, se tiene el siguiente resultado.

\begin{teo} 
La ecuación estocástica del calor, con función $f$ Lipschitz y acotada, tiene una única solución $\P$- casi seguramente, que satisface lo siguiente para cualquier $T>0$.
\[
\sup_{0\leq x\leq L}\sup_{0\leq t \leq T}\E\left[\abs{u(x,t)}^2\right]<\infty.   
\]
\end{teo}
Este es el resultado principal del capítulo. Y para su prueba, se requiere el siguiente lema bastante conocido en el ámbito de las ecuaciones diferenciales.

\begin{teo}[\textbf{Lema de Gronwall}] 
Supongamos $\phi_1,\phi_2,...:[0,T]\to[0,+\infty)$ son medibles y no decrecientes. Supongamos también que existe una constante $A\in \R$ tal que para cualquier entero $n\geq1$ y para cualquier $t\in[0,T]$,
\[
\phi_{n+1}(t)\leq A\int_{0}^{t}\phi_n(s)ds.   
\]
Entonces 
\[
\phi_n(t)\leq \phi_1(T)\frac{(At)^{n-1}}{(n-1)!}, \qquad \text{ para cualquier }n\geq1, \text{ y } t\in[0,T].   
\]
\end{teo}
Finalmente, el siguiente teorema nos muestra que no solamente hay existencia y unicidad de las solución a la ecuación estocástica del calor, sino que también existe una modificación continua de la misma.

\begin{teo} 
Sea $\Tilde{u}(x,t)$ la solución única $\P$- casi seguramente de la ecuación estocástica del calor. Entonces existe una modificación $u(x,t)$ de dicha solución cuyas trayectorias son continuas.
\end{teo}

Retomaremos la teoría desarrollada hasta aquí sobre ecuaciones diferenciales parciales estocásticas cuando lleguemos al capítulo 4, en donde estudiamos convergencia de las densidades de los promedios espaciales de la solución a la ecuación estocástica del calor. 


% \begin{itemize}
%     \item Planteamiento de la ecuación integral como una versión débil del problema planteado. Definición de la solución Mild utilizando funciones de Green.
%     \item Existencia de una única solución bajo condiciones Lipschitz. Argumento usando Lema de Gronwall para unicidad. Argumento usando iteraciones de Piccard para la existencia.
% \end{itemize}

\chapter{Elementos de Cálculo de Malliavin}% y Método de Stein}
En este capítulo se revisa material esencial de cálculo de Malliavin. Esta disciplina nació a finales de la década de 1970 a partir de los trabajos de Paul Malliavin. La intención de Malliavin era hallar una prueba probabilista del teorema de Hörmander sobre el efecto regularizante de las soluciones de cierta ecuación diferencial estocástica, puesto que Lars Hörmander en 1967 encontró una prueba que utilizaba herramientas puramente analíticas. Con dicha motivación inicial, en las últimas décadas el cálculo de Malliavin se ha convertido en una poderosa herramienta matemática para estudiar problemas diversos en probabilidad. 

En palabras de Nourdin y Peccati (ver prefacio de \cite{Nourdin_Peccati_2012}), el cálculo de Malliavin se puede entender como un cálculo diferencial en infinitas dimensiones, cuyos operadores actúan sobre funcionales de procesos Gaussianos generales, y que en conjunto con el método de Stein, se obtiene una amalgama poderosa para deducir teoremas tipo límite central, así como tasas de convergencia explícitas para los mismos. 

Tres operadores son esenciales en esta área: la derivada de Malliavin $D$, el operador de divergencia $\delta$ (o integral de Skorokhod) y el generador infinitesimal. El propósito de este capítulo es estudiar el material nuclear correspondiente a estos operadores.
\section{Cálculo de Malliavin en el caso unidimensional}
Siguiendo de cerca la exposición hecha por \cite{Nourdin_Peccati_2012}, comenzamos el estudio del tópico por el caso unidimensional. Aquí es más sencillo observar propiedades de estos objetos, los cuales se generalizan a dimensiones infinitas en la siguiente sección.

\subsection{La derivada de Malliavin.}

El operador con el que comenzamos nuestro estudio es la derivada de Malliavin. La idea es definir la derivada de variables aleatorias del tipo $f(N)$, donde $f$ es una función determinista, y $N$ es una variable aleatoria con distribución Normal estándar.

En el contexto unidimensional, la definición de este operador se realiza utilizando ideas similares a aquellas que nos llevan a la definición de derivada débil en los espacios de Sobolev. Dichas ideas consisten, a grandes rasgos, en generalizar la noción de derivada a objetos más generales que las funciones diferenciables en el sentido usual. Para ello primero buscamos una clase de funciones diferenciables con propiedades nobles pero que a su vez sea suficientemente rica, y posteriormente extendemos el operador en algún sentido.

Dado que buscamos definir la derivada de funciones de variables normales estándar, es natural definir el operador derivada en un subconjunto de los espacios $L^q(\R,\B(\R))$ con $q\in [1,\infty)$, pero dotados de la medida gaussiana estándar. 

Dicho lo anterior, denotemos por $\gamma:\B(\R)\to [0,1]$ a la medida mencionada, a saber, 
\[
\gamma(A)=\int_A\frac{1}{2\pi}e^{-\frac{x^2}{2}}dx.    
\]
Consideremos el espacio $L^{q}(\R,\B(\R),\gamma(dx))$, con $q\in [1,\infty)$, que durante esta subsección denotaremos simplemente como $L^{q}(\gamma)$. En este espacio, se valen algunas propiedades resultan de gran utilidad. Tal es el caso del siguiente lema.

\begin{lema}\label{lema1}
Sea $f:\R\to\R$ una función absolutamente continua con respecto a la medida de Lebesgue y tal que $f'\in L^{1}(\gamma)$. Entonces la función $h:\R\to\R$ dada por $h(x)=xf(x)$ está en $L^1(\gamma)$ y se cumple que 
\[
\int_\R xf(x)d\gamma(x)=\int_\R f'(x)d\gamma(x).
\]
\end{lema}
\begin{proof} 
  Supongamos primero que $f(0)=0$. Probamos que $h\in L^{1}(\gamma)$. En efecto,  
  \begin{align*}
   \int_{-\infty}^{\infty}|x||f(x)|d\gamma(x)&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty\abs{\int_0^{x}f'(y)dy}|x|e^{-\frac{x^2}{2}}dx\\
   &\leq \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \left(\int_0^{x}\abs{f'(y)}dy\right) \ |x|e^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \left(\int_x^{0}\abs{f'(y)}dy\right) \ (-x)e^{-\frac{x^2}{2}}dx\\
   & \ \ \  +\frac{1}{\sqrt{2\pi}}\int_{0}^\infty \left(\int_0^{x}\abs{f'(y)}dy\right) \ xe^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \int_{-\infty}^{y}\abs{f'(y)}(-x)e^{-\frac{x^2}{2}}dx \ dy+\frac{1}{\sqrt{2\pi}} \int_{0}^{\infty}\int_{y}^\infty \abs{f'(y)}xe^{-\frac{x^2}{2}}dx \ dy\\
   &=\int_{-\infty}^0 \abs{f'(y)}\frac{1}{\sqrt{2\pi}}\left(e^{-\frac{x^2}{2}}\Big|_{-\infty}^{y}\right) dy + \int_{0}^{\infty}\abs{f'(y)}\frac{1}{\sqrt{2\pi}}\left(-e^{-\frac{x^2}{2}}\Big|_{y}^{\infty}\right)dy\\
   &=\int_{-\infty}^{\infty}|f'(y)|d\gamma(y)<\infty,
  \end{align*}
  en donde hemos hecho uso del teorema de Fubini en la tercera igualdad. Por lo tanto, la función $h(x)=xf(x)$ en efecto está en $L^1(\gamma)$. Realizando cuentas similares, obtenemos que 
  \begin{align*}
   \int_{-\infty}^{\infty}xf(x)d\gamma(x)&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \left(\int_0^{x}f'(y)dy\right) xe^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \left(\int_x^{0}f'(y)dy\right) \ (-x)e^{-\frac{x^2}{2}}dx\\
   & \ \ \  +\frac{1}{\sqrt{2\pi}}\int_{0}^\infty \left(\int_0^{x}f'(y) dy\right) \ xe^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \int_{-\infty}^{y}f'(y)(-x)e^{-\frac{x^2}{2}}dx\ dy+\frac{1}{\sqrt{2\pi}} \int_{0}^{\infty}\int_{y}^\infty f'(y)xe^{-\frac{x^2}{2}}dx\ dy\\
   &=\int_{-\infty}^0 f'(y)\frac{1}{\sqrt{2\pi}}\left(e^{-\frac{x^2}{2}}\Big|_{-\infty}^{y}\right) dy + \int_{0}^{\infty}f'(y)\frac{1}{\sqrt{2\pi}}\left(-e^{-\frac{x^2}{2}}\Big|_{y}^{\infty}\right)dy\\
   &=\int_{-\infty}^{\infty}f(y)d\gamma(y),
  \end{align*}
  como queríamos. Para el caso en el que $f(0)\neq 0$, notamos que 
\end{proof}
En la búsqueda de la generalización de la derivada a funciones más generales, definimos a la clase de Schwarz $\mathcal{S}$ como el conjunto de funciones en $C^{\infty}(\R,\R)$ tales que ella y sus derivadas tienen crecimiento a lo más polinomial, es decir,
\[
\mathcal{S}:=\left\{f\in C^{\infty}(\R) : \forall p\in \{0,1,2,...\} \ \exists n\geq1 \text{ tal que }\lim_{x\to\pm\infty}\frac{|f^{(p)}(x)|}{1+|x|^{n}}<\infty \right\}.    
\]
Observamos que este conjunto dotado de la restricción correspondiente de la norma $L^q(\gamma)$ forma subespacio lineal de $L^{q}(\gamma)$, puesto que las combinaciones lineales de elementos infinitamente derivables vuelven a ser infinitamente derivables, y las combinaciones lineales también son compatibles con los límites. Es de destacar que este espacio es suficientemente amplio como para poseer a los polinomios. Más aún, este último conjunto es denso en $L^{q}(\gamma)$ para cualquier $q\in [1,\infty)$. Esto es consecuencia del siguiente lema.

\begin{lema} 
Para cualquier $q\in [1,\infty)$, el conjunto de los monomios de la forma $\{x^n: n\geq0\}$ genera un subespacio denso del espacio $L^{q}(\gamma)$. En particular, la clase de Schwarz $\S$ forma un subconjunto denso de $L^q(\gamma)$.
\end{lema}
\begin{proof} 
   Un corolario del teorema de Hann-Banach caracteriza la propiedad de ser un subespacio lineal denso de un espacio $L^{q}$. En nuestro caso basta con probar que para cualquier elemento $g \in L^{p}(\gamma)$, 
   \[
   \int_\R g(x)x^{n} \ d\gamma(x)=0, \ \ \forall n\geq0  \quad \ent \quad g=0 \ \ \gamma- \text{casi en todas partes},
   \]
   donde $p\in (1,\infty]$ es el exponente conjugado de $q$. Sea pues $g\in L^{p}(\gamma)$ y supongamos que para cualquier $n\geq0$, el antecedente de la implicación anterior es válido. Usaremos un argumento que involucra la transformada de Fourier. Notamos primero que la transformada de $g(x)e^{-\frac{x^2}{2}}$ existe. En efecto, por la desigualdad de Hölder, para $t\in \R$ arbitrario,
      \begin{align*}
         \int_{\R}|g(x)e^{-\frac{x^2}{2}}e^{itx}|dx&\leq \int_{\R}|g(x)|e^{-\frac{x^2}{2}}dx\\
         &=\int_{\R}|g(x)|d\gamma(x)\\
         &\leq\norm{g}_{L^{p}(\gamma)}\left(\int_{\R}d\gamma(x)\right)^{\frac{1}{q}}\\
         &=\norm{g}_{L^{p}(\gamma)}<\infty.
      \end{align*}
   
   Por otro lado, para cualquier $x\in \R$,  
   \[
   \abs{g(x)\sum_{k=0}^{n}\frac{(itx)^k}{k!}}\leq |g(x)|\sum_{k=0}^{n}\frac{|tx|^{k}}{k!}\leq |g(x)|\sum_{k=0}^{\infty}\frac{|tx|^{k}}{k!}=|g(x)|e^{|tx|},
   \]
   y la cota anterior vale para cualquier $n\geq1$. Nuevamente usando la desigualdad de Hölder,
   \[
   \int_\R\abs{g(x)\sum_{k=0}^{n}\frac{(itx)^{k}}{k!}}d\gamma(x)\leq \int_\R |g(x)|e^{|tx|}d\gamma(x)\leq \norm{g}_{L^{p}(\gamma)}\left(\int_\R (e^{|tx|})^qd\gamma(x)\right)^{\frac{1}{q}}. 
   \]
   Observamos que se tiene la igualdad 
   \[
   \int_\R (e^{|tx|})^{q}d\gamma(x)=\frac{1}{\sqrt{2\pi}}\int_\R e^{q|tx|}e^{-\frac{x^2}{2}}dx,
   \]
   y notamos que esta última integral es finita para cualquier $q\in [1,\infty)$, ya que el término correspondiente a la medida gaussiana domina al otro término exponencial. Por lo tanto, $|g(x)|e^{|tx|}$ es una función en $L^{1}(\gamma)$ que domina a las sumas parciales anteriores. Así, usando el teorema de convergencia dominada, 
   \[
   \int_\R g(x)e^{-\frac{x^2}{2}}e^{itx}dx=\int_\R \lim_{n\to \infty} g(x)\sum_{k=0}^{n}\frac{(itx)^k}{k!}d\gamma(x)=\lim_{n\to \infty}\sum_{k=0}^{n}\frac{(it)^k}{k!}\int_\R g(x)x^k d\gamma(x)=0, 
   \]
   donde hemos hecho uso de la hipótesis del enunciado en la última igualdad. Hemos hallado que la transformada de Fourier de $g(x)e^{-\frac{x^2}{2}}$ es idénticamente la función 0. Al ser la transformada de Fourier inyectiva en $L^{1}(\R,\B(\R),dx)$, se sigue que $g(x)e^{-\frac{x^2}{2}}$ es la función idénticamente 0 salvo conjuntos de medida de Lebesgue 0. En particular, $g$ es la función idénticamente 0, $\gamma \ -$ casi en todas partes.  

   Concluimos la prueba notando que los monomios son un subconjunto de la clase de Schwarz $\S$.
 \end{proof}
Sabiendo que $\mathcal{S}$ es un subespacio lineal denso de $L^{q}(\gamma)$, podemos generalizar la derivada de las funciones en el espacio de Schwarz a funciones más generales que pertenecen a $L^{q}(\gamma)$. Para ello denotemos por ahora como $\frac{d^p}{dx^p}f$ a la derivada de orden $p$ de una función $f\in \S$, para $p\in \{0,1,2,...\}$. Dicha derivada siempre está definida para tales funciones. Si pensamos en la derivación de orden $p$ como aplicar un operador a un elemento del espacio de Schwarz, resulta que dicho operador es lineal. Dado que buscamos extender la derivada a funciones más generales, es natural pensar en la posibilidad de extender el operador $\frac{d^p}{dx^{p}}:\S\to L^{q}(\gamma)$ a un operador que sea cerrado en un dominio más grande que $\S$. Este es el contenido del siguiente teorema.
\begin{teo} 
Sea $\frac{d^{p}}{dx^p}:\mathcal{S}\longrightarrow L^{q}(\gamma)$ el operador derivada de orden $p$. Dicho operador es cerrable para todo $q\in [1,\infty)$ y todo entero $p\geq1$
\end{teo}
\begin{proof} 
  Usamos la siguiente caracterización de ser un operador cerrable. Supongamos que $(f_n)_{n\geq1}$ es una sucesión en $\S$ y que $g\in L^{q}(\gamma)$ son tales que 
  \[
  f_n\xrightarrow[n\to \infty]{}0 \qquad \text{ y } \qquad \frac{d^{p}}{dx^{p}}f_n\xrightarrow[n\to \infty]{}g.
  \] 
   Buscamos probar que $g=0$, $\gamma \ - $ casi en todas partes.
   
   Supongamos primero el caso $q>1$. Sea $h\in \S$. Definimos $\delta h(x):=xh(x)-h'(x)$ y para $n\geq1$, $\delta^{n+1}h(x):=\delta^{n} h(x)$, donde $\delta^{1}:=\delta$. Observemos primero que para cualquier $n\in \{1,2,...,p\}$, $\delta^{n}h\in \S$. Esto se sigue de que $xh(x)-h'(x)$ sigue siendo una función infinitamente diferenciable con crecimiento a lo más polinomial, y un argumento inductivo.
   
   Ahora bien, usando la igualdad
   \[
   \int_\R \left(\frac{d^{p-1}}{dx^{p-1}}f_n(x)h(x)\right)'d\gamma(x)=\int_\R \frac{d^{p}}{dx^{p}}f_n(x)h(x)d\gamma(x) +\int _\R \frac{d^{p-1}}{dx^{p-1}}f_n(x)h'(x)d\gamma(x),
   \] 
   la cual se obtiene con la derivada del producto, se tiene que 
   \begin{align*}
       \int_{\R}h(x)g(x)d\gamma(x)&=\lim_{n\to\infty}\int_\R \frac{d^{p}}{dx^{p}}f_n(x) h(x) d\gamma(x)\\
       &=\lim_{n\to\infty}\left(\int_\R \left(\frac{d^{p-1}}{dx^{p-1}}f_n(x)h(x)\right)'d\gamma(x)-\int _\R \frac{d^{p-1}}{dx^{p-1}}f_n(x)h'(x)d\gamma(x)\right)\\
       &=\lim_{n\to\infty}\left(\int_\R x\frac{d^{p-1}}{dx^{p-1}}f_n(x)h(x)d\gamma(x)-\int _\R \frac{d^{p-1}}{dx^{p-1}}f_n(x)h'(x)d\gamma(x)\right)\\
       &=\lim_{n\to\infty}\int_\R \frac{d^{p-1}}{dx^{p-1}}f_n(x) \left(xh(x)-h'(x)\right) d\gamma(x)\\
       &=\lim_{n\to\infty}\int_\R \frac{d^{p-1}}{dx^{p-1}}f_n(x) \delta h(x) d\gamma(x),
   \end{align*}
   donde en la primera igualdad hemos hecho uso del teorema de convergencia dominada, y en la tercera igualdad hemos hecho uso del lema \ref{lema1}. Utilizando un argumento inductivo y la continuidad del valor absoluto, obtenemos que 
   \[
       \abs{\int_{\R}h(x)g(x)d\gamma(x)}\leq\int_{\R}\abs{h(x)g(x)} d\gamma(x)=\lim_{n\to\infty}\int_\R \abs{f_n(x) \delta^{p} h(x)} d\gamma(x).
   \]
   Por hipótesis, dado que la sucesión $f_n$ converge a 0 en $L^{q}(\gamma)$ y $\delta^{p}h \in \S\subseteq L^{\frac{q}{q-1}}$ según el Lema 2, usando la desigualdad de Hölder, 
    \[
       \abs{\int_{\R}h(x)g(x)d\gamma(x)}\leq\lim_{n\to\infty}\int_\R |f_n(x) \delta^{p} h(x)| d\gamma(x)\leq \lim_{n\to \infty}\norm{f_n}_{L^{q}(\gamma)}\norm{\delta^{p}h}_{L^{\frac{q}{q-1}}(\gamma)}=0,
    \]
 \end{proof}
 de donde deducimos que para cualquier $h\in \S$, 
 \[
 \int_{\R}h(x)g(x)d\gamma(x)=0.
 \]
 Dado que $\S$ es denso en $L^{q}(\gamma)$, se sigue que $g=0$, $\gamma \ - $ casi en todas partes.

Hemos demostrado que el operador derivada de orden $p$ es cerrable en $\S$. Por lo tanto, por definición existe un operador cerrado, que vamos a denotar por $D^{p}$, con dominio $\text{Dom}(D^{p})$ un subespacio lineal de $L^{q}(\gamma)$ tal que $\S\subseteq \text{Dom}(D^{p})$ y $D^{p}f=\frac{d^{p}}{dx^{p}}f$ para cualquier $f\in \S$. Estos elementos nos permiten definir la derivada de Malliavin.

\begin{dfn}
   Sea $p\geq1$ un entero positivo. Definimos la derivada de Malliavin como el operador $D^{p}$ que extiende a la derivada $\frac{d^{p}}{dx^{p}}$ vista como operador de $\S$ a $L^{q}(\gamma)$, para $q\in [1,\infty)$.
\end{dfn}

Denotaremos al dominio de $D^{p}$ como $\D^{p,q}:=\text{Dom}(D^{p})$. A dichos espacios también se les conoce como espacios de Watanabe-Sobolev. Es conocido que al ser $D^{p}$ un operador cerrado, podemos definir la \textit{norma de la gráfica} de dicho operador en su dominio como la función $\|.\|_{G}:\D^{p,q}\longrightarrow[0,\infty)$ dada por,
\[
       \|f\|_{G}:=\norm{f}_{L^q(\gamma)}+\norm{D^{p}f}_{L^{q}(\gamma)}.
   \]
Dicha norma vuelve a la derivada de Malliavin $D^{p}$ un operador continuo. Más aún, es un resultado conocido de análisis funcional que, al ser $D^{p}$ un operador cerrado, su dominio $\D^{p,q}$ es un espacio de Banach con la norma $\norm{\cdot}_{G}$ definida antes.

Ahora bien, es natural pensar en lo que sucede con una aplicación sucesiva de las derivadas de Malliavin. Se tiene que para cualquier $p\geq1$ entero, y $f\in \D^{p+1,q}$, 
\[
   D^{p+1}f=D(D^{p}f)=D^{p}(Df),
\]
lo que nos dice que la derivada de Malliavin en efecto respeta la composición, tal y como ocurre en la derivación usual. Se sigue de esto que, al ser consistente la derivación sucesiva en en el sentido de Malliavin, para una función $f\in \D^{p,q}$, las derivadas $D^{k}f$ existen para cualquier $k\in \{1,...,p\}$, y por lo tanto, las normas $\norm{D^kf}_{L^2(\gamma)}$ son finitas para cualquier $k\in \{1,...,p\}$. Luego, se puede demostrar que la función $\|.\|_{\mathbb{D}^{p,q}}:\D^{p,q}\longrightarrow[0,\infty)$ dada por
\[
   \|f\|_{\D^{p,q}}:=\left(\norm{f}_{L^{q}(\gamma)}^q+\sum_{k=1}^p\norm{D^{k}f}_{L^{q}(\gamma)}^{q}\right)^{\frac{1}{q}}
\] 
es una norma equivalente a la norma de la gráfica definida en $\D^{p,q}$ antes. Por lo tanto, esta norma también hace a $\D^{p,q}$ un espacio de Banach.

Concluimos observando que $\D^{p,q}$ también puede ser obtenido como la cerradura del subespacio $\S$ con respecto a la norma $\norm{\cdot}_{\D^{p,q}}$ definida antes, pero restringida al espacio $\S$, en donde la derivada de Malliavin coincide con la derivada usual.

Lo anterior permite hacer una caracterización del espacio $\D^{p,q}$ y una definición alternativa de la derivada de Malliavin: una función $f$ está en dicho espacio si y solo si existe una sucesión de funciones $(f_n)_{n\geq1}\subseteq \S$ tales que $f_n\xrightarrow[n\to\infty]{}f$ y $(\frac{d^{p}}{dx^{p}}f_n)_{n\geq1}$ es una sucesión de Cauchy en $L^q(\gamma)$, para cualquier $k\in \{1,...,p\}$. Para tal sucesión de funciones, definimos 
\[
D^{k}f:=\lim_{n\to \infty}\frac{d^k}{dx^{k}}f_n,
\]
en donde el límite es en sentido $L^{q}(\gamma)$, y dicha función límite coincide con la derivada de Malliavin definida antes.

Este operador es de suma importancia. En la siguiente sección veremos que en el caso en que $q=2$, la derivada de Malliavin vista como operador posee propiedades interesantes.

\subsection{El operador de divergencia}

A lo largo de esta sección consideraremos el espacio $L^2(\gamma)$, el cual es un espacio de Hilbert con producto interno dado por 
\[
\langle f,g\rangle=\int_\R f(x)g(x)\ d\gamma(x).
\]
A partir de ahora, y para el resto del texto, denotaremos simplemente por $D^{p}$ a la derivada de Malliavin de orden $p$, aún cuando nos estemos refiriendo a la derivada de orden $p$ convencional de una función $f$ en el espacio de Schwarz.

Hasta ahora tenemos que la derivada de Malliavin es un operador $D^{p}:\mathbb{D}^{p,2}\subseteq L^2(\gamma)\longrightarrow L^2(\gamma)$ y dicho operador, es cerrado por construcción, extiende al operador derivada convencional $D^{p}$ definido en $\mathcal{S}\subseteq L^2(\gamma)$. 

Siendo $L^{2}(\gamma)$ un espacio de Hilbert, si denotamos por $\text{Dom}(\delta^{p})$ como el conjunto de funciones $g\in L^{2}(\gamma)$ tales que existe una constante $c_g>0$ y se cumple que

\[
\abs{\langle D^{p}f,g\rangle_{L^2(\gamma)}}\leq c_g\|f\|_{L^2(\gamma)}, \qquad \forall f\in \mathbb{D}^{p,2},                 xd
\]
entonces para cada $g\in \text{Dom}(\delta^{p})$ el funcional lineal, con dominio $\D^{2,q}$ y con regla de correspondencia $f\longmapsto \langle D^{p}f,g\rangle$, es acotado precisamente por la condición anterior. De esta forma, por el teorema de Hanh-Banach podemos extender dicho funcional a un funcional lineal acotado cuyo dominio es todo $L^2(\gamma)$.

Con el funcional anterior podemos usar el teorema de representación de Riesz para obtener la existencia de un único elemento en $L^2(\gamma)$, que denotaremos por $\delta^{p}g$ tal que 
\[
\langle D^pf,g\rangle_{L^2(\gamma)}=\langle f,\delta^p g\rangle_{L^2(\gamma)}, \qquad \forall f\in L^2(\gamma).
\]
En términos de integrales, se tiene que para cualquier $g\in \text{Dom}(\delta^p)$, existe un único elemento $\delta^p g\in L^2(\gamma)$ tal que 
\[
 \int_\R D^{p}f(x) g(x) \ d\gamma(x)=\int_\R f(x)\delta^{p}g(x) \ d\gamma(x), \qquad \forall f\in L^2(\gamma).  
\]
Con los elementos anteriores, podemos definir el operador de divergencia.

\begin{dfn}
   Sea $p\geq1$ un entero positivo. Denotamos $$\text{Dom}(\delta^p):=\left\{g\in L^2(\gamma) \ : \ \abs{\langle D^{p}f,g\rangle_{L^2(\gamma)}}\leq c\|f\|_{L^2(\gamma)}, \qquad \forall f\in \D^{2,p}\right\},$$ 

   y definimos el operador de divergencia $p$-ésimo como la aplicación $\delta^p:\text{Dom}(\delta^{p})\longrightarrow L^2(\gamma)$ tal que asigna a cada $g\in \text{Dom}(\delta^{p})$ al único elemento $\delta^{p} g\in L^2(\gamma)$ que, gracias al teorema de representación de Riesz, cumple la igualdad
       \[
       \int_\R D^{p}f(x)g(x) \ d\gamma(x)=\int_\R f(x)\delta^{p}g(x) \ d\gamma(x), \qquad \forall f\in L^2(\gamma).
      \]
\end{dfn}
En el caso en que $p=1$, directamente escribimos $\delta^1=\delta$. Observamos que el operador $\delta^p$ es directamente el operador adjunto de $D^{p}$, y como tal, es un operador cerrado.

Es de interés encontrar una forma de calcular el operador $\delta$ aplicado a alguna función en $\text{Dom}(\delta)$. En este sentido, existe una fórmula para calcular $\delta g$ para $g\in \D^{1,2}$. Este es el contenido del siguiente lema.

\begin{lema}\label{formuladelta}
   Se cumple que $\S\subseteq \text{Dom}(\delta)$ y para cualquier $g\in \D^{1,2}$, 
   \[
      \delta g(x)=xg(x)-Dg(x)=xg(x)-g'(x).
   \]
 En particular, para cualquier $g\in \S$, se cumple la fórmula anterior.
 \end{lema}

 \begin{proof} 
    Nótese que para $f, g\in \S$, por el lema \ref{lema1},
   \[
   \int_\R (f(x)g(x))'d\gamma(x)=\int_\R xf(x)g(x)d\gamma(x), 
   \]
   por lo que 
   \[
   \int_\R xf(x)g(x)d\gamma(x)=\int_\R D \left(f(x)g(x)\right) d\gamma(x)=\int_\R Df(x)g(x)d\gamma(x) +\int_\R f(x)Dg(x)d\gamma(x),
   \]
   así que restando, 
   \[
   \int_\R Df(x)g(x)d\gamma(x)=\int_\R xf(x)g(x)d\gamma(x)-\int_\R f(x)Dg(x)d\gamma(x),
   \]
   y por definición de $\delta g$, se tiene que 
   \[
   \int_\R f(x)\delta g(x)d\gamma(x)=\int_\R xf(x)g(x)d\gamma(x)-\int_\R f(x)Dg(x)d\gamma(x).   
   \]
   Pero lo anterior ocurre para cualquier $f\in \mathcal{S}$. Esto nos dice que $\mathcal{S}\subseteq \text{Dom}(\delta)$ y que para cualquier $g\in \mathcal{S}$, la fórmula anterior es válida. 

Utilizando un argumento de aproximación, tenemos que para cualquier $g\in \mathbb{D}^{1,2}$, $\delta g(x)=xg(x)-Dg(x)$.

  \end{proof}

\subsection{El semigrupo de Ornstein-Uhlenbeck}
El semigrupo de Ornstein-Uhlenbeck es una generalización del operador Laplaciano a dimensión infinita. Comenzamos directamente definiendo el semigrupo de Ornstein-Uhlenbeck. 
\begin{dfn}
   El semigrupo de Ornstein-Uhlenbeck, denotado por $(P_t)_{t\geq0}$, se define como una familia de operadores del espacio $\S$ dotado de la norma $\norm{\cdot}_{L^q(\gamma)}$ a $L^q(\gamma)$ como sigue: para $f\in \mathcal{S}$ y $t\geq0$,
   \[
   P_tf(x)=\int_\R f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y), \qquad  \forall x\in \R.
   \]
\end{dfn}
De manera inmediata se observa que esta es una familia de operadores lineales. Tal como se puede esperar, esta familia de operadores cumple la propiedad de semigrupo, y está directamente relacionada con el proceso de Ornstein-Uhlenbeck. Ambos hechos se exploran más adelante. Comenzamos estudiando propiedades básicas de $(P_t)_{t\geq0}$.

\begin{teo}
   Sea $f\in \mathcal{S}$ y $q\in [1,\infty)$. Entonces se cumplen las siguientes proposiciones:
   \begin{itemize}
       \item $P_0f(x)=f(x)$.
       \item $P_\infty f(x):=\displaystyle\lim_{t\to\infty}P_tf(x)=\int_\R f(y)d\gamma(y)$.
       \item $\displaystyle\int_\R \abs{P_tf(x)}^qd\gamma(x)\leq \int_\R\abs{f(x)}^qd\gamma(x)$.
   \end{itemize}
\end{teo}
\begin{proof} 
  \begin{itemize}
   \item  Calculamos el semigrupo en $t=0$. Dado que $\gamma(\R)=1$, se tiene que 
    \[
    P_0f(x)=\int_\R f\left(e^{0}x+\sqrt{1-e^{0}}y\right)d\gamma(y)=\int_\R f(x)d\gamma(y)=f(x).
    \]
    \item Como $f\in \S$, tiene crecimiento a lo más polinomial. Por lo tanto existe un real $M>0$ y un natural $n\geq1$ tal que para cualquier $y\in \R\setminus[-M,M]$, $|f(y)|\leq C(1+|y|^n)$, para alguna constante $C>0$. Observamos que esta última cota es una función en $L^q(\gamma)$. Por otro lado, es claro que en $[-M,M]$, la función $|f|$ alcanza su máximo, por lo que $\norm{f\1_{[-M,M]}}_\infty<\infty$. 
    
    Finalmente, notamos que la función $g(y)=e^{-t}x+\sqrt{1-e^{-2t}}y$ es una función continua y por lo tanto medible, así que
    
    \begin{align*}
       \abs{f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)}&\leq C(1+|y|^n)\1_{g^{-1}[-M,M]^{c}}(y)+\norm{f\1_{[-M,M]}}_\infty\1_{g^{-1}[-M,M]}(y)\\
       &\leq C(1+|y|^n)+\norm{f\1_{[-M,M]}}_\infty,
    \end{align*}
    función que está en $L^1(\gamma)$, y que es independiente de $t$. Por lo tanto, por el teorema de convergencia dominada, 
    \[
    P_\infty f(x)=\lim_{t\to\infty}P_tf(x)=\int_\R\lim_{t\to\infty}f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)=\int_\R f(y)d\gamma(y).
    \]
    \item Para $q\in [1,\infty)$, por definición,
    \begin{align*}
       \int_\R |P_tf(x)|^q d\gamma(x)&=\int_\R \abs{\int_\R f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)}^qd\gamma(x)\\
       &\leq \int_\R \int_\R \abs{f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)}^qd\gamma(y)d\gamma(x)\\
       &=\E\left[f\left(\abs{e^{-t}X+\sqrt{1-e^{-2t}}Y}^q \right)\right],
    \end{align*}
    donde hemos hecho uso de la desigualdad de Jensen, y $X,Y$ son variables aleatorias normales estándar independientes. Pero notemos que, denotando $Z:= e^{-t}X+\sqrt{1-e^{-2t}}Y$, la función característica de $Z$ es
    \[
    \phi_Z(z)=\E\left[e^{ize^{-t}X+iz\sqrt{1-e^{-2t}}Y}\right]=\E\left[e^{ize^{-t}X}\right]\E\left[e^{iz\sqrt{1-e^{-2t}}Y}\right]=e^{-\frac{z^2e^{-2t}}{2}}e^{-\frac{z^2(1-e^{-2t})}{2}}=e^{-\frac{z^2}{2}},
    \]  
    por lo que $Z$ tiene un distribución normal estándar. Se sigue que 
    \[
       \int_\R |P_tf(x)|^q d\gamma(x)=\E\left[f\left(\abs{e^{-t}X+\sqrt{1-e^{-2t}}Y}^q\right)\right]\leq\E\left[\abs{f(Z)}^q\right]=\int_\R \abs{f(x)}^qd\gamma(x).
    \]
  \end{itemize}
\end{proof}
Del último ítem de la proposición anterior se deduce que para cualquier $f\in \S$ y cualquier $t\geq0$, $P_tf\in L^q(\gamma)$. Más aún, la familia de operadores $(P_t)_{t\geq0}$ que van del espacio $(\S,\norm{\cdot}_{L^q(\gamma)})$ a $L^q(\gamma)$ son uniformemente acotados por la constante $1$.

Al ser $\S$ un subconjunto denso de $L^q(\gamma)$ y ser este último un espacio completo, por el conocido teorema BLT de análisis funcional, los operadores de $(P_t)_{t\geq0}$ se pueden extender de manera única a una familia de operadores, que también denotaremos por $(P_t)_{t\geq0}$, con dominio todo el espacio $L^q(\gamma)$, y cuyas normas siguen siendo acotadas por la constante 1. Esto último hace a $(P_t)_{t\geq0}$ una familia de operadores lineales contractivos.

Probamos ahora que $(P_t)_{t\geq0}$ en efecto forma un semigrupo.
\begin{teo} 
Sean $s,t\geq0$. Se tiene que $P_tP_s=P_{t+s}$ en $L^q(\gamma)$, para $q\in [1,\infty)$.
\end{teo}
\begin{proof} 
  Dado que la medida del espacio es finita, basta con probar el resultado para $L^1(\gamma)$. Sea $f\in L^1(\gamma)$. Tenemos las siguientes igualdades para $x\in \R$:
\begin{align*}
   P_{t}(P_sf)(x)&=\int_\R P_sf\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(x)\\
   &=\int_\R \int_\R f\left(e^{-s}(e^{-t}x+\sqrt{1-e^{-2t}}y)+\sqrt{1-e^{-2s}}z\right)d\gamma(z)d\gamma(y)\\
   &=\int_\R\int_\R f \left(e^{-(t+s)}x+e^{-s}\sqrt{1-e^{-2t}}y+\sqrt{1-e^{-2s}}z\right)d\gamma(z)d\gamma(y)\\
   &=\E\left[f \left(e^{-(t+s)}x+e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z\right)\right],
\end{align*}
donde $Y$ y $Z$ denotan variables aleatorias normales estándar e independientes. Ahora bien, nótese que la función característica de la variable $W:=e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z$ está dada por 

\begin{align*}
   \phi_W(w)&=\E\left[\exp \left\{iw(e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z)\right\}\right]\\
   &=\E\left[\exp \left\{iwe^{-s}\sqrt{1-e^{-2t}}Y\right\}\right]\E\left[\exp \left\{iw\sqrt{1-e^{-2s}}Z\right\}\right]\\
   &=\exp \left\{\frac{-w^2e^{-2s}(1-e^{-2t})}{2}\right\}\exp \left\{\frac{-w^2(1-e^{-2s})}{2}\right\}\\
   &=\exp \left\{\frac{-w^2(e^{-2s}-e^{-2(t+s)}+1-e^{-2s})}{2}\right\}\\
   &=\exp \left\{\frac{-w^2(1-e^{-2(t+s)})}{2}\right\},
\end{align*}
la cual corresponde con la función característica de una variable $V:=\sqrt{1-e^{-2(t+s)}}N$, donde $N$ es gaussiana estándar. Se sigue que 
\begin{align*}
   P_{t}(P_sf)(x)&=\E\left[f \left(e^{-(t+s)}x+e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z\right)\right]\\
   &=\E\left[f \left(e^{-(t+s)}+\sqrt{1-e^{-2(t+s)}}V\right)\right]\\
   &=\int_\R f \left(e^{-(t+s)}+\sqrt{1-e^{-2(t+s)}}v\right)d\gamma(v)\\
   &=P_{t+s}f(x).
\end{align*}
 \end{proof}
Finalmente, la siguiente proposición nos dice que tanto $P_t$ como $D$, vistos como operadores, pueden ser intercambiados en el espacio $\mathbb{D}^{1,2}$.
\begin{teo} 
 Sea $f\in \D^{1,2}$ y $t\geq0$. Entonces $P_tf\in \D^{1,2}$ y $DP_tf=e^{-t}P_tDf$.
 \end{teo}
 \begin{proof} 
   Sea $f\in \S$. La derivada de Malliavin coincide con la derivada usual en dicho espacio y por lo tanto, dado que la función $e^{-t}f' \left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)$ está dominada por una función similar a la del ítem 3 del teorema 8, podemos derivar bajo el signo de la integral, hallando que
   \begin{align*}
       DP_tf(x)&=\frac{d}{dx}\int_\R f \left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)\\
       &=\int_\R e^{-t}\frac{d}{dx}f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)\\
       &=e^{-t}P_t Df(x),
   \end{align*}
   para cualquier $x\in \R$. Esto prueba la propiedad en $\S\subseteq \D^{1,2}$. Para $f$ en dicho espacio general, usamos que $\S$ es denso en $\D^{1,2}$.
  \end{proof}
  Antes de continuar con la siguiente sección, mostramos la conexión de esta familia de operadores con el semigrupo de Ornstein-Uhlenbeck:
  \begin{teo} 
   Sea $B$ un movimiento browniano estándar. Consideramos la ecuación diferencial estocástica dada por 
   \[
   dX_t^{x}=\sqrt{2}dB_t-X^{x}_tdt, \qquad t\geq0,
   \]
   donde $X^{x}_0=x\in \R$. Se tiene que la solución de la ecuación diferencial anterior está dada por 
   \[
   X_t^x=e^{-t}x+\sqrt{2}\int_0^te^{-(t-s)}dB_s, \qquad t\geq0,
   \]
   el cual es un proceso de Markov. Más aún, el semigrupo asociado a dicho proceso es justamente $(P_t)_{t\geq0}$. Tal proceso es justamente el proceso de Ornstein-Uhlenbeck.
  \end{teo}
   \begin{proof} 
     Comenzamos notando que en efecto el proceso anteriormente definido cumple que $X_0^x=x$. Pero esto es claro ya que 
     \[
     X_0^x=e^{0}x+\int_{0}^{0}e^{-(t-s)}dB_s=x.
     \] 
     Vemos ahora que se cumple la ecuación diferencial estocástica. Sea $t>0$ y nótese que 
     \begin{align*}
       X_t^x&=e^{-t}x+\sqrt{2}\int_0^te^{-(t-s)}dB_s\\
       &=e^{-t}x+\sqrt{2}e^{-t}\int_0^{t}e^{s}dB_s\\
       &=e^{-t}\left(X_0^{x}e^{0}+\sqrt{2}e^{-t}\int_0^te^{s}dB_s \right),
     \end{align*}
     de donde concluimos que 
     \[
     e^{t}X_t^x-e^{0}X_0^x=\sqrt{2}\int_{0}^{t}e^{s}dB_s,
     \]
     lo cual en notación diferencial equivale a que 
     \[
     d \left(e^tX_t^x\right)=\sqrt{2}e^{t}dB_t,  \qquad t>0.
     \]
     Usando ahora la regla del producto y el hecho de que $e^{-t}$ es un proceso de variación finita,  
     \begin{align*}
       dX_t^{x}&=d(e^{-t}e^{t}X_t^{x})\\
       &=e^{-t}d(e^{t}X_t^{x})+e^{t}X_t^{x}d(e^{-t})+0\\
       &=e^{-t}\sqrt{2}e^{t}dB_t-e^{t}X_t^{t}e^{-t}dt\\
       &=\sqrt{2}dB_t-X_t^xdt,
     \end{align*}
     tal como queríamos probar. Por lo tanto el proceso $(X_t^x)_{t\geq0}$ definido como antes es solución a la ecuación diferencial estocástica dada. Como tal, dicho proceso resulta ser una difusión, y en particular un proceso de Markov. Resta ver que su semigrupo justamente es el semigrupo de Ornstein-Uhlenbeck definido antes.

     Directamente de la expresión de $X_t^{x}$, observamos que tenemos una integral de Itô cuyo integrando es estocástico. Por lo tanto, dicha integral forma un proceso Gaussiano adaptado a la filtración generada por $B$. Por otro lado, calculando la función de medias y covarianzas del proceso hallamos lo siguiente:
     \begin{itemize}
        \item $\mu(t)=\E\left[X_t^{x}\right]=e^{-t}x$
        \item \begin{align*}\Gamma(s,t)&=\text{Cov}\left(X_s^x,X_t^x\right)\\
            &=\E\left[\left(X_t^{x}-e^{-t}x\right)\left(X_s^x-e^{-s}x\right)\right]\\
            &=\E\left[\left(\sqrt{2}e^{-t}\int_{0}^{t}e^{u}dB_u\right)\left(\sqrt{2}e^{-s}\int_{0}^{s}e^{v}dB_v\right)\right]\\
            &=2e^{-(t+s)}\E\left[\int_{0}^{s\wedge t}e^{2u}du\right]\\
            &=\frac{2}{2}e^{-(t+s)}(e^{2s\wedge t}-1)\\
            &=e^{-|t-s|}-e^{-(t+s)}.
        \end{align*}
     \end{itemize}
     En particular, se tiene para $s=t$ que $X_t^x\sim Normal(e^{-t}x,1-e^{-2t})$. Finalmente, si $B=(B_t)_{t\geq0}$ representa el movimiento browniano sobre el cual $(X_t^x)_{t\geq0}$ está definido, notamos que para $t\geq0$,
     \[
     \E\left[e^{-t}x+e^{-t}B_{e^{2t}-1}\right]=e^{-t}x
     \]
     y además, 
     \[
     \E\left[\left(e^{-t}B_{e^{2t}-1}\right) \left(e^{-s}B_{e^{2s}-1}\right)\right]=e^{-(t+s)}(\min(e^{2t},e^{2s})-1)=e^{-(t+s)}(e^{2s\wedge t}-1)=e^{-|t-s|}-e^{-(t+s)}.
     \]
     Concluimos que el proceso de Ornstein-Uhlenbeck $(X_t^{x})_{t\geq0}$ como solución a la ecuación diferencial estocástica dada antes, puede verse como 
     \[
        X_t^{x}=e^{-t}x-e^{-t}B_{e^{2t}-1}, \qquad t\geq0.
     \]
     De lo anterior, es muy sencillo ver que para $f$ medible y acotada, 
     \begin{align*}
      P_tf(x)&=\E\left[f(X_{t+s}^{x})|\F_s\right]\\
      &=\E_x\left[f(e^{-(t+s)}x-e^{-(t+s)}B_{e^{2(t+s)}-1})|\F_s\right]\\
      &=\int_\R f \left(e^{-t}x-\sqrt{1-e^{-2t}}y\right)d\gamma(y),
     \end{align*}
     
     el cual es justamente el semigrupo que hemos estado trabajando.
   \end{proof}

\subsection{El generador infinitesimal}
 Como es clásico en el contexto de procesos de Markov, asociado a un semigrupo tenemos la noción de generador infinitesimal. En nuestro caso consideremos al conjunto siguiente:
 \[
 \text{Dom}(L):=\left\{f\in L^2(\gamma): \lim_{h\to 0} \frac{P_{h}f-f}{h} \text{ existe en } L^{2}(\gamma)\right\}.
 \]  
Dicho es directamente el dominio del generador infinitesimal $L$. Así, dada $f\in \text{Dom}(L)$, se tiene que 
\[
Lf:=\lim_{h\to 0}\frac{P_hf-f}{h}.    
\]
Ya que el límite anterior recrea la definición de derivada en funciones reales, denotaremos
\[
   \frac{d}{dt}P_tf:=\lim_{h\to0}\frac{P_{t+h}f-P_tf}{h}, \qquad \text{ y en particular } \qquad  Lf:=\frac{d}{dt}P_tf\bigg|_{t=0},
\]
omitiendo en ocasiones la función $f$ cuando estemos hablando de una función en un conjunto particular. Ahora bien, en el espacio $\S$, para cualquier $t\geq0$ se tiene el siguiente resultado:
\[
\frac{d}{dt}P_t=\lim_{h\to0}\frac{P_{t+h}-P_t}{h}=\lim_{h\to0}P_t\frac{P_h-Id}{h}=P_t\lim_{h\to0}\frac{P_h-Id}{h}=P_t\frac{d}{dh}\bigg|_{h=0}P_h=P_tL.
\]
Invirtiendo la manera en cómo se toman los límites, se obtiene también que
\[
\frac{d}{dt}P_t=LP_t,  
\]
así que para cualquier $t\geq0$, y $f\in \S$, tenemos que 
\[
\frac{d}{dt}P_t=P_tL=LP_t.
\]

Finalmente, una proposición sumamente importante es el siguiente teorema.
\begin{teo} 
En el espacio de Schwarz, se tiene que $Lf=-\delta Df$. Más específicamente, para cualquier $f\in \S$, 
\[
Lf(x)=-xf'(x)+f''(x).   
\]
\end{teo}
 \begin{proof} 
   Sea $f\in \S$ y $t>0$. Notamos que, utilizando una función dominante, que existe gracias al hecho de que $f$ es suave, intercambiamos derivada con integral, por lo que
   
   \begin{align*}
    \frac{d}{dt}P_tf(x)&=\frac{d}{dt}\int_\R f(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
    &=\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)\left(-e^{-t}x+\frac{-y(-2e^{-2t})}{2\sqrt{1-e^{-2t}}}\right)d\gamma(y)\\
    &=\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)\left(-e^{-t}x+\frac{ye^{-2t}}{\sqrt{1-e^{-2t}}}\right)d\gamma(y)\\
    &=-x\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)e^{-t}d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\int_\R yf'(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y).
   \end{align*}
   Reconocemos en el primer integrando la derivada con respecto a $x$ de $P_tf$, mientras que en el segundo término reconocemos la integral de una función de la forma $yg(y)$, donde $g(y)=f'(e^{-t}x+\sqrt{1-e^{-2t}}y)$, función que está en $L^1(\gamma)$. Por lo tanto, por el Lema \ref{lema1}, 
    \begin{align*}
        &-x\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)e^{-t}d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\int_\R yf'(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
        &=-x\int_\R \frac{d}{dx}f(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\int_\R \frac{d}{dy}f'(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
        &=-x\frac{d}{dx}\int_\R f(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\sqrt{1-e^{-2t}}\int_\R f''(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
        &=-x(P_tf(x))'+e^{-2t}P_tf''(x).
    \end{align*}
    Tomando límite cuando $t\to0$ hallamos que 
    \[
    Lf(x)=\frac{d}{dt}P_tf(x)\bigg|_{t=0}=-\left(xf'(x)+f''(x)\right).
    \]
    Finalmente, dado que $f\in \S$, su derivada de Malliavin coincide con su derivada usual, por lo que usando la proposición ???, el generador infinitesimal de una función en el espacio de Schwarz viene dado por 
    \[
    Lf(x)=-\delta Df(x).
    \]
  \end{proof}
Como ejemplo de la potencia de la proposición anterior, se demuestra a continuación la desigualdad de Poincaré en el contexto de los espacios de Watanabe-Sobolev.
Para tener una idea de la comparación entre ambos contextos, presentamos tanto la desigualdad de Poincaré en el contexto de espacios de Sobolev clásicos en $\R$ como la desigualdad de Poincaré en $\D^{2,p}$.

Recordemos que $W^{k,p}(U)=\left\{f\in L^{p}(U,\B(U),dx): D^jf\in L^p(U,\B(U),dx), j\in \{1,2,...,k\}\right\}$ es el espacio de Sobolev que consiste en el conjunto de aquellas funciones en $L^p(U)$ ($U\subseteq \R$ subconjunto abierto) tales que todas sus derivadas hasta la $k$-ésima en el sentido distribucional también pertenecen a $L^p(U,\B(U),dx)$.

\begin{teo}\textbf{(Desigualdad de Poincaré clásica)}
   Sean $p\in [1,\infty)$ y $W_0^{1,p}(U)=\overline{C^{\infty}_c(U)}$ la cerradura del espacio de las funciones test, con respecto a la norma $\|.\|_{W^{1,p}(U)}$ definida como
   \[
     \|f\|_{W^{1,p}(U)}=\left(\|f\|_{L^{p}(U)}^p+\|Df\|_{L^p(U)}^p\right)^{1/p}, \qquad \text{ para } f\in W^{1,p}(U).
    \]
   Entonces si $U$ es un abierto acotado de $\R$, y $u\in W_0^{1,p}(U)$, se tiene la desigualdad 
   \[
   \|u\|_{L^{p}(U)}\leq C\|Du\|_{L^p(U)},   
   \]
   donde $C$ es una constante que solo depende de $p$ y $U$. 
   \end{teo}
   Esta cota nos da la posibilidad de controlar el crecimiento en $L^p(U)$de una función $u\in W_0^{k,p}$ vía la norma de su derivada $Du$ en $L^p(U)$.

   Más aún, gracias a la definición de $\|.\|_{W^{1,p}(U)}$, la cota anterior nos dice que si tenemos un subconjunto $U\subseteq \R$ abierto y acotado, en $W_0^{1,p}(U)$ las normas $\|\cdot\|_{W_{1,p}(U)}$ y $\|D\cdot\|_{L^p(U)}$ son equivalentes. Pasamos ahora a nuestra versión de la desigualdad.
   \begin{teo} \textbf{(Desigualdad de Poincaré unidimensional)} Sea $N\sim Normal(0,1)$ y sea $f\in \D^{1,2}$. Entonces
       \[
       \text{Var}\left(f(N)\right)=\E\left[(Df(N))^2\right]    
       \]     
    \end{teo}
    \begin{proof} 
        Supongamos que $f\in \S$ y para $f\in \D^{1,2}$ utilizamos un argumento de aproximación. Dado que $f\in \S$, podemos intercambiar los símbolos de derivada e integral. Por lo tanto, tenemos que 
        \begin{align*}
            \text{Var}\left(f(N)\right)&=\E\left[f(N)(f(N)-\E\left[f(N)\right])\right]\\
            &=\E\left[f(N)(P_0f(N)-P_\infty)f(N)\right]\\
            &=-\int_{0}^{\infty}\E\left[f(N)\frac{d}{dt}P_tf(N)\right]dt\\
            &=\int_{0}^{\infty}\E\left[f(N)\delta DP_tf(N)\right]dt\\
            &=\int_{0}^{\infty}\E\left[f'(N)DP_tf(N)\right]dt\\
            &=\int_{0}^{\infty}e^{-t}\E\left[f'(N)P_tf'(N)\right]dt\\
            &\leq e^{-t}\sqrt{\E\left[f'^2(N)\right]}\sqrt{\E\left[(P_tf')^2(N)\right]}dt\\
            &\leq \E\left[f'^2(N)\right]\int_{0}^{\infty}e^{-t}dt\\
            &=\E\left[f'^2(N)\right].
        \end{align*} 
     \end{proof}
Nótese que esta desigualdad en términos de la norma $L^2(\gamma)$ se escribe como sigue:
\[
\|f\|^2_{L^2(\gamma)}\leq \E^2\left[f(N)\right]+\|Df\|^2_{L^2(\gamma)}, \qquad \forall f\in \D^{1,2},
\]
de manera que, si $\E\left[f(N)\right]=0$, directamente recuperamos la desigualdad de Poincaré clásica, aunque en distinto contexto. Y como tal, esta desigualdad nos permite controlar el crecimiento de las funciones $f\in \D^{1,2}$ utilizando la norma de su derivada de Malliavin.

Concluimos esta sección con la denominada `relación de conmutatividad de Heisenberg'.
\begin{teo} 
 Para $f\in \S$, y $p\geq1$ se tiene que 
 \[
    (D\delta^p-\delta^p D)f=p\delta^{p-1}f.
 \]
 En particular para $p=1$, se tiene que
 \[
 (D\delta-\delta D)f=f.
 \]
 \end{teo}
\begin{proof} 
   Realizamos la prueba para el caso $p=1$. El caso general se sigue usando inducción. Sea $f\in \S$ y notemos que para cualquier $x\in \R$, 
\begin{align*}
   (D\delta-\delta D)f(x)&=D\delta f(x) - \delta D f(x)\\
   &=D \delta f(x) - (xDf(x)-D^2f(x))\\
   &=D \left(\delta f(x)+Df(x)\right) -xDf(x)\\
   &=D \left(xf(x)\right)-xDf(x)\\
   &=f(x)+xDf(x)-xDf(x)\\
   &=f(x),
\end{align*}
en donde en la segunda y cuarta igualdad hemos usado la fórmula del lema \ref{formuladelta}.
 \end{proof}
\section{Cálculo de Malliavin en el caso general.}

En esta sección se presentan los operadores de Malliavin generales, que serán utilizados en lo subsecuente. Seguimos de cerca los requerimientos de \cite[sección 2.1]{KUZGUN202268}

Denotemos por $\mathfrak{H}:=L^2(\R_+\times\R)$. Ahora denotaremos por $\S$ al siguiente conjunto:
\[
\S:=\left\{F=f(W(h_1),...,W(h_n))) \ : \ f\in C^{\infty}_b, \ h_1,...,h_n\in \mathfrak{H} \right\},
\]
donde $W=\left\{W(h):h\in \mathfrak{H}\right\}$ es el proceso Gaussiano isonormal sobre el cual estamos trabajando. Definimos ahora la derivada de Malliavin.
\begin{dfn} 
 Sea $F\in \S$, y sea $p\geq1$ un entero. La derivada de Malliavin de orden $p$ de $F$ es el elemento de $L^2(\Omega;\mathfrak{H}^{\odot})$ definido por 
 \[
 D^pF:=\sum_{i_1,...,i_p=1}^{n}\frac{\partial^pf}{\partial x_{i_1}...\partial x_{i_p}}(W(h_1),...,W(h_n))h_{i_1}\otimes...\otimes h_{i_p}.
 \] 
 En el caso en que $p=1$ hablaremos simplemente de la <<derivada de Malliavin>>.
 \end{dfn}
 Esta derivada cumple con una condición similar a la del caso unidimensional.

 \begin{prop} 
  Sea $q\in [1,\infty)$, y sea $p\geq 1$ un entero. Entonces el operador 
  \[
  D^{p}:\S\subseteq L^q(\Omega)\to L^q(\Omega;\mathfrak{H}^{\odot p})
  \]
  es cerrable.
  \end{prop}
En consecuencia, para $q\in [1,\infty)$ y $p\geq1$ un entero, definimos la norma $\norm{\cdot}_{\D^{p,q}}:\S\to [0,\infty)$ dada por 
\[
\norm{F}_{D^{p,q}}:= \left(\E\left[\abs{F}^{q}\right]+\E\left[\norm{DF}^{q}_{\mathfrak{H}}\right]+...+\E\left[\norm{D^pF}_{\mathfrak{H}^{\otimes p}}^q\right]\right)^{1/q}.
\]
y denotamos por $\D^{p,q}$ a la cerradura del espacio $\S$ con respecto a la norma anterior.

Pasamos ahora al operador de divergencia. Dado un entero $p\geq1$, definimos el poerador de divergencia de orden $p$ como el operador adjunto del operador derivada $D^{p}:\D^{p,2}\to L^{2}(\Omega,\mathfrak{H})$.

\begin{dfn} 
 Sea $p\geq1$ un entero. Denotamos por $\text{Dom}(\delta^p)$ al subconjunto de $L^2(\Omega;\mathfrak{H}^\otimes p)$ compuesto de aquellos elementos $u$ tales que existe una constante $C>0$ que satisface 
 \[
 \abs{\E\left[\langle D^pF,u\rangle_{\mathfrak{H}^{\otimes p}}\right]}\leq c\sqrt{\E\left[F^2\right]}, \qquad \forall F \in \S.
 \]
 \end{dfn}
 Argumentando de manera similar al caso unidimensional, por el teorema de representación de Riesz, existe un único elemento en $L^2(\Omega)$, denotado por $\delta^p(u)$ tal que cumple la relación 
 \[
 \E\left[\langle D^pF,u\rangle_{\mathfrak{h}^{\otimes p}}\right]=\E\left[F\delta^{p}(u)\right], \qquad \forall f\in \S.
 \]
 Llegamos así a la siguiente definición.

 \begin{dfn} 
  Si $u\in \text{Dom}(\delta^p)$, entonces $\delta^p(u)$ es el único elemento de $L^2(\Omega)$ caracterizado por la siguiente fórmula:
\[
 \E\left[\langle D^pF,u\rangle_{\mathfrak{h}^{\otimes p}}\right]=\E\left[F\delta^{p}(u)\right], \qquad \forall f\in \S.
 \]
  \end{dfn}
Este operador se denomina el operador de divergencia múltiple de orden $p$. Cuando $p=1$ simplemente lo llamamos el <<operador de divergencia>>,  escribiendo $\delta$ en lugar de $\delta^1$. La fórmula anterior se suele llamar <<fórmula de integración por partes>>.

El siguiente resultado nos permite <<sacar>> una variable aleatoria escalar dentro del operador de divergencia $\delta$. Este resultado se puede consultar en \cite[proposición 2.5.4]{Nourdin_Peccati_2012}, sin embargo, enunciamos una versión general que se encuentra en \cite[lema 1]{Caballero1998-hz}.

\begin{teo}\label{factordeltafuera}
 Sean $p,p'>1$ tales que $\frac{1}{p}+\frac{1}{p'}=1$. Supongamos que $F\in \D^{1,p'}$, y sea $u \in \text{Dom}(\delta)$ tal que $u\in L^{p}(\Omega;\mathfrak{H})$ y $\delta(u)\in L^{p}(\Omega)$. Entonces $Fu\in \text{Dom}(\delta)$ y se cumple la igualdad 
 \begin{equation}\label{eqfactordeltafuera}
   \delta (Fu)=F\delta(u)-\langle DF,u\rangle_{\mathfrak{H}}.
\end{equation}
 
 \end{teo}

 Tenemos también la propiedad de conmutatividad de Heisenberg en este caso. Dicha propiedad es compleamente análoga al caso unidimensional.

\begin{teo} 
Se cumple que 
\[
D\delta(u)-\delta(Du)=u.
\]
 \end{teo}

Finalmente, es de destacar una propiedad muy interesante del operador de divergencia: cuando estamos en el caso en que $\mathfrak{H}=L^2(\R_+\times\R)$, el operador delta coincide con la integral de Itô-Walsh.

\begin{teo} 
 Sea $u$ un proceso adaptado y cuadrado-integrable. Entonces $u\in \text{Dom}(\delta)$ y se cumple que 
 \[
 \delta(v)=\int_{\R_+\times \R}^{}v(s,y)W(ds,dy).
 \]
 \end{teo}
 
 \chapter{Aplicaciones a Teoremas Límite y estudio de densidades}
En este capítulo se visita la aplicación de los conceptos del cálculo de Malliavin al estudio de Teoremas límite y al estudio de densidades. 
Una herramienta clave en esta tarea es el conocido como Método de Stein. 
Dicho método, desarrollado por Charles Stein en la década de 1970, es una herramienta bastante popular para evaluar la distancia entre las leyes de dos leyes de probabilidad. Lo anterior es posible lograrlo estudiando operadores diferenciales involucrados con las leyes de probabilidad en cuestión.

\section{Método de Stein}

Dentro del mundo de la probabilidad, la distribución normal juega un papel importante debido a la gran cantidad de propiedades que la misma presenta. Entre ellas, dicha distribución está determinada por sus momentos. Esto hace posible caracterizar la distribución en términos de los mismos. Ésta propiedad, la cual está codificada en el conocido como `lema de Stein', forma parte importante de este texto. 

Comenzamos dando una prueba de que la ley de una variable Gaussiana estándar está caracterizada por sus momentos. Recordemos que, dada una medida $\nu$ en $\R$, el $n$-ésimo momento de la misma, cuando existe, está dado por 
\[
   m_n(\nu):=\int_{\R}x^{n}d\nu(x), \qquad n\geq0.
\]

\begin{lema} 
 Sea $\gamma$ la distribución gaussiana estándar. Dicha distribución está caracterizada por sus momentos.
 \end{lema}
 \begin{proof} 
    Debemos mostrar que, si existe otra medida definida en $\R$ con todos sus momentos definidos, digamos $\mu$, y  tales momentos coinciden con los de $\gamma$, entonces $\mu=\gamma$.

    Sea entonces $\mu$ una medida como antes. Es suficiente mostrar que la función característica de ambas medidas coincide en todo $\R$, esto es, que 
    \[
      \int_\R e^{itx}d\mu(x)=\int_\R e^{itx}d\gamma(x), \qquad \forall t\in \R.
    \]
    Utilizando la forma de Lagrange del residuo en el teorema de Taylor y la desigualdad de Cauchy-Schwarz, tenemos para cualquier $n\geq1$ que
    \begin{align*}
      \abs{\int_{\R}e^{itx}d\nu(x)-\int_\R e^{itx}d\gamma(x)}&\leq \int_\R \abs{e^{itx}-\sum_{k=0}^{n}\frac{(itx)^k}{k!}}d\nu(x)+\int_\R \abs{e^{itx}-\sum_{k=0}^{n}\frac{(itx)^k}{k!}}d\gamma(x)\\
      &=\int_\R \abs{\frac{(it)^{n+1}e^{it\xi_L}x^{n+1}}{(n+1)!}}d\nu(x)+\int_\R \abs{\frac{(it)^{n+1}e^{it\eta_L}x^{n+1}}{(n+1)!}}d\gamma(x)\\
      &\leq\int_\R \frac{\abs{tx}^{n+1}}{(n+1)!}d\nu(x)+\int_\R \frac{\abs{tx}^{n+1}}{(n+1)!}d\gamma(x)\\
      &\leq \left(\int_{\R}\frac{\abs{tx}^{2n+2}}{(n+1)!^2}d\nu(x)\right)^{1/2}+\left(\int_{\R}\frac{\abs{tx}^{2n+2}}{(n+1)!^2}d\gamma(x)\right)^{1/2}\\
      &=\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\nu)\right)^{1/2}+\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\gamma)\right)^{1/2}\\
      &=2\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\gamma)\right)^{1/2}.
    \end{align*}
   Recordando que para la distribución Gaussiana se cumple que $m_{2n}(\gamma)=\frac{(2n)!}{2^{n}n!}$, y usando cotas superior e inferior de la fórmula de Stirling, tenemos que 
   \begin{align*}
      2\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\gamma)\right)^{1/2}&=2|t|^{n+1}\left(\frac{(2n+2)!}{2^{n+1}(n+1)!^3}\right)^{1/2}\\
      &\leq 2|t|^{n+1}\left(\frac{\sqrt{4\pi(n+1)}\left(\tfrac{2(n+1)}{e}\right)^{2n+2}e^{-12\cdot 2(n+1)}}{2^{n+1}\left(\sqrt{2\pi(n+1)}\left(\tfrac{n+1}{e}\right)^{n+1}e^{-12(n+1)+1}\right)^{3}}\right)^{1/2}\\
      &\leq 2|t|^{n+1}\left(\frac{2^ne^{13(n+1)+3}}{\pi(n+1)^{n+2}}\right)^{1/2}\\
      &=\frac{K_t^{\tfrac{13}{2}(n+1)}}{(n+1)^{n/2+1}},\\
   \end{align*} 
   donde $K_t$ es una constante dependiente sólo de $t$. Pero esta última expresión tiende a cero cuando $n$ tiende a infinito. Concluimos que las funciones características de ambas medidas coinciden y por lo tanto, son equivalentes.
  \end{proof}

Ahora enunciamos y demostramos el conocido como `lema de Stein'.
\begin{teo}\textbf{(Lema de Stein)} \label{LemaStein} Sea $X$ una variable aleatoria. $X$ tiene una distribución normal estándar si y sólo sí, para cualquier función $f$ derivable y tal que $f'\in L^{1}(\gamma)$, se tiene que
   \begin{enumerate}
       \item $\E\left[Xf(X)\right]<\infty \ $ y $ \ \E\left[f'(X)\right]<\infty$.
       \item $\E\left[Xf(X)\right]=\E\left[f'(X)\right]$.
   \end{enumerate}
\end{teo}
\begin{proof} 
  Si directamente $X\sim$ Normal$(0,1)$, cualquier función que cumpla las condiciones del enunciado cumple las del lema \ref{lema1}, el cual precisamente estipula la relación anterior. 

  Supongamos ahora que $X$ satisface la relación $\E\left[Xf(X)\right]=\E\left[f'(X)\right]$ para cualquier función $f$ con las propiedades del enunciado. Si definimos $f_n(x)=x^n$ para cualquier $n\geq1$, tenemos que la variable $X$ tiene todos sus momentos definidos, y además 
  \[
   \E\left[X^{n+1}\right]=\E\left[Xf(X)\right]=\E\left[f'(X)\right]=n\E\left[X^{n-1}\right],
  \] 
  de tal forma que los momentos de la variable $X$ coinciden con los de una variable normal estándar. Dado que la distribución normal está caracterizada por sus momentos según vimos en el resultado previo, se sigue que $X\sim$ Normal$(0,1)$.
 \end{proof}
El resultado anterior es interesante por varios motivos. Uno de ellos es que nos permite caracterizar plenamente la distribución normal a partir de funciones y sus derivadas aplicadas a una variable aleatoria que siga dicha distribución. Por otro lado, un instante de reflexión nos puede llevar a la siguiente pregunta.

Supongamos que ahora $X$ es una variable aleatoria y que para una cierta clase suficentemente rica de funciones suaves, se cumple que 
\[
\E\left[Xf(X)-f'(X)\right]\approx0.    
\]
En virtud del lema de Stein, ¿es posible decir que la distribución de $X$ está cerca (en algún sentido) de la distribución normal estándar?

La pregunta anterior se conoce como la \textit{heurística de Stein}, y la respuesta es afirmativa en algunos casos. Para llegar a una formulación explícita de dicha respuesta, debemos introducir la idea de `distancia entre dos medidas de probabilidad' y también la famosa `ecuación de Stein' asociada a alguna función $h\in L^1(\gamma)$. 

La definición de distancia se otorga para variables que tomen valores en $\R^d$. Comenzamos definiendo la noción de distancia entre dos variables aleatorias. Recordamos la noción de clase separante de funciones.

\begin{dfn} 
 En un espacio de probabilidad $(\Omega, \F, \P)$, sea $\mathscr{H}$ un conjunto de funciones borelianas complejas con valores en $\R^d$, para algún $d\geq1$. Decimos que $\mathscr{H}$ es una familia de funciones separantes si para cualesquiera dos variables aleatorias $F$ y $G$ con valores en $\R^{d}$ tales que $h(F)$ y $h(G)$ son elementos de $L^1(\Omega)$ y $\E\left[h(F)\right]=\E\left[h(G)\right]$, para cualquier elemento $h\in \mathscr{H}$ implica que $F$ y $G$ tienen la misma ley.
 \end{dfn}
Ejemplos clásicos de familias $\mathscr{H}$ separantes son el conjunto de funciones borelianas continuas y acotadas, el conjunto de funciones indicadoras de los borelianos de $\R^{d}$, o el conjunto de funciones exponenciales de la forma $\mathscr{H}=\left\{e^{i \langle v,\cdot\rangle}: v\in \R^{d} \right\}$, donde el producto interno anterior es el producto interno usual de $\R^{d}$.

Definimos a continuación, y de manera rigurosa, la noción de distancia entre las leyes de dos variables aleatorias.
\begin{dfn} 
 Sea $\mathscr{H}$ una clase separante de funciones, y sean $F$ y $G$ dos variables aleatorias con valores en $\R^{d}$ y tales que $h(F)$ y $h(g)$ están en $L^1(\Omega)$ para cualquier $h\in \mathscr{H}$. Definimos la distancia inducida por $\mathscr{H}$ entre las distribuciones de $F$ y $G$ como 
 \[
 d_\mathscr{H}(F,G):=\sup_{h\in \mathscr{H}}\left\{\abs{\E\left[h(F)\right]-\E\left[h(G)\right]}\right\}.
 \]
 \end{dfn}

Es sencillo demostrar que para una clase de funciones separantes $\mathscr{H}$, el mapeo $d_\mathscr{H}$ definido anteriormente en efecto define una métrica en cierto subconjunto de las medidas de probabilidad sobre $\R^{d}$. Es pertinente hacer hincapié en que la distancia inducida entre las leyes de variables depende de la clase $\mathscr{H}$. Ello resulta en una amplia variedad de opciones con las cuales considerar la distancia entre dos distribuciones de probabilidad. Algunas de las más usadas se encuentran a continuación.

\begin{itemize}
   \item \textbf{La distancia de Kolmogorov.} Esta distancia entre dos variables $F$ y $G$, denotada por $d_{\text{Kol}}(F,G)$, se obtiene utilizando a la familia \[\mathscr{H}= \left\{h:\R^d\to \R \ :\ h(x_1,...,x_d)=\1_{(-\infty,z_1]}(x_d)\cdot ... \cdot \1_{(-\infty,z_d]}(x_d), \ \  z_1,...,z_d\in \R\right\},\]
   de manera que dicha distancia está dada por la expresión
   \[
   d_{\text{Kol}}(F,G)=\sup_{z_1,...,z_d\in \R}\abs{\P(F\in (-\infty,z_1]\times...\times(-\infty,z_d])-\P(G\in (-\infty,z_1]\times...\times(-\infty,z_d])}.
   \]
   \item \textbf{La distancia de variación total.} Dicha distancia entre las leyes de las variables $F$ y $G$ se define utilizando la clase \[\mathscr{H}=\left\{h:\R^d\to \R \ :\ h(x_1,...,x_d)=\1_B(x_1,...,x_d), \ \ B\in \B(\R^{d})\right\},\] 
   y dicha distancia se denota por $d_{\text{TV}}(F,G).$ Su expresión explícita es 
   \[
      d_{\text{TV}}(F,G)= \sup_{B\in \B(\R^{d})}\abs{\P(F\in B)-\P(G\in B)}.
   \]
   Observamos directamente que $d_{\text{Kol}}(F,G)\leq d_{\text{TV}}(F,G)$.
   \item \textbf{La distancia de Wasserstein.} Usando la clase de funciones separantes \[\mathscr{H}=\left\{h:\R^{d}\to \R \ :\ \norm{h}_{\text{Lip}}\leq 1\right\},\] donde claramente $\norm{\cdot}_{Lip}$ denota la norma Lipschitz en $\R^{d}$. Esta distancia se denota como $d_{\text{W}}(F,G)$.
\end{itemize}

Armados con la noción de distancia entre dos leyes de probabilidad, pasamos ahora a estudiar brevemente las ecuaciones de Stein. Dichas ecuaciones nos permiten codificar la distancia entre la ley de una variable $F$ y una variable aleatoria normal $N$, en virtud del lema de Stein \ref{LemaStein}. De manera rigurosa, tenemos la siguiente definición.

\begin{dfn} 
 Sea $N$ una variable gaussiana estándar y sea $h:\R\to\R$ una función boreliana tal que $\E\left[h(N)\right]<\infty$. La ecuación de Stein asociada con $h$ es la ecuación diferencial ordinaria 
 
 \begin{equation}\label{eqstein}
   f'(x)-xf(x)=h(x)-\E\left[h(N)\right].
 \end{equation}
 
 Diremos que $f$ es una solución a la ecuación \eqref{eqstein} si dicha función es absolutamente continua y tal que existe función equivalente a la derivada $f'$ que satisface \eqref{eqstein} para cualquier $x\in \R$.
 \end{dfn}

 Es de notar que dicha ecuación diferencial es sencilla de resolver. Este es el contenido del siguiente resultado.

 \begin{teo}\label{soleqstein}
  Las soluciones a la ecuación \eqref{eqstein} son de la forma 
  \[
  f(x)=ce^{x^2/2}+e^{x^2/2}\int_{-\infty}^x(h(y)-\E\left[h(N)\right])e^{-y^2/2}dy, \qquad \forall x\in \R,
  \] 
  donde $c\in \R$ es constante. En particular, si escribimos 
  \[
   f_h(x):=e^{x^2/2}\int_{-\infty}^x(h(y)-\E\left[h(N)\right])e^{-y^2/2}dy,
  \]
  entonces $f_h$ es la única solución a la ecuación de Stein \eqref{eqstein} que satisface la condición 
  \[
  \lim_{x\to \infty}e^{-x^2/2}f(x)=0.
  \]
  \end{teo}
  \begin{proof} 
    Supongamos que $h$ cumple las hipótesis del enunciado. Partiendo de las igualdades 
    \[
    \frac{d}{dx}\left(e^{-x^2/2}f(x)\right)=-xe^{-x^2/2}f(x)+e^{-x^2/2}f'(x)=e^{-x^2/2}(f'(x)-xf(x)),
    \]
    la ecuación de Stein $\eqref{eqstein}$ se puede reformular como 
    \[
      e^{x^2/2}\frac{d}{dx}\left(e^{-x^2/2}f(x)\right)=h(x)-\E\left[h(N)\right].
    \] 
    Usando el método de factor integrante, es claro que las soluciones a la ecuación  \eqref{eqstein} tienen la forma 
    \[
    f(x)=ce^{x^2/2}+^{x^2/2}\int_{-\infty}^{x}(h(y)-\E\left[h(N)\right])e^{-y^2/2}dy, \qquad \forall x\in \R.
    \]
   Para el comportamiento en $\pm\infty$ de las solución $f_h$, usando la función $g(x)=\abs{h(y)-\E\left[h(N)\right]}e^{-y^2/2}$, la cual es integrable e independiente de $x\in \R$, por el teorema de convergencia dominada, tenemos para el límite en $+\infty$ que
   \begin{align*}
      \lim_{x\to \infty}\int_{-\infty}^x \left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}dy&=\int_\R\lim_{x\to\infty}\left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}\1_{(-\infty,x]}(y)dy\\
      &=\int_\R h(y)e^{-y^2/2}dy-\E\left[h(N)\right]\int_\R e^{-y^2/2}dy\\
      &=\sqrt{2\pi}\E\left[h(N)\right]-\sqrt{2\pi}\E\left[h(N)\right]\\
      &=0,
      \end{align*}
      mientras que para el límite en $-\infty$ tenemos que 
      \[
         \lim_{x\to -\infty}\int_{-\infty}^x \left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}dy=\int_\R\lim_{x\to-\infty}\left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}\1_{(-\infty,x]}(y)dy=0.
      \]
      Se sigue entonces que si $f$ es una solución como las anteriormente mencionadas, entonces $e^{-x^2/2}f(x)\xrightarrow[x\to\pm\infty]{}0$ si y solo si $c=0$.
   \end{proof}

 Contando ahora con estas herramientas, podemos evaluar de manera rigurosa la distancia que hay entre la ley de una variable $X$ y una variable normal estándar $N$. Para ello, consideremos una función $h:\R\to\R$ que cumple que $\E\left[\abs{h(X)}\right]<\infty$ y $\E\left[\abs{h(N)}\right]<\infty$. Sea $f_h$ la solución única del teorema \ref{soleqstein}. Tomando esperanza a ambos lados de la ecuación de Stein, tenemos que $f_h$ cumple que 
\begin{equation}\label{eqmetodostein}
   \E\left[h(X)\right]-\E\left[h(N)\right]=\E\left[f'_h(X)-Xf_h(X)\right].
 \end{equation}
 
 En particular, notamos que para una clase de funciones adecuada $\mathscr{H}$, 
  \begin{equation*}
   d_\mathscr{H}(X,N)=\sup_{h\in \mathscr{H}}\abs{\E\left[h(X)\right]-\E\left[h(N)\right]}=\sup_{h\in \mathscr{H}}\abs{\E\left[f_h'(X)-Xf_h(X)\right]}.
   \end{equation*}
 Es de destacar que en la expresión anterior, la distancia entre las leyes de las variables $X$ y $N$ están codificadas en la cantidad de la derecha, la cual no involucra la ley Gaussiana estándar de manera directa. Por otro lado, es deseable desarrollar algunas cotas para la norma de las soluciones $f_h$, dependiendo del tipo de funciones $h\in\mathscr{H}$ que estemos considerando. Dependiendo de dicha clase, podremos obtener distintas cotas para las distancias en las que estemos interesados. A dichas cotas se les conoce como \textit{cotas de Stein}.
 
 En nuestro caso, nos vamos a enfocar en dos familias de funciones distintas. La primera de ellas es la familia $\mathscr{H}$ que origina la distancia de Kolmogorov, mientras que en el segundo caso consideramos una familia específica que nos será de utilidad en el capítulo 4.
 
En cuanto a la distancia de Kolmogorov, tenemos el siguiente teorema que nos da una cota universal explícita de las normas de las soluciones a la ecuación de Stein asociada a la familia $\mathscr{H}= \left\{h:\R\to \R \ :\ h(x)=\1_{(-\infty,z]}(x),\ \  z\in \R\right\}$.
  
 \begin{teo}\label{cotakolmogorovstein}
   Sea $z\in \R$ y consideremos a la función $h=\1_{(-\infty,z]}\in \mathscr{H}$ asociada. Denotemos por $f_z$ a la única solución de la ecuación de Stein asociada a la función $h$ anterior. Se tiene la siguiente cota.
  \[
      \norm{f_z}_{\infty}\leq \frac{\sqrt{2\pi}}{4} \qquad \text{y} \qquad \norm{f'_z}_\infty\leq 1.
  \]
En particular, para $N$ una variable aleatoria normal estándar, y para cualquier variable integrable $X$, se tiene que 
\[
   d_{\text{Kol}}(X,N)\leq \sup_{f\in \F_{Kol}}\abs{\E\left[f'(X)\right]-\E\left[Xf(X)\right]},
\]
en donde 
\[\F_{\text{Kol}}:=\left\{f:\R\to\R \text{ abs. cont.}: f \text{ derivable salvo un conjunto finito}, \norm{f}_\infty\leq \frac{\sqrt{2\pi}}{4}, \ \norm{f'}_\infty\leq 1\right\},\]
y donde el supremo anterior se entiende como la cantidad 
\[
\sup \abs{\E\left[g(X)\right]-\E\left[Xf(X)\right]},
\]
donde el supremo se toma sobre todos los pares de funciones borelianas $(f,g)$, con $f$ derivable salvo un conjunto finito de puntos, y $g$ es una versión de $f'$ tal que $\norm{g}_\infty\leq 1$.
\end{teo}

Una guía para la demostración de este resultado se puede encontrar en la sección 3.4 de \cite{Nourdin_Peccati_2012}. Concluimos esta sección con el siguiente resultado, el cual nos da cotas explícitas para una familia de funciones específicas.

\begin{teo}\label{metodosteinfamespecial}
 Consideremos la familia de funciones $\mathscr{H}=\left\{h:\R\to\R : h(z)=\1_{\{z>x\}}z, \ x\in \R\right\}$. Entonces la solución $f_h$ a la ecuación de Stein \eqref{eqstein} satisface 
 \[
 |f'_h(x)|\leq C(\abs{x}+1), \qquad \forall x\in \R,
 \]
 en donde $C$ es una constante.
 \end{teo}
Nuevamente este resultado se enuncia sin demostración. Una versión general del mismo, así como su demostración, se encuentran en el apéndice A de \cite{HU2014814}.

Los dos resultados anteriores se encuentran en el núcleo de las técnicas que se conocen en su conjunto como \textit{método de Stein.}, y que son amplimente usadas para deducir resultados similiares al teorema central del límite, además de deducir tasas de convergencia para los mismos. Ejemplo de lo anterior es el célebre teorema de Berry-Essen, el cual cuantifica la velocidad de convergencia de una sucesión i.i.d de variables con media cero, varianza 1 y tercer momento absoluto finito, a la distribución normal estándar. Es decir, es una versión cuantificable del clásico teorema del límite central.

Es posible otorgar una demostración del mismo utilizando el método de Stein, y más específicamente el teorema \ref{cotakolmogorovstein}. Para el enunciado y demostración del mismo, nos referimos a la sección 3.7 de \cite{Nourdin_Peccati_2012}.

\section{Existencia de densidades y estimación de la distancia uniforme}

En preparación para el capítulo 4, en esta sección estudiamos brevemente un par de resultados. El primero de ellos nos garantiza la existencia y unicidad de una variable aleatoria unidimensional bajo ciertas hipótesis, además de proveer una fórmula para la misma. También enunciamos un resultado que consiste en una cota para la distancia uniforme entre la densidad de una variable aleatoria y la densidad de una variable normal estándar, siendo este último un resultado medular en el capítulo 4. Es de destacar que estos resultados son consecuencias del teorema \ref{factordeltafuera} y ambos, por orden de aparición, se pueden encontrar en \cite[proposición 1]{Caballero1998-hz} y en \cite[teorema 3.2]{KUZGUN202268}.

Comenzamos directamente con el primer resultado. La demostración del mismo se omite en este texto.
\begin{teo}\label{existdensidad}
 Sean $F$ una variable aleatoria en el espacio $\D^{1,1}$ y $\mathfrak{H}$ un espacio de Hilbert. Supongamos que $u$ es una variable en $L^{1}(\Omega;\mathfrak{H})$ tal que $D_uF\neq 0$ casi seguramente. Supongamos también que $\tfrac{u}{D_uF}\in \text{Dom}(\delta)$. Entonces la ley de $F$ tiene una densidad continua y acotada dada por 
   \begin{equation}\label{formuladensidad}
      f_F(x)=\E\left[\1_{\{F>x\}}\delta \left(\frac{u}{D_uF}\right)\right].
   \end{equation}
 \end{teo}
Es de nuestro interés saber en qué caso es cierto que $\tfrac{u}{D_uF}\in \text{Dom}(\delta)$. El siguiente lema nos da una respuesta.

\begin{lema}\label{lemaexistdensidad}
 Sean $p,p'>1$ tales que $\frac{1}{p}+\frac{1}{p'}=1$, y sea $F\in \D^{2,1}$. Supongamos que $u\in \text{Dom}(\delta)$. Si se cumplen las siguientes condiciones 
 \begin{enumerate}
   \item $u\in L^{p}(\Omega;\mathfrak{H})$,
   \item $\delta(u)\in L^{p}(\Omega)$,
   \item $(D_uF)^{-1}\in L^{p'}(\Omega)$,
   \item $(D_uF)^{-2}\left(\norm{D^2F}_{\mathfrak{H}^{\otimes 2}}\norm{u}_{\mathfrak{H}}+\norm{Du}_{\mathfrak{H}}\norm{DF}_{\mathfrak{H}}\right)\in L^{p'}(\Omega)$,
 \end{enumerate}
 entonces $\tfrac{u}{D_uF}\in \text{Dom}(\delta)$ y en particular, se cumple la ecuación \eqref{eqfactordeltafuera}. 
 \end{lema}
 Para una prueba de este lema, se puede consultar \cite[lema 3, lema 4]{Caballero1998-hz}.

Estamos interesados en aplicar tanto el lema anterior como el teorema \ref{existdensidad} en condiciones que aparecerán en el capítulo 4. Por lo tanto, y siguiendo de cerca a \cite[sección 3]{KUZGUN202268}, tenemos el siguiente resultado. 

\begin{teo}\label{teocotafundamental}
 Sean $u\in \D^{1,6}(\Omega;\mathcal{\mathfrak{H}})$ y definamos la variable $F:=\delta(u)$. Supongamos que $F\in \D^{2,6}$, con $\E\left[F\right]=0, \ \E\left[F^{2}\right]=1$ y $\left(D_uF\right)^{-1}\in L^4(\Omega)$. Entonces $u/D_uF \in \text{Dom}(\delta)$, la variable $F$ admite una densidad continua y acotada $f_F(x)$ y la misma cumple la siguiente desigualdad
 \begin{equation}\label{cotafundamental}
    \sup_{x\in \R} \abs{f_F(x)-\Phi(x)}\leq \left(\|F\|_4\|\left(D_vF\right)^{-1}\|_4+2\right)\|1-D_vF\|_2+\|\left(D_vF\right)^{-1}\|^{2}_4\|D_v \left(D_vF\right)\|_2,
 \end{equation}
donde $\Phi(x)$ es la función de densidad de una variable normal estándar.

\end{teo}
\begin{proof} 
   Demostramos primero que $\frac{u}{D_uF}\in \text{Dom}(\delta)$. Buscamos satisfacer las hipótesis del lema \ref{lemaexistdensidad}. Para ello usamos $p=p'=2$. Claramente $p,p'>1$ y se cumple que $\frac{1}{p}+\frac{1}{p'}=1$. Por otro lado, por hipótesis $u\in \text{Dom}(\delta)$ y además, dado que en nuestro caso la variable $F$ es por definición la divergencia de $u$, de la hipótesis y de la contención entre los espacios $\D^{k,q}$ se sigue que $F\in \D^{2,6}\subseteq\D^{2,1}$. Resta mostrar que las cuatro condiciones del lema \ref{lemaexistdensidad} se satisfacen.
   \begin{enumerate}
      \item Dado que $u\in \D^{1,6}(\Omega;\mathfrak{H})\subseteq\D^{1,2}(\Omega;\mathfrak{H})$, y que por construcción este último espacio es un subconjunto de $L^{2}(\Omega;\mathfrak{H})$, se cumple la primera condición.
      \item Por hipótesis $\delta(u)=F\in \D^{2,6}$, y este es subconjunto de $L^{2}(\Omega)$, por lo que la segunda condición también se cumple.
      \item El enunciado nos indica que $(D_uF)^{-1}\in L^4(\Omega)$. Como la medida del espacio es finita, se da la contención $L^4(\Omega)\subseteq L^2(\Omega)$ y la tercera condición se satisface.
      \item Denotemos $a:=\left(\norm{D^2F}_{\mathfrak{H}^{\otimes 2}}\norm{u}_{\mathfrak{H}}+\norm{Du}_{\mathfrak{H}}\norm{DF}_{\mathfrak{H}}\right)$. Obsérvese que dichas cantidades están bien definidas gracias a las hipótesis. Notamos que
      \[
      \norm{a(D_uF)^{-2}}_2^2=\int_\Omega (a(D_uF)^{-2})^2d\P=a^2\int_\Omega ((D_uF)^{-1})^4d\P=a^2\norm{(D_uF)^{-1}}_4^{4}<\infty.
      \] 
      Concluimos que la última condición se satisface.   
   \end{enumerate}
   Por lo tanto, $u/D_uF\in \text{Dom}(\delta)$. Ahora bien, el resto de hipótesis del teorema \ref{existdensidad} se satisfacen, pues se tiene que $F\in \D^{2,6}\subseteq \D^{1,1}$. Además, $u\in \D^{1,6}\subseteq \D^{1,1}$ y este último es subconjunto de $L^{1}(\Omega;\mathfrak{H})$. Finalmente si fuera el caso que $D_uF=0$ en un conjunto medible de probabilidad positiva $A\subseteq \Omega$, tendríamos que 
   \[
   \norm{(D_uF)^{-1}}_1=\int_\Omega \abs{(D_uF)}^{-1}d\P\geq\int_A\abs{(D_uF)}^{-1}d\P=+\infty,
   \]
   lo cual es una contradicción ya que $(D_uF)^{-1}\in L^{4}(\Omega)$, que es subconjunto de $L^{1}(\Omega)$.

   Satisfechas las hipótesis del teorema \ref{existdensidad}, tenemos garantizada la existencia de una densidad continua y acotada para la variable $F=\delta(v)$, la cual está dada por la fórmula \eqref{formuladensidad}. Ahora buscamos una cota uniforme sobre $x\in \R$ para las siguientes cantidades
   \begin{equation}\label{eqdiferencia1}
   \abs{f_F(x)-\phi(x)}=\abs{\E\left[\1_{\{F>x\}}\delta\left(\frac{u}{D_uF}\right)\right]-\E\left[\1_{\{N>x\}}N\right]}, 
   \end{equation}
  
  donde $N\sim$ Normal$(0,1)$.
   En virtud de que se satisfacen las hipótesis del lema \ref{existdensidad}, en la fórmula \eqref{formuladensidad} podemos usar la ecuación \eqref{eqfactordeltafuera} y obtenemos que, para $x\in \R$ arbitrario,
   \begin{align*}
   f_F(x)=\E\left[\1_{\{F>x\}}\delta\left(\frac{u}{D_uF}\right)\right]&=\E\left[\1_{\{F>x\}}\left(\frac{1}{D_{u}F}\delta(u)-\Big\langle D \left(\frac{1}{D_{u}F}\right),u \Big\rangle \right)\right]\\
   &=\E\left[\1_{\{F>x\}}\frac{F}{D_{u}F}\right]+\E\left[\1_{\{F>x\}}\left(\frac{1}{\left(D_{u}F\right)^2}\right)D_{u}D_{u}F\right],
      \end{align*}
      donde hemos hecho uso de que $F=\delta(u)$ y de la regla de la cadena en la segunda igualdad.
  Incrustando esta última igualdad en la fórmula \eqref{eqdiferencia1}, y subsecuentemente sumando y restando el término $\E\left[\1_{\{F>x\}}F\right]$,
  
\begin{align}\label{eqdiferencia2}
   \abs{f_F(x)-\Phi(x)}&=\abs{\E\left[\1_{\{F>x\}}\frac{F}{D_{u}F}\right]+\E\left[\1_{\{F>x\}}\frac{D_uD_uF}{\left(D_{u}F\right)^2}\right]-\E\left[\1_{\{N>x\}}N\right]}\notag\\
   &=+\Bigg|\E\left[\1_{\{F>x\}}\frac{F}{D_{u}F}\right]-\E\left[\1_{\{F>x\}}F\right]+\E\left[\1_{\{F>x\}}\frac{D_uD_uF}{\left(D_{u}F\right)^2}\right]\notag\\
   &\ \ \ +\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]\Bigg|\notag\\
   &\leq \abs{\E\left[\1_{\{F>x\}}\left(\frac{F}{D_uF}-F\frac{D_uF}{D_uF}\right)\right]}+\abs{\E\left[\1_{\{F>x\}}\frac{D_uD_uF}{\left(D_{u}F\right)^2}\right]}\notag\\
   &\ \ \ +\abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}\notag\\
   &\leq \E\left[\abs{\frac{F(1-D_uF)}{D_uF}}\right]+\E\left[\frac{\abs{D_uD_uF}}{(D_uF)^2}\right]+\abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}.
\end{align}
Ahora, en la desigualdad \eqref{eqdiferencia2} usamos sucesivamente la desigualdad de Hölder en el primer y segundo términos, de manera que
\begin{equation}\label{eqdiferencia3}
   \E\left[\abs{\frac{F(1-D_uF)}{D_uF}}\right]\leq \norm{F(D_uF)}_2\norm{1-D_uF}_2\leq \norm{F}_4\norm{(D_vF)^{-1}}_4\norm{1-D_uF}_2,
\end{equation}
y también
\begin{equation}\label{eqdiferencia4}
   \E\left[\frac{\abs{D_uD_uF}}{(D_uF)^2}\right]\leq \norm{D_uD_uF}_2\norm{(D_uF)^{-2}}_2=\norm{D_uD_uF}_2\norm{(D_uF)^{-1}}_4^{2}.
\end{equation}
Finalmente, estudiamos el tercer término de la desigualdad \eqref{eqdiferencia2}. Aquí es donde utilizamos el <<método de Stein>>. Observemos que, para $x\in \R$,
\[
   \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}=\abs{\E\left[h(F)\right]-\E\left[h(N)\right]},
\]
donde $h(z)=\1_{\{z>x\}}z$ es una función perteneciente a la familia $\mathscr{H}$ correspondiente al teorema \ref{metodosteinfamespecial}. En virtud de la ecuación \eqref{eqmetodostein}, la igualdad anterior se convierte en
\begin{equation}\label{eqdiferencia5}
   \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}=\abs{\E\left[f'_h(F)\right]-\E\left[Ff_h(F)\right]}.
\end{equation}
En este punto usamos nuevamente que $F=\delta(u)$, y como tal, cumple la fórmula de integración por partes. Por lo tanto, tomando como variable a $f_h(F)$ y usando regla de la cadena se tienen las siguientes igualdades
\begin{equation*}
   \E\left[Ff_h(F)\right]=\E\left[\delta(u)f_h(F)\right]=\E\left[\langle D(f_h(F)),u\rangle\right]=\E\left[f'_h(F)\langle DF,u\rangle\right]=\E\left[f'_h(F)D_uF\right].
\end{equation*}
Insertando la igualdad anterior en \eqref{eqdiferencia5} se sigue que 
   \begin{equation*}
      \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}=\abs{\E\left[f'_h(F)\right]-\E\left[f'_h(F)D_uF\right]}= \abs{\E\left[f_h'(F)(1-D_vF)\right]}.
   \end{equation*}
Usamos nuevamente la desigualdad de Hölder en el último termino de la ecuación anterior, obtenemos
\begin{equation}\label{eqdiferencia6}
   \abs{\E\left[f_h'(F)(1-D_vF)\right]}\leq \norm{f_h'(F)}_2\norm{1-D_vF}_2.
\end{equation}
Finalmente, usamos el teorema \ref{metodosteinfamespecial}, en \ref{eqdiferencia6}, por lo que 
\begin{equation}\label{eqdiferencia7}
      \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}\leq C(\abs{F}+1)\norm{1-D_vF}_2
\end{equation}
Finalmente, tenemos que $|F|\leq 1$ y $C\leq 1$, por lo que \ref{eqdiferencia7} se convierte en 
\begin{equation}\label{eqdiferencia8}
   \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}\leq2\norm{1-D_vF}.
\end{equation}
Combinando las desigualdades \ref{eqdiferencia3}, \ref{eqdiferencia4} y \ref{eqdiferencia7}, tenemos para $x\in \R$,
\begin{align*}
   \abs{f_F(x)-\Phi(x)}&\leq \norm{F}_4\norm{(D_vF)^{-1}}_4\norm{1-D_uF}_2+\norm{D_uD_uF}_2\norm{(D_uF)^{-1}}_4^{2}+2\norm{1-D_vF}\\
   &=\left(\|F\|_4\|\left(D_vF\right)^{-1}\|_4+2\right)\|1-D_vF\|_2+\|\left(D_vF\right)^{-1}\|_4^{2}\|D_v \left(D_vF\right)\|_2.   
\end{align*}

Tomando supremo obtenemos la desigualdad \eqref{cotafundamental}.
\end{proof}

Tal y como se comentó al inicio de la sección, el resultado anterior resulta ser la cota principal desde la cual se deducirá la tasa de convergencia de las densidades de los promedios espaciales de la solución a la SHE, a la densidad normal. 

Hasta aquí llega nuestro estudio del método de Stein y de la existencia de la densidad de una variable aleatoria bajo ciertas condiciones de la misma. Es de destacar que el estudio realizado es solamente una pequeñísima parte de la vanguardia de ambos temas. Por un lado las ecuaciones de Stein unidimensionales han sido ampliamente estudiadas, de acuerdo con \cite{Nourdin_Peccati_2012}. Por otro lado, el estudio de existencia de densidades también se encuentra bastante avanzado hoy en día.

En lo que a este texto respecta, la herramienta necesaria para abordar el problema planteado al inicio ha sido cubierta en su totalidad, por lo que en lo que resta del texto, nos enfocaremos en el mismo.



\chapter{Propiedades espaciales de la Ecuación estocástica del calor}
En este capítulo presentamos en su plenitud el problema planteado al inicio de este texto, el cual consiste en estudiar la convergencia de las densidades de los promedios espaciales de la solución a la SHE, a la densidad de una variable aleatoria normal estándar $\Phi$, usando la distancia uniforme en $\R$.

Ahora que contamos con las herramientas necesarias para abordar nuestro problema, procedemos a enunciarlo de manera rigurosa. Este capítulo está completamente basado en \cite[pp. 68-71, 75-86.]{KUZGUN202268}. Consideremos el siguiente problema del calor.
\begin{equation}\label{shedefinitiva}
   \begin{cases}
      \partial_t u=\frac{1}{2}\partial_{xx}u+\sigma(u)\dot{W} & \qquad \forall x\in \R, \ \forall t>0,\\
      u_0(x):=u(x,0)=1 & \qquad \forall x\in \R,
   \end{cases}
\end{equation}
donde $\dot{W}$ es un ruido blanco en $[0,\infty)\times\R$, esto es, tanto en el espacio como en el tiempo, y $\sigma$ es una función Lipschitz tal que $\sigma(1)\neq 0$.

Nos referiremos a este como nuestro problema principal, y siempre que hagamos referencia a él, entenderemos que las hipótesis sobre $u_0$ y $\sigma$ anteriormente mencionadas se satisfacen.

Como convención, a lo largo de este capítulo escribiremos a la densidad de una variable gaussiana de varianza $t>0$ como
\[
p_t(x):=\frac{1}{\sqrt{2\pi t}}e^{-x^2/2t}, \qquad \forall x\in \R.
\]
% Tal y como hemos mencionado, la existencia y la unicidad de la solución mild a la ecuación estocástica del calor ha sido ya demostrada desde la década de los ochentas en el caso en que la condición inicial $u_0$ es una función acotada y la función $\sigma$ es Lipschitz continua (ver, por ejemplo, \cite{Walsh_J.B_Introduction_to_SPDEs}). 

Del capítulo 1 tenemos garantizada la existencia y unicidad de la solución en el sentido mild, $u=\left\{u(t,x):(t,x)\in [0,\infty)\times\R\right\}$ al problema anterior. Concretamente, podemos extraer el siguiente teorema, el cual es enunciado en concordancia con \cite[proposición 2.1]{KUZGUN202268}.

\begin{teo}\label{solucionmild}
 Consideremos el problema de calor \eqref{shedefinitiva}. Entonces existe un único campo aleatorio $u=\{u(t,x): (t,x)\in [0,\infty)\times \R\}$ medible y adaptado, tal que para cualquier $T>0$ y $p\geq2$, 
 \begin{equation}\label{cotasolucionmild}
   \sup_{(t,x)\in [0,T]\times\R}\E\left[\abs{u(t,x)}^p\right]=C_{T,p},
 \end{equation}
 
 y para cualquier $t\geq0$ y $x\in \R$, $u$ satisface
 \begin{equation}\label{eqsolucionmild}
   u(t,x)=1+\int_{[0,t]\times \R}^{}p_{t-s}(x-y)\sigma(u(s,y))W(ds,dy).
 \end{equation}
 \end{teo}
 \begin{proof} 
    pendiente
  \end{proof}
Tal y como mencionamos en el capítulo 1, diremos que $u$ es la \textit{solución mild} al problema \eqref{shedefinitiva}. Con dicha solución al alcance, la cual es de naturaleza estocástica, nos interesamos ahora por en el comportamiento asintótico de un nuevo objeto construido a partir de la misma: los promedios espaciales. Estos objetos, en nuestro contexto, se definen a continuación. 

\begin{dfn}\label{defpromediosespaciales}
 Sea $u=\left\{u(t,x):(t,x)\in [0,\infty)\times\R\right\}$ la solución mild al problema \eqref{shedefinitiva}. Para $R>0$ fijo, definimos los promedios espaciales centrados y normalizados de $u$ como 
 \begin{equation*}
   F_{R,t}:=\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}u(t,x)dx -2R\right), \qquad \text{donde} \ \ \sigma^2_{R,t}:=\text{Var}\left(\int_{[-R,R]}u(t,x)dx\right).
\end{equation*}
 \end{dfn}
Nos referiremos a las variables $F_{R,t}$ simplemente como los promedios espaciales de la solución $u$, entendiéndolos siempre en el sentido de la definición anterior. Tal y como se mencionó al inicio, en \cite{HUANG20207170}, para un $t>0$ fijo, Huang, Nualart y Viitasaari estudian el comportamiento límite de las variables $F_{R,t}$ conforme $R$ tiende a infinito, siendo capaces de probar resultados tipo teorema central del límite para los promedios espaciales. Más aún, en dicha publicación, utilizando método de Stein junto con cálculo de Malliavin, se obtuvieron cotas superiores para la tasa de convergencia de $F_{R,t}$ a la distribución normal estándar en distancia de variación total. De manera precisa, obtuvieron el siguiente resultado

\begin{teo} 
 Sean $u$ la solución mild al problema \eqref{shedefinitiva} y $Z\sim$ Normal(0,1). Supongamos que $\sigma_{R,t}>0$. Entonces existe una constante $C>0$ que depende solamente de $t$ tal que 
 \begin{equation}
    d_{\text{TV}}(F_{R,t},N)\leq \frac{C}{\sqrt{R}}, \qquad \text{para }t>0 \ \text{ y } \ R\geq1.  
 \end{equation}
 \end{teo}
En particular, los autores de \cite{HUANG20207170} resaltan que la condición $\sigma(1)\neq 0$ que aparece en el problema \eqref{shedefinitiva} implica que $\sigma_{R,t}>0$.

Con este trabajo como antecedente, el propósito final del capítulo es obtener cotas superiores pero ahora para la tasa de convergencia de la distancia uniforme entre las densidades de los promedios espaciales y la densidad de una variable aleatoria normal estándar. Concretamente, el resultado principal que se busca probar en este capítulo es el siguiente.
\begin{teo}\label{teoremaprincipal}
Sea $u=\left\{u(t,x):(t,x)\in [0,\infty)\times\R\right\}$ la solución mild al problema \eqref{shedefinitiva}. Supongamos que $\sigma:\R\to\R$, además de las hipótesis propias del problema del calor, cumple las siguientes tres condiciones.
\begin{itemize}
   \item $\sigma\in C^2(\R)$.
   \item $\sigma'$ es acotada.
   \item $\abs{\sigma''(x)}\leq C(1+\abs{x}^{m})$, \qquad para cualquier $x\in \R$ real y para alguna $m>0$.
\end{itemize}
Supongamos también que, para algún $q>10$, se cumple que 
\[
\E\left[\abs{\sigma(u(t,0))}^{-q}\right]<\infty.    
\]
Entonces para $t>0$ fijo, se tiene que para cualquier $R>0$, los promedios espaciales $F_{R,t}$ poseen una densidad continua y acotada $f_{F_{R,t}}$, y la distancia entre las densidades de los promedios espaciales y la densidad de la distribución normal satisface la siguiente cota.  
\[
\sup_{x\in \R} \abs{f_{F_{R,t}}(x)-\Phi(x)}\leq \frac{C_t}{\sqrt{R}},   
\]
donde $\Phi$ es la densidad de una variable aleatoria normal estándar y $C_t>0$ es una constante dependiente de $t$.
\end{teo}

Este capítulo, dedicado a la prueba de este último teorema, consiste en dos secciones. La primera la dedicamos a mostrar dos resultados clave para la prueba. El primero de ellos tiene que ver con una expresión para la segunda derivada de Malliavin de la solución al problema del calor $\eqref{shedefinitiva}$, mientras que el segundo tiene que ver con la existencia de los momentos negativos. La segunda sección consiste en el cuerpo de la prueba como tal, y al inicio de aquella sección se describe como está estructurada la misma.

\section{Cotas para la primera y segunda derivada de la solución mild y existencia de momentos negativos}

Tal y como se mencionó, estudiamos primero el comportamiento de la segunda derivada de la solución mild $u$ a la ecuación del calor \eqref{shedefinitiva}. Cabe mencionar que para lograrlo, debemos basarnos en un resultado análogo para la primera derivada de la solución. Esto debido a que a partir éste resultado es que se llega a aquél, además de ser necesario para la prueba del primero. El resultado para la primera derivada lo enunciamos a continuación, tal y como viene en \cite[proposición 5.1]{Nualart2007}, aunque adaptado a la notación de este texto, y directamente suponiendo que el término de deriva $b$ en \cite{Nualart2007} es cero.

\begin{teo}\label{teocota1eraderivada}
   Sea $u$ la solución al problema \eqref{shedefinitiva}. Supongamos además que $\sigma:\R\to\R$ es de clase $C^1(\R,\R)$, y su derivada es una función Lipschitz y acotada. Entonces para cualquier $(t,x)\in [0,T]\times\R$, $u(t,x)\in\bigcap_{p\geq1}\D^{1,p}$ y además, la derivada de Malliavin cumple que 
   \begin{equation}\label{formula1eraderivada}
      D_{s,y}u(t,x)=p_{t-s}(x-y)\sigma(u(s,y))+ \int_{[s,t]\times\R} p_{t-\tau}(x-\xi)\sigma'(u(\tau,\xi))D_{s,y}u(\tau,\xi)W(d\tau,d\xi),
   \end{equation}
   
   para casi cualquier $s\in [0,t]$ y $y\in \R$. Más aún, se cumple que 
   \begin{equation}\label{cota1eraderivada} 
      \norm{D_{s,y}u(t,x)}_{p}\leq C_{T,p \ }p_{t-s}(x-y),
   \end{equation}
   para cualquier $0\leq s<t\leq T$, y para $x,y\in \R$.
 \end{teo}
Con el resultado anterior a la mano, procedemos a enunciar lo respectivo para la segunda derivada de $u$, tal y como se enuncia en \cite{KUZGUN202268}. Es destacable que, de acuerdo con los autores del artículo, la cota de la segunda derivada de la solución mild resuelve un problema abierto en el cálculo de Malliavin y tiene su interés en sí.
 \begin{teo}\label{teocota2daderivada}
   Sea $u$ la solución al problema \eqref{shedefinitiva}. Supongamos que $\sigma$ cumple las hipótesis del teorema \ref{teoremaprincipal}. Fijando $(t,x)\in [0,\infty)\times \R$, tenemos que $u(t,x)\in \bigcap_{p\geq2}\D^{2,p}$ y para casi todo $0<r<s<t$, $y,z\in \R$, la segunda derivada de $u$, que denotamos como $D_{r,z}D_{s,y}u(t,x)$, satisface la siguiente ecuación diferencial estocástica lineal:
   \begin{align}
      D_{r,z}D_{s,y}u(t,x)&=p_{t-s}(x-y)\sigma'(u(s,y))D_{r,z}u(s,y)\notag\\
      &+\int_{[s,t]\times \R}p_{t-\tau}(x-\xi)\sigma''(u(\tau,\xi))D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\notag\\
      &+\int_{[s,t]\times \R}p_{t-\tau}(x-\xi)\sigma'(u(\tau,\xi))D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\label{formula2daderivada}
   \end{align}
   Más aún, para cualesquiera $0\leq r<s<t\leq T$  y $x,y,z\in \R$, tenemos que 
   \begin{equation}\label{cota2daderivada}
         \|D_{r,z}D_{s,y}u(t,x)\|_p\leq C_{T,p \ }\Phi_{r,z,s,y}(t,x),
   \end{equation}
   donde $C_{T,p}$ es una constante que depende de $T,p$ y $\sigma$, mientras que
      \begin{align*}
          \Phi_{r,z,s,y}(t,x):=&p_{t-s}(x-y)\left(p_{s-r}(y-z)+\frac{p_{t-r}(z-y)+p_{t-r}(z-x)+\1_{\{\abs{y-x}>\abs{z-y}\}}}{(s-r)^{1/4}}\right).
      \end{align*} 
   \end{teo}
   Por ser de carácter fundamentalmente técnico, la demostración de este resultado se omite en esta tesis. No obstante, mencionamos que la idea de su prueba consiste en iteraciones de Picard, la desigualdad de Burkholder-Davis-Gundy, y lemas técnicos que se encuentran en \cite[lema A.1]{HUANG20207170} y \cite[lema A.1, lema A.2]{KUZGUN202268}. Es de destacar que las hipótesis del teorema anterior implican las hipótesis del teorema \ref{teocota1eraderivada}.

   Tornamos nuestra atención a la existencia de los momentos negativos de la derivada de la solución $u$ al problema del calor \eqref{shedefinitiva}.
\begin{teo}\label{teocotamomentosinversos}
   Sea $u$ la solución mild a la ecuación estocástica del calor. Supongamos que $\sigma$ es Lipschitz. Sea $p\geq2$ fijo, $t>0$ y supongamos que existe un $q>5p$ tal que $\E\left[\abs{\sigma(u(t,0))}^{-2q}\right]<\infty$. Entonces existe una constante $R_0>0$ tal que 
   \begin{equation}\label{cotamomentosinversos}
      \sup_{R\geq R_0}\E\left[\abs{D_{v_{R,t}}F_{R,t}}^{-p}\right]<\infty.
   \end{equation}
   
   \end{teo}
   La prueba de este resultado se omite en este texto, por lo que se remite a $\cite{KUZGUN202268}$ para su consulta. En la prueba del mismo se utilizan distintos lemas presentes en el apéndice del artículo citado.

   Culminada la revisión de estos resultados necesarios para la prueba del resultado principal, pasamos a realizar la misma.
\section{Prueba del resultado principal}
Con intención de desmenuzar con suficiente claridad la prueba, describimos brevemente su estructura.

La prueba parte de la cota \eqref{cotafundamental} aplicada a los promedios espaciales $F_{R,t}$. Es por lo tanto necesario verificar que dichas variables cumplen las hipótesis del teorema \ref{teocotafundamental}. Asimismo, serán usadas constantemente las desigualdades \eqref{cota1eraderivada}, \eqref{cota2daderivada} y \eqref{cotamomentosinversos}, por lo que debemos verificar que las hipótesis de los teoremas \ref{teocota1eraderivada}, \ref{teocota2daderivada} y \ref{teocotamomentosinversos} se satisfacen. Habilitados para usar dichos resultados, de la cota \eqref{cotafundamental} nos surgen dos sumandos. El primero de ellos se puede acotar con relativa rapidez apoyándonos de las cotas anteriores y usando la prueba de \cite[teorema 1.1]{HUANG20207170}, por lo que el grueso de la prueba consiste en acotar la norma de la segunda derivada de los promedios espaciales. Allí se descompondrá dicha derivada de segundo orden en dos términos integrales y se procede a acotar cada término por separado utilizando las propiedades de la integral de Itô-Walsh, las cotas mencionadas antes y algunos resultados técnicos. 

Por lo anterior, la prueba está dividida en tres partes. La primera de ellas dedicada a verificar la validez de las hipótesis de los resultados a usar, la segunda de ellas está dirigida a acotar el primero de los sumandos y la última parte se dedica a acotar la norma de la segunda derivada de los promedios espaciales. Dicho todo lo anterior, comenzamos la prueba del resultado principal.
\begin{proof}[Demostración (del teorema \ref{teoremaprincipal})]

\begin{proofpart}[Validez de las hipótesis para las cotas]
Sean $R$ y $t>0$ fijos. Consideremos a los promedios espaciales $F_{R,t}$ de la solución mild $u$ al problema \eqref{shedefinitiva}. Buscamos primero ver que $F_{R,t}$ satisface las hipótesis del teorema \ref{teocotafundamental}. Para ello, debemos ver que dicha variable puede verse como la divergencia de algún elemento $v\in \D^{1,6}(\mathfrak{H})$. Obsérvese que, de la definición de los promedios espaciales y de la relación en el teorema \ref{solucionmild}, tenemos las siguientes igualdades.
\begin{align}
   F_{R,t}&=\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}u(t,x)dx-2R\right)\notag\\
   &=\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}\left(1+\int_{[0,t]\times \R}p_{t-s}(x-y)\sigma(u(s,y))W(ds,dy)\right)dx-2R\right)\notag\\
   &=\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}\int_{[0,t]\times \R}p_{t-s}(x-y)\sigma(u(s,y))W(ds,dy)\right)dx\notag\\
   &=\int_{[0,t]\times \R}\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}p_{t-s}(x-y)\sigma(u(s,y))dx\right)W(ds,dy)\notag\\
   &=\int_{[0,\infty)\times \R}\1_{[0,t]}(s)\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}p_{t-s}(x-y)\sigma(u(s,y))dx\right)W(ds,dy)\label{promespacdiv1},
\end{align}
en donde en la penúltima igualdad hemos hecho uso del teorema de Fubini. Por otro lado, sabemos que la integral de Itô-Walsh coincide con el operador de divergencia $\delta$ en dos dimensiones, así que definiendo $v_{R,t}$ como 

\begin{equation}\label{vectordireccion}
      v_{R,t}:=\1_{[0,t]}(s)\frac{1}{\sigma_{R,t}}\int_{[-R,R]}p_{t-s}(x-y)\sigma(u(s,y))dx,
\end{equation}
 la relación \eqref{promespacdiv1} se convierte en
\begin{equation}\label{promespacialdiv2}
F_{R,t}=\int_{[0,\infty)\times \R}\1_{[0,t]}(s)\frac{1}{\sigma_{R,t}}\left(\int_{[-R,R]}p_{t-s}(x-y)\sigma(u(s,y))dx\right)W(ds,dy)=\delta(v_{R,t}).
\end{equation}
Por lo tanto, los promedios espaciales se pueden ver como el operador divergencia aplicado a $v_{R,t}$. Observamos ahora que de las tres hipótesis sobre $\sigma$ que tenemos, estamos en condiciones de aplicar el teorema $\ref{cota2daderivada}$, por lo que para $(t,x)\in [0,\infty)\times\R$ fijo, $u(t,x)\in \bigcap_{p\geq2} \D^{2,p}$. Además, para cualquier $T>0$ y $p\geq2$, se tiene que 
\[
   \sup_{(t,x)\in [0,\infty)\times\R}\norm{u(t,x)}_{2,p}<\infty.
\]
Esto nos dice que para un $t>0$ fijo, $F_{R,t}\in \D^{2,6}$ y $v_{R,t}\in \D^{1,2}(\mathfrak{H})$. 

Por otro lado, por hipótesis existe algún $q>10$ tal que $\E\left[\abs{\sigma(u(t,0))}^{-q}\right]<\infty$, de tal forma que eligiendo $p=4$ en el teorema \ref{teocotamomentosinversos}, tenemos que para $q':=2q>2(10)=5(4)=5p$, al ser el caso que $2q'>q$ y ser $\Omega$ un espacio de medida finita, 
\[
\E\left[\abs{\sigma(u(t,0))}^{-2q'}\right]=\E\left[\abs{\frac{1}{\sigma(u(t,0))}}^{2q'}\right]\leq \E\left[\abs{\frac{1}{\sigma(u(t,0))}}^{q}\right]=\E\left[\abs{\sigma(u(t,0))}^{-q}\right]<\infty,
\]
por lo que las hipótesis del teorema $\ref{teocotamomentosinversos}$ se satisfacen para $p=4$ y por lo tanto, para $t>0$ fijo, existe una constante $R_0$ tal que la cota de los momentos inversos \eqref{cotamomentosinversos} es válida. En particular, para $t>0$ fijo y $R>R_0$ se sigue que $D_{v_R,t}F_{R,t}\in L^{4}(\Omega)$. Finalmente, por definición estamos tomando los promedios espaciales como variables centradas y con varianza 1. 

Hemos mostrado que para $R>R_0$ y $t>0$, las hipótesis del teorema \ref{teocotafundamental} se cumplen y por lo tanto, la variable $F_{R,t}/D_{v_R,t}F_{R,t}\in \text{Dom}(\delta)$, los promedios espaciales tienen una densidad continua y acotada, que denotamos como $f_{F_{R,t}}$, y la misma cumple  $\eqref{cotafundamental}$. Con precisión, sustituyendo en dicha cota obtenemos que 
\begin{align}\label{cotapromespacfundamental}
   \sup_{x\in \R} \abs{f_{F_{R,t}}(x)-\Phi(x)}&\leq \left(\|F_{R,t}\|_4\|\left(D_{v_{R,t}}F_{R,t}\right)^{-1}\|_4+2\right)\|1-D_{v_{R,t}}F_{R,t}\|_2\notag\\
   &\ \ \ +\|\left(D_{v_{R,t}}F_{R,t}\right)^{-1}\|^{2}_4\|D_{v_{R,t}} \left(D_{v_{R,t}}F_{R,t}\right)\|_2,
\end{align}
Observamos dos sumandos en la desigualdad anterior. Mostramos a continuación que cada uno de estos sumandos está acotada por una cantidad proporcional a $1/\sqrt{R}$.
\end{proofpart}
\begin{proofpart}[Cota del primer sumando]
Notemos primero que, de la relación \eqref{cotapromespacfundamental}, al ser $F_{R,t}\in \D^{2,6}$, y darse las contenciones $\D^{2,6}\subseteq \D^{2,4}\subseteq{L^{4}}(\Omega)$, se sigue que $\norm{F_{R,t}}_4<\infty$. Por otro lado, para $t>0$ y $R>R_0$ fijos, de \eqref{cotamomentosinversos} se sigue que 
\begin{equation}\label{cotapromespacmomentosinversos}
   \norm{(D_{v_{R,t}}F_{R,t})^{-1}}_4=\E\left[\abs{\left(D_{v_{R,t}}F_{R,t}\right)^{-1}}^{4}\right]^{1/4}=\E\left[\abs{D_{v_{R,t}}F_{R,t}}^{-4}\right]^{1/4}<\infty.
\end{equation}

Definiendo $C_{t}^{(1)}:=\left(\|F_{R,t}\|_4\|\left(D_{v_{R,t}}F_{R,t}\right)^{-1}\|_4+2\right)$, de lo anterior tenemos que $0<C_{t}^{(1)}<\infty$, y dicha constante depende solamente de $t>0$ fijo, para $R>R_0$. Resta estudiar el término $\norm{1-D_{v_{R,t}F_{R,t}}}_2$. Esta parte de la prueba está basada en la prueba de \cite[teorema 1.1]{HUANG20207170}. Dado que $F_{R,t}$ es una variable centrada y de varianza 1, se sigue que 
\begin{align*}
   \norm{1-D_{v_{R,t}}F_{R,t}}_2&=\E\left[(\E\left[F_{R,t}^2\right]-D_{v_{R,t}}F_{R,t})^2\right]^{1/2}\\
   &=\E\left[(\E\left[F_{R,t}\delta(v_{R,t})\right]-D_{v_{R,t}}F_{R,t})^2\right]^{1/2}\\
   &=\E\left[(\E\left[D_{v_{R,t}}F_{R,t}\right]-D_{v_{R,t}}F_{R,t})^2\right]^{1/2}\\
   &=\sqrt{\text{Var}\left(D_{v_{R,t}}F_{R,t}\right)},
\end{align*}
donde hemos hecho uso de la definición de $\delta$. Llegados a este punto, remitimos a la prueba del teorema 1.1 en el artículo \cite{HUANG20207170}, en donde el grueso de la prueba consiste en acotar $\sqrt{\text{Var}\left(D_{v_{R,t}}F_{R,t}\right)}$ por $C/\sqrt{R}$, donde $C$ es una constante que depende de $t>0$ y de $p$, que en nuestro caso está fijado en $p=4$. Por lo tanto, definiendo $C_{t}^{(2)}:=C_{t}^{(1)}\cdot C$, para $t>0$ y $R\geq R_0$,
\begin{equation}\label{cotaprimersumando}
   \left(\|F_{R,t}\|_4\|\left(D_{v_{R,t}}F_{R,t}\right)^{-1}\|_4+2\right)\|1-D_{v_{R,t}}F_{R,t}\|_2\leq \frac{C_{t}^{(2)}}{\sqrt{R}},
\end{equation}
donde $C_{t}^{(2)}$ depende solamente de $t>0$.
\end{proofpart}
\begin{proofpart}[Cota del segundo sumando]
Directamente notamos que el término $\norm{(D_{v_{R,t}}F_{R,t})^{-1}}^2_4$ del segundo sumando está acotado, según vimos en \eqref{cotapromespacmomentosinversos}, por lo que nos enfocamos en acotar el término $\norm{D_{v_{R,t}}(D_{v_{R,t}}F_{R,t})}_2$. 
Para tal efecto, establecemos la siguiente notación que nos será útil. Definimos, para $t>0$ y $R>0$, 
\[
Q_R:=[-R,R], \qquad \Sigma_{t,x}:=\sigma(u(t,x)), \qquad \Sigma_{t,x}^{(1)}:=\sigma'(u(t,x)) \qquad \text{ y } \qquad \Sigma_{t,x}^{(2)}:=\sigma''(u(t,x)),
\]
y además, para $0<s<t$, y $y\in \R$, definimos  
\begin{equation}\label{defnotacionphi}
   \phi_{R,t}(s,y):=\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-s}(x-y)dx.
\end{equation}

Armados con la notación anterior, directamente de la definición de los promedios espaciales tenemos que la derivada de Malliavin de los mismos está dada por 

\[
   D_{r,z}F_{R,t}=D_{r,z}\left(\frac{1}{\sigma_{R,t}}\int_{-[R,R]}u(t,x)dx-2R\right)=\frac{1}{\sigma_{R,t}}\int_{Q_R}D_{r,z}u(t,x)dx,
\]
por lo que tomando la derivada en dirección de $v_{R,t}$ definido en \eqref{vectordireccion} y sustituyendo, obtenemos 
\begin{align*}
   D_{v_{R,t}}&F_{R,t}=\langle D_{r,z}F_{R,t},v_{R,t}\rangle_{L^{2}([0,\infty)\times\R)}\\
   &=\int_{[0,\infty)}\int_\R v_{R,t}D_{r,z}F_{R,t}dzdr\\
   &=\int_{[0,\infty)}\int_\R\left(\1_{[0,t]}(r)\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)\sigma(u(r,z))dx_1\right)\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}D_{r,z}u(t,x_2)dx_2\right) dzdr\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)dx_1\right)\sigma(u(r,z))D_{r,z}u(t,x_2) dx_2dzdr\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(r,z)\sigma(u(r,z))D_{r,z}u(t,x_2) dx_2dzdr\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma(u(s,y))D_{s,y}u(t,x) dxdyds,
\end{align*}
   en donde en la última igualdad sólo hemos hecho un cambio de variables. Tomamos nuevamente la derivada de Malliavin de la expresión anterior para obtener 
\begin{align*}
   D_{r,z}(D_{v_{R,t}}F_{R,t})&=\frac{1}{\sigma_{R,t}}D_{r,z}\left(\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma(u(s,y))D_{s,y}u(t,x) dxdyds\right)\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)D_{r,z}\left(\sigma(u(s,y))D_{s,y}u(t,x)\right) dxdyds\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma'(u(s,y))D_{r,z}u(s,y)D_{s,y}u(t,x)dxdyds\\
   & \ \ \ +\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma(u(s,y))D_{r,z}D_{s,y}u(t,x)dxdyds
\end{align*}
Sacando nuevamente la derivada direccional, la expresión anterior nos lleva a las siguientes igualdades.
\begin{align}
   D_{v_{R,t}}&(D_{v_{R,t}}F_{R,t})=\langle D_{r,z}\left(D_{v_{R,t}F_{R,t}}\right),v_{R,t}\rangle_{L^{2}([0,\infty)\times\R)}\notag\\
   &=\int_{[0,\infty)}\int_\R \left(\1_{[0,t]}(r)\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)\sigma(u(r,z))dx_1\right)\notag\\
   &\ \ \ \ \cdot \left(\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma'(u(s,y))D_{r,z}u(s,y)D_{s,y}u(t,x)dxdyds\right)dzdr\notag\\
   &\ \ \ +\int_{[0,\infty)}\int_\R \left(\1_{[0,t]}(r)\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)\sigma(u(r,z))dx_1\right)\notag\\
   &\ \ \ \ \cdot\left(\frac{1}{\sigma_{R,t}}\int_{[0,t]}\int_\R \int_{Q_R}\phi_{R,t}(s,y)\sigma(u(s,y))D_{r,z}D_{s,y}u(t,x)dxdyds\right)dzdr\notag\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)dx_1\right)\notag\\
   &\ \ \ \ \cdot \left(\Sigma_{r,z}\phi_{R,t}(s,y)\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)D_{s,y}u(t,x)\right) dxdydzdsdr\notag\\
   &\ \ \ +\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R} \left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-r}(x_1-z)dx_1\right)\notag\\
   &\ \ \ \ \cdot\left(\Sigma_{r,z}\phi_{R,t}(s,y)\Sigma_{s,y}D_{r,z}D_{s,y}u(t,x)\right)dxdydzdsdr\notag\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)D_{s,y}u(t,x)dxdydzdsdr\label{pruebasumando1}\\ 
   &\ \ \ +\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}D_{r,z}D_{s,y}u(t,x)dxdydzdsdr,\label{pruebasumando2}
\end{align}
   donde hemos hecho uso múltiples veces del teorema de Fubini y de la notación previamente establecida. A continuación, utilizamos las fórmulas \eqref{formula1eraderivada} y \eqref{formula2daderivada} en los dos sumandos de la última igualdad anterior. Así, para el sumando \eqref{pruebasumando1}, sustitiuyendo la derivada $D_{s,y}u(t,x)$ se tiene la expresión
   \begin{align}
      \frac{1}{\sigma_{R,t}}&\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)D_{s,y}u(t,x)dxdydzdsdr\notag\\
      &=\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\notag\\
      & \ \ \ \ \cdot\left(p_{t-s}(x-y)\Sigma_{s,y}+ \int_{[s,t]\times\R} p_{t-\tau}(x-\xi)\Sigma^{(1)}_{\tau,\xi}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dxdydzdsdr\notag\\
      &=\int_{[0,t]^2}\int_{\R^2}\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-s}(x-y)dx\right)\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dydzdsdr\notag\\
      & \ \ \ + \int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\notag\\
      &\ \ \ \ \cdot\left(\int_{[s,t]\times\R} p_{t-\tau}(x-\xi)\Sigma^{(1)}_{\tau,\xi}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dxdydzdsdr\notag\\
      &=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dydzdsdr\notag\\
      & \ \ \ + \int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\notag\\
      &\ \ \ \ \cdot\left(\int_{[s,t]\times\R} \left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t-\tau}(x-\xi)dx\right)\Sigma^{(1)}_{\tau,\xi}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dydzdsdr\notag\\
      &=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dydzdsdr\label{pruebasumando3}\\
      &\ \ \ + \int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\notag\\
      &\ \ \ \ \cdot\left(\int_{[s,t]\times\R} \phi_{R,t}(\tau,\xi)\Sigma^{(1)}_{\tau,\xi}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dydzdsdr\label{pruebasumando4}.
   \end{align}
Haciendo lo respectivo para el sumando \eqref{pruebasumando2}, obtenemos las siguientes expresiones.
\begin{align}
   &\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}D_{r,z}D_{s,y}u(t,x)dxdydzdsdr\notag\\
   &=\frac{1}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R^2}\int_{Q_R}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\left( p_{t-s}(x-y)\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\notag\\
   &\ \ \ +\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\left(\int_{[s,t]\times \R} p_{t-\tau}(x-\xi)\Sigma^{(2)}_{\tau,\xi}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)\notag\\
   &\ \ \ +\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y} \left(\int_{[s,t]\times \R}p_{t-\tau}(x-\xi)\Sigma^{(1)}_{\tau,\xi}D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right) dxdydzdsdr\notag\\
   &=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\left(\frac{1}{\sigma_{R,t}}\int_{Q_R} p_{t-s}(x-y)dx\right)\notag\\
   &\ \ \ +\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   & \ \ \ \ \cdot\left(\int_{[s,t]\times \R} \left(\frac{1}{\sigma_{R,t}}\int_{Q_R} p_{t-\tau}(x-\xi)dx\right)\Sigma^{(2)}_{\tau,\xi}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)\notag\\
   &\ \ \ +\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   &\ \ \ \ \cdot \left(\int_{[s,t]\times \R}\left(\frac{1}{\sigma_{R,t}}\int_{Q_R} p_{t-\tau}(x-\xi)dx\right)\Sigma^{(1)}_{\tau,\xi}D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right) dydzdsdr\notag\\
   &=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dydzdsdr\label{pruebasumando5}\\
   &\ \ \ +\int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   &\ \ \ \ \cdot\left(\int_{[s,t]\times \R} \phi_{R,t}(\tau,\xi)\Sigma^{(2)}_{\tau,\xi}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right) dydzdsdr\label{pruebasumando6}\\
   &\ \ \ +\int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   &\ \ \ \ \cdot\left(\int_{[s,t]\times \R}\phi_{R,t}(\tau,\xi)\Sigma^{(1)}_{\tau,\xi}D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right) dydzdsdr.\label{pruebasumando7}
\end{align}
Observamos que los términos \eqref{pruebasumando3} y \eqref{pruebasumando5} coinciden, mientras que el término $\phi_{R,t}(r,z)\phi_{R,t}(s,y)$ es común dentro de las integrales en las expresiones \eqref{pruebasumando4}, \eqref{pruebasumando6} y \eqref{pruebasumando7}. Luego, definiendo 
\begin{align*}
   Z_{r,z,s,y}(\tau,\xi)&:=\Sigma_{r,z}\Sigma_{s,y}^{(1)}\Sigma_{\tau,\xi}^{(1)}D_{r,z}u(s,y)D_{s,y}u(\tau,\xi)\\
   &\ \ \ +\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(2)}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)\\
   &\ \ \ +\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(1)}D_{r,z}D_{s,y}u(\tau,\xi),
\end{align*}
y combinando las expresiones \eqref{pruebasumando3} a \eqref{pruebasumando7}, hallamos que la segunda derivada direccional de $F_{R,t}$ está dada por
\begin{align}
   &D_{v_{R,t}}(D_{v_{R,t}}F_{R,t})=2\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dydzdsdr\notag\\
   &\ \ \ \ + \int _{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}^{(1)}D_{r,z}u(s,y)\right)\notag\\
   & \ \ \ \ \cdot \left(\int_{[s,t]\times\R}\phi_{R,t}(\tau,\xi)\Sigma_{\tau,\xi}^{(1)}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dydzdsdr\notag\\
   &\ \ \ +\int _{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   &\ \ \ \ \cdot \left(\int_{[s,t]\times\R}\phi_{R,t}(\tau,\xi)\Sigma_{\tau,\xi}^{(2)}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dydzdsdr\notag\\
   &\ \ \ +\int _{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}(s,y)\Sigma_{r,z}\Sigma_{s,y}\right)\notag\\
   &\ \ \ \ \cdot \left(\int_{[s,t]\times\R}\phi_{R,t}(\tau,\xi)\Sigma_{\tau,\xi}^{(21)}D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\right)dydzdsdr\notag\\
   &=\mathcal{Y}_{R,t}^{(1)}+\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\notag\\
   & \ \ \ \ \cdot \left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\left[\Sigma_{r,z}\Sigma_{s,y}^{(1)}\Sigma_{\tau,\xi}^{(1)}D_{r,z}u(s,y)D_{s,y}u(\tau,\xi)\right]dydzdsdr\right) W(d\tau,d\xi)\notag\\
   &\ \ \ +\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\notag\\
   &\ \ \ \ \cdot\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\left[\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(2)}D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)\right]dydzdsdr\right) W(d\tau,d\xi)\notag\\
   &\ \ \ +\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\notag\\
   &\ \ \ \ \cdot\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)\left[\Sigma_{r,z}\Sigma_{s,y}\Sigma_{\tau,\xi}^{(1)}D_{r,z}D_{s,y}u(\tau,\xi)\right]dydzdsdr\right) W(d\tau,d\xi)\notag\\
   &=\mathcal{Y}_{R,t}^{(1)}+\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dydzdsdr\right)W(d\tau,d\xi)\notag\\
   &=2\mathcal{Y}_{R,t}^{(1)}+\mathcal{Y}_{R,t}^{(2)},\label{pruebasumando8}
\end{align}
en donde 
\begin{align*}
   &\mathcal{Y}_{R,t}^{(1)}:=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dydzdsdr\\
   &\mathcal{Y}_{R,t}^{(2)}:=\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dydzdsdr\right)W(d\tau,d\xi),
\end{align*}
y donde se ha usado múltiples veces del teorema de Fubini y la notación previamente establecida. Una vez que llegamos a este punto, buscamos acotar cada uno de los sumandos de \eqref{pruebasumando8}. 

Comenzamos con el término $\mathcal{Y}_{R,t}^{(1)}$. Primero hallamos una cota para la norma $L^2(\Omega)$ de la expresión $\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}D_{r,z}u(s,y)$. Usando la desigualdad de Cauchy-Schwarz, 
\[
   \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}D_{r,z}u(s,y)}_2\leq \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}}_4\norm{D_{r,z}u(s,y)}_4, 
\]
y usando ahora la desigualdad \eqref{cota1eraderivada}, para $0\leq r<s\leq T$, \ $y,z\in \R$ y $p=4$, hallamos que 
\begin{equation}\label{pruebacotasigma1}
      \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}}_4\norm{D_{r,z}u(s,y)}_4\leq \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}}_4C_{T}^{(3)}p_{s-r}(y-z),   
\end{equation}
donde $C_{T}^{(3)}$ es una constante que solo depende de $T$. Por otro lado, por hipótesis $\sigma'$ es una función acotada, digamos, por una constante $K_1>0$, de forma que 
\begin{equation}\label{pruebacotasigma2}
   \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}}_4\leq K_1\norm{\Sigma_{r,z}\Sigma_{s,y}}_4\leq K_1\norm{\Sigma_{r,z}}_8\norm{\Sigma_{s,y}}_{8}.
\end{equation}
   
Ahora bien, como la condición inicial en el problema del calor que estamos tratando es $u_0(w)=u(0,w)=1$, para cualquier $w\in \R$, y dado que $\sigma$ es Lipschitz,  se tienen las siguientes relaciones para $\Sigma_{r,z}$:
\begin{align}
      \norm{\Sigma_{r,z}}_8&\leq\norm{\sigma(u(r,z))-\sigma(u(0,z))}_8+\norm{\sigma(u(0,z))}_8\notag\\
      &\leq K_2\norm{u(r,z)-u(0,z)}+\norm{\sigma(1)}_8\notag\\
      &\leq K_2(\norm{u(r,z)}_8+\norm{1}_8)+|\sigma(1)|\notag\\
      &\leq K_2(C_{T}^{(4)}+1)+|\sigma(1)|\label{pruebacotasigma3},
\end{align}
en donde $K_2>0$ es la constante de Lipschitz de $\sigma$, y en la cuarta desigualdad hemos hecho uso de la igualdad \eqref{cotasolucionmild} con $T>0$ y $p=8$. Obsérvese que la cota obtenida antes depende solamente de $\sigma$ y de $T>0$. De manera análoga se puede acotar el término $\norm{\Sigma_{s,y}}_8$. Por lo tanto, combinando \eqref{pruebacotasigma1}, \eqref{pruebacotasigma2}, \eqref{pruebacotasigma3} y la cota \eqref{pruebacotasigma3} análoga para $\Sigma_{s,y}$, hallamos que 
\begin{equation}\label{pruebacotasigma4}
   \norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma_{s,y}^{(1)}D_{r,z}u(s,y)}_2\leq C_{T}^{(5)}p_{s-r}(y-z),
\end{equation} 
en donde $C^{(5)}_T>0$ es una constante que solo depende de $T>0$ y $\sigma$. Obtenida esta cota, procedemos a extraer la norma $L^{2}(\Omega)$ de $\mathcal{Y}_{R,t}^{(1)}$. Obtenemos así las siguientes relaciones.
 \begin{align*}
   &\norm{\mathcal{Y}_{R,t}^{(1)}}_2=\left(\int_\Omega \left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)dydzdsdr\right)^2d\P\right)^{1/2}\\
   &=\Bigg(\int_\Omega\int_{[0,t]^2}\int_{\R^2}\int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\right)\\
   &\ \ \ \ \cdot \left(\phi_{R,t}(r',z')\phi_{R,t}^2(s',y')\Sigma_{r',z'}\Sigma_{s',y'}\Sigma^{(1)}_{s',y'}D_{r',z'}u(s',y')\right)dydzdsdrdy'dz'ds'dr'd\P\Bigg)^{1/2}\\
   &=\Bigg(\int_{[0,t]^2}\int_{\R^2}\int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\phi_{R,t}(r',z')\phi_{R,t}^2(s',y')\right)\\
   &\ \ \ \ \cdot \left(\int_\Omega\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)\Sigma_{r',z'}\Sigma_{s',y'}\Sigma^{(1)}_{s',y'}D_{r',z'}u(s',y')\right) dydzdsdrdy'dz'ds'dr'd\P\Bigg)^{1/2}\\
   &\leq \Bigg(\int_{[0,t]^2}\int_{\R^2}\int_{[0,t]^2}\int_{\R^2}\left(\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\phi_{R,t}(r',z')\phi_{R,t}^2(s',y')\right)\\
   &\ \ \ \ \cdot \left(\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)}_2\norm{\Sigma_{r',z'}\Sigma_{s',y'}\Sigma^{(1)}_{s',y'}D_{r',z'}u(s',y')}_2\right) dydzdsdrdy'dz'ds'dr'\Bigg)^{1/2}\\
   &=\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)}_2dydzdsdr\right)^{1/2}\\
   & \ \ \ \ \ \cdot \left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r',z')\phi_{R,t}^2(s',y')\norm{\Sigma_{r',z'}\Sigma_{s',y'}\Sigma^{(1)}_{s',y'}D_{r',z'}u(s',y')}_2dy'dz'ds'dr'\right)^{1/2}\\
   &=\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)}_2dydzdsdr.
 \end{align*}
 Usamos ahora \eqref{pruebacotasigma4} en la última desigualdad anterior para obtener que
 \begin{align*}
   \norm{\mathcal{Y}_{R,t}^{(1)}}_2&\leq\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)\norm{\Sigma_{r,z}\Sigma_{s,y}\Sigma^{(1)}_{s,y}D_{r,z}u(s,y)}_2dydzdsdr\\
   &\leq C^{(5)}_T\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}^2(s,y)p_{s-r}(y-z)dydzdsdr.
\end{align*}
En esta última expresión integramos sobre $z$ y expandimos el término $\phi_{R,t}(r,z)$ para hallar que 
\begin{align*}
   \norm{\mathcal{Y}_{R,t}^{(1)}}_2&\leq C^{(5)}_T\int_{[0,t]^2}\int_{\R}\phi_{R,t}^2(s,y)\left(\int_\R\phi_{R,t}(r,z)p_{s-r}(z-y)dz\right)dydsdr\\
   &=C^{(5)}_T\int_{[0,t]^2}\int_{\R}\phi_{R,t}^2(s,y)\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}\int_\R p_{t-r}(x-z)p_{s-r}(z-y)dzdx\right)dydsdr\\
   &=C^{(5)}_T\int_{[0,t]^2}\int_{\R}\phi_{R,t}^2(s,y)\left(\frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t+s-2r}(x-y)dx\right)dydsdr,
\end{align*}
donde hemos hecho uso de la propiedad de semigrupo del núcleo del calor $(p_t)_{t\geq0}$ en la última igualdad. Claramente se tiene que 
\[
   \frac{1}{\sigma_{R,t}}\int_{Q_R}p_{t+s-2r}(x-y)dx\leq \frac{1}{\sigma_{R,t}}\int_{\R}p_{t+s-2r}(x-y)dx\leq \frac{1}{\sigma_{R,t}},
\]
por lo que insertando esta última desigualdad en la anterior, llegamos a que 
\[
\norm{\mathcal{Y}_{R,t}^{(1)}}_2\leq \frac{C_T^{(5)}}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R}\phi_{R,t}^2(s,y).
\]
Utilizando el lema \ref{LemaA.4a}, obtenemos que existe un $R_0\geq1$ dependiente de $t>0$ tal que para cualquier $R\geq R_0$,
\[
   \frac{C_T^{(5)}}{\sigma_{R,t}}\int_{[0,t]^2}\int_{\R}\phi_{R,t}^2(s,y)\leq \frac{C_T^{(5)}}{\sigma_{R,t}}\int_{[0,t]^2}C_tdsdr\leq \frac{C_t^{(6)}}{\sigma_{R,t}},
\]
por lo que tenemos la siguiente cota
\[
\norm{\mathcal{Y}_{R,t}^{(1)}}_2\leq \frac{C_t^{(6)}}{\sigma_{R,t}}.
\]
Finalmente, utilizamos el lema \ref{lemaA.3a}, para argumentar que, como $\tfrac{\sigma^2_{R,t}}{R} \xrightarrow[n\to\infty]{}2\int_{0}^{t}\xi(s)ds$, donde $\xi(s)=\E\left[\sigma^2(u(s,y))\right]$, y esta última función no depende de $y\in \R$ (ver el enunciado del lema mencionado) entonces existe un $R_1>0$ tal que para cualquier $R>R_1$,
\begin{equation}\label{cotafinal1}
      \norm{\mathcal{Y}_{R,t}^{(1)}}_2\leq \frac{C_t^{(6)}}{\sigma_{R,t}}=\frac{C_t^{(6)}}{\sqrt{\frac{\sigma_{R,t}^2}{R}}\sqrt{R}}\leq \frac{C_t^{(7)}}{\sqrt{R}},  
\end{equation}
en donde $C_t^{(7)}$ es una constante que depende de $C_t^{(6)}$ y de la integral en $[0,t]$ de $\xi$, función que es independiente de $y$, y por ende tanto la integral como la constante $C_t^{(7)}$ sólo dependen de $t$.

Procedemos ahora a estimar el término $\mathcal{Y}_{R,t}^{(2)}$. Extrayendo el cuadrado de la norma $L^{2}(\Omega)$ de dicho término, y utilizando la isometría de Itô-Walsh, tenemos las siguientes igualdades.
\begin{align*}
   &\norm{\mathcal{Y}_{R,t}^{(2)}}_2^{2}=\norm{\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dydzdsdr\right)W(d\tau,d\xi)}_2^2\\
   &=\E\left[\left(\int_{[0,t]\times\R}\phi_{R,t}(\tau,\xi)\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dydzdsdr\right)W(d\tau,d\xi)\right)^2\right]\\
   &=\E\left[\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dydzdsdr\right)^2d\xi d\tau\right]\\
   &=\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\E\left[\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dydzdsdr\right)^2\right]d\xi d\tau.
\end{align*}
Reescribiendo el cuadrado de la integral anterior, tenemos que 
\begin{align*}
   &\norm{\mathcal{Y}_{R,t}^{(2)}}_2^{2}=\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\E\left[\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r,z)\phi_{R,t}(s,y)Z_{r,z,s,y}(\tau,\xi)dydzdsdr\right)^2\right]d\xi d\tau\\
   &=\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\E\Bigg[\left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r_1,z_1)\phi_{R,t}(s_1,y_1)Z_{r_1,z_1,s_1,y_1}(\tau,\xi)dy_1dz_1ds_1dr_1\right)\\
   &\ \ \ \ \cdot \left(\int_{[0,t]^2}\int_{\R^2}\phi_{R,t}(r_2,z_2)\phi_{R,t}(s_2,y_2)Z_{r_2,z_2,s_2,y_2}(\tau,\xi)dy_2dz_2ds_2dr_2\right)\Bigg]d\xi d\tau\\
   &=\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\int_{0\leq r_1\leq s_1\leq \tau}\int_{0\leq r_2\leq s_2\leq \tau}\int_{\R^4}\phi_{R,t}(r_1,z_1)\phi_{R,t}(s_1,y_1)\phi_{R,t}(r_2,z_2)\phi_{R,t}(s_2,y_2)\\
   &\ \ \ \ \cdot \E\left[Z_{r_1,z_1,s_1,y_1}(\tau,\xi)Z_{r_2,z_2,s_2,y_2}(\tau,\xi)\right]dy_1dz_1ds_1dr_1dy_2dz_2ds_2dr_2d\xi d\tau,
\end{align*}
en donde $Z_{r_1,z_1,s_1,y_1}(\xi,\tau)$ y $Z_{r_2,z_2,s_2,y_2}(\xi,\tau)$ son copias i.i.d. de $Z_{r,z,s,y}(\xi,\tau)$. A continuación, utilizamos la desigualdad de Cauchy-Schwarz justo en estos términos para obtener que 
\begin{align*}
   \norm{\mathcal{Y}_{R,t}^{(2)}}_2^{2}&\leq \int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\int_{0\leq r_1\leq s_1\leq \tau}\int_{0\leq r_2\leq s_2\leq \tau}\int_{\R^4}\phi_{R,t}(r_1,z_1)\phi_{R,t}(s_1,y_1)\phi_{R,t}(r_2,z_2)\phi_{R,t}(s_2,y_2)\\
   &\ \ \ \ \cdot \norm{Z_{r_1,z_1,s_1,y_1}(\tau,\xi)}_2 \norm{Z_{r_2,z_2,s_2,y_2}(\tau,\xi)}_2 dy_1dz_1ds_1dr_1dy_2dz_2ds_2dr_2d\xi d\tau\\
   &=\int_{[0,t]}\int_\R\phi_{R,t}^2(\tau,\xi)\int_{0\leq r_1\leq s_1\leq \tau}\int_{0\leq r_2\leq s_2\leq \tau}\int_{\R^4}\prod_{i=1,2}\phi_{R,t}(r_i,z_i)\phi_{R,t}(s_i,y_i)\\
   &\ \ \ \ \cdot \norm{Z_{r_i,z_i,s_i,y_i}(\tau,\xi)}_2  dy_1dz_1ds_1dr_1dy_2dz_2ds_2dr_2d\xi d\tau
\end{align*}
En este punto, nos interesamos por 
\end{proofpart}
\qedhere
\end{proof}



% % % primero se demuestra una cota superior elemental para la distancia uniforme entre la densidad de una variable aleatoria $F$ que está dada coom un funcional de un proceso Gaussiano isonormal, y la densidad de una variable aleatoria normal estándar.

% % % Para lograr dicha estimación, es necesario ver a la función de distribución de la variable aleatoria $F$ como la integral de Skorokhod (o como divergencia en el sentido del cálculo de Malliavin). Esto es, $F=\delta(v)$ para algún $v$.

% % % La idea es aplicar las técnicas de estimación anteriores a los promedios espaciales $F_{R,t}$ de la solución mild a la ecuación del calor. 

% % % Tenemos el siguiente resultado, el cual se encuentra en \cite{Caballero1998-hz}, y del cual haremos uso sin ahondar en su demostración.
% % % \begin{teo} 
% % % Sea $F\in \D^{1,1}$ y  sea $v\in L^{1}(\Omega;\mathcal{H})$ tal que $D_vF\neq 0$ c.s. Supongamos que $v/D_vF\in \text{Dom}(\delta)$. Entonces la distribución de $F$ tiene una densidad continua y acotada dada por 
% % % \[
% % % f_F(x)=\E\left[\1_{\{F>x\}}\delta \left(\frac{v}{D_vF}\right)\right]   
% % % \]
% % % \end{teo}$

% % % El resultado anterior es válido para cualquier espacio de Hilbert $\mathfrak{H}$. Aplicando el resultado anterior al contexto de una variable aleatoria adecuada, se tiene lo siguiente:

% % % \begin{teo} 
% % %  Sea $v\in \D^{1,6}(\Omega;\mathcal{H})$ y $F=\delta(v)\in \D^{2,6}$ con $\E\left[F\right]=0, \E\left[F^{2}\right]=1$ y $\left(D_vF\right)^{-1}\in L^4(\Omega)$. Entonces $v/D_vF \in \text{Dom}(\delta)$, $F$ admite una densidad $f_F(x)$ y se tiene la siguiente desigualdad
% % %  \[
% % %    \sup_{x\in \R} \abs{f_F(x)-\Phi(x)}\leq \left(\|F\|_4\|\left(D_vF\right)^{-1}\|_4+2\right)\|1-D_vF\|_2+\|\left(D_vF\right)^{-1}\|^{2}\|D_v \left(D_vF\right)\|_2,
% % %    \]
% % % donde $\Phi(x)$ es la función de densidad de una variable normal estándar.

% % % \end{teo}
% % % \begin{proof} 
% % %   Vamos a denotar por $F_R$ a los promedios espaciales de la ecuación estocástica del calor, y directamente por $\sigma$ a la varianza de los promedios espaciales.

% % %   Buscamos hallar una cota uniforme para $x\in \R$ para las siguientes cantidades
% % %   \[
% % %   \abs{f_F(x)-\phi(x)}. 
% % %   \]
% % %   Gracias a que los promedios espaciales admite una densidad dada por 
% % %   \[
% % %   f_F(x)=\E\left[\1_{\{F_R>x\}}\delta(\frac{v}{D_vF_R})\right], 
% % %   \]
% % %   podemos operar con estas cantidades de la siguiente forma:
% % %    \begin{align*}
% % %    f_F(x)=\E\left[\1_{\{F_R>x\}}\delta(\frac{v}{D_vF_R})\right]&=\E\left[\1_{\{F_R>x\}}\frac{1}{D_{v_R}}\delta(v_R)\right]-\E\left[\1_{\{F_R>x\}}\langle D \left(\frac{1}{D_{v_R}F_R}\right),v_R\rangle_{\mathcal{h}}\right]\\
% % %    &=\E\left[\1_{\{F_R>x\}}\frac{F_R}{D_{v_R}F_R}\right]+\E\left[\1_{\{F_R>x\}}\left(\frac{1}{\left(D_{v_R}F_R\right)^2}\right)D_{v_R}D_{v_R}F_R\right].
% % %    \end{align*}
% % %   Ahora bien, en el término de la derecha de antes, podemos acotar la cantidad de la siguiente forma:
% % %   \[
% % %    \E\left[\1_{\{F_R>x\}}\left(\frac{1}{\left(D_{v_R}F_R\right)^2}\right)D_{v_R}D_{v_R}F_R\right]\leq \sqrt{\E\left[\abs{D_{v_R}F_R}^{-4}\right]\E\left[\abs{D_{v_R}D_{v_R}F_R}^2\right]},
% % %   \]
% % %   de tal forma que el problema simplificado consiste en analizar el término de la izquierda, es decir, analizar.
% % %   \[
% % %    \E\left[\1_{\{F_R>x\}}\frac{F_R}{D_{v_R}F_R}\right] 
% % %   \]
% % %    Pero en cuanto a este término, notamos lo siguiente:
% % %   \begin{align*}
% % %    \E\left[\1_{[x,\infty)}(F_R)\frac{F_R}{D_vF_R}\right]&=\E\left[\1_{[x,\infty)}(F_R)F_R \left(\frac{1}{D_vF_R}-\frac{1}{\sigma^2}\right)\right] + \frac{1}{\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right]\\
% % %   \end{align*}
% % %   Ahora bien, nosotros tenemos la siguiente estimación 
% % %   \[
% % %   \sigma^2\approx \E\left[D_{V_R}F_R\right]\approx 1,  
% % %   \]
% % %   por lo que insertando dicha aproximación en la igualdad anterior, tenemos que 
% % %   \begin{align*}
% % %    \E\left[\1_{[x,\infty)}(F_R)F_R \left(\frac{1}{D_vF_R}-\frac{1}{\sigma^2}\right)\right] &+ \frac{1}{\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right]\\
% % %    &=\frac{1}{(D_vF_R)\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right] \left(\E\left[D_vF_R\right]-D_vF_R\right)\\
% % %    &+\frac{1}{\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right]\\
% % %   \end{align*}
% % %   Tenemos aquí dos términos nuevamente. El primero de ellos lo podemos acotar de la siguiente forma:
% % %   \[
% % %     \|F_R\|_{L^4(\Omega)}\|\left(D_{v_R}F_R\right)^{-1}\|_{L^{4}(\Omega)}\text{Var}\abs{D_{v_R}F_R}^{1/2}\le C\|DD_{v_R}F_R\|^{1/2}_{L^{2}(\Omega,\mathcal{h})},
% % %   \] 
% % %   donde $C$ es una constante adecuada.

% % %   Resta entonces analizar qué sucede con el siguiente término:
% % %   \[
% % %    \E\left[\1_{[x,\infty)}(F_R)F_R\right].
% % %   \]
% % %   Para ello, utilizamos la herramienta de Malliavin-Stein. Concretamente notemos que, sumando y restando el término $\1_{\{N>x\}}N$, donde $N$ es una variable aleatoria normal estándar, tenemos que, denotando por $g_x(F_R):=\1_{\{F_R>x\}}F_R$ y lo correspondiente para $g_x(N)$, 
% % %   \[
% % %    \E\left[\1_{\{F_R>x\}}\right]=\E\left[g_x(F_R)-g_x(N)\right]+\E\left[g_x(N)\right].
% % %   \]
% % %   El término de la izquierda en la ecuación anterior es sencillo de controlar, por lo que resta analizar la diferencia entre las esperanzas de $g_x$ evaluada en $F_R$ y en $N$. Para ello, de acuerdo al capítulo de método de Stein, notamos que 
% % %   \[
% % %   \E\left[g_x(F_R)-g_x(N)\right]=\E\left[F_R\psi_x(F_R)-\psi_x'(F_R)\right], 
% % %   \]
% % %   donde $y\psi_x(y)-\psi_x'(y)=g_x(y)-\E\left[g_x(N)\right]$.

% % %   Ahora bien, analizando la esperanza en términos de $\psi_x$, tenemos que 
  
% % %   \begin{align*}
% % %    \E\left[F_R\psi_x(F_R)\right]&=\E\left[\delta(v_R)\psi_x(F_R)\right]\\
% % %    &=\E\left[\abs{\langle v_R,DF_R\rangle}\psi_x'(F_R)\right]\\
% % %    &=\E\left[D_{V_R}F_R\psi_x'(F_R)\right]\\
% % %    &=\sigma^2 \E\left[\psi_x(F_R)\right]+\E\left[\left(D_{v_R}F_R-\sigma^2\right)\psi_x'(F_R)\right]\\
% % %    &\le \|\psi_x'\|_\infty \text{Var}\abs{D_{v_R}F_R}^{-1/2}\\
% % %    &\leq \|\psi_x'\|_\infty \|DD_{v_R}F_R\|_{L^2(\Omega,\mathfrak{H})}.
% % %   \end{align*}
% % %   Finalmente, para la prueba de la última cota, si nosotros vemos a la variable aleatoria normal $N$ como una divergencia, esto es, $N=\delta(h)$ para algún $h$ tal que $\|h\|=1$, entonces 
% % %   \[
% % %    f_N(x)=\E\left[\1_{\{N>x\}}\delta(h)\right]=\E\left[\1_{\{N>x\}N}\right]=\E\left[g_x(N)\right],
% % %   \]
% % %   por lo que concluimos la cota.
% % % \end{proof}

% % % Y para la prueba del resultado principal, es esencial el uso de dos resultados distintos: uno de ellos consiste en una cotas de momentos para la segunda derivada de Malliavin de la solución, y los momentos negativos de la proyección $DF_{R,t}$ en $v_{R,t}$, donde $F_{R,t}=\delta(v_{R,t})$. El primero de ellos se encuentra en el siguiente resultado:






%\section{Formulación Mild en el caso sin deriva}
%\section{Una ecuación estocástica para $Du$ y $D^2u$}
%\section{Densidad explícita de los promedios espaciales}
%Disclaimer: no se van a manejar los momentos inversos.
%\textit{Contribución del Kernel de Stein}
%\textit{Contribución de las derivadas de orden superior}

%\section{Aproximación uniforme de las densidades} %La sección integradora de los conceptos

\chapter{Conclusiones y trabajo futuro}
dfasdf
\appendix
\chapter{Resultados técnicos}
En este apéndice colocamos los tres resultados técnicos utilizados en la tercera parte de la prueba del resultado principal \ref{teoremaprincipal} de este texto.  Específicamente, los lemas A.1, A.2 y A.3 de este apéndice corresponden respectivamente a los lemas A.2, A.3 parte (a) y A.4 parte (a), del apéndice presente en el artículo de Sefika Kuzgun y David Nualart \cite{KUZGUN202268}. Las pruebas del lema A.1 y A.3 se pueden encontrar en el artículo antes mencionado, mientras que la prueba del lema A.2 se puede encontrar en \cite[proposición 3.1]{HUANG20207170}.

\begin{lema}\label{lemaA.2}
 Sea $\phi_{r,z,s,y}(t,x)$ de acuerdo al enunciado del teorema \ref{teocota2daderivada} y sea $(t,x)\in [0,\infty)\times\R$. Entonces para $0<r<s<t$,
 \end{lema}
 \begin{lema}\label{lemaA.3a} 
  Sea $\sigma_{R,t}$ según la definición \ref{defpromediosespaciales}. Entonces se cumple que 
  \[
      \lim_{R\to\infty}\frac{\sigma_{R,t}^2}{R}=2\int_{0}^{t}\xi(s)ds,
  \] 
  en donde $\xi(s)=\E\left[\sigma^2(u(s,y))\right]$, donde tal y como la notación indica, la función $\xi$ es independiente del punto $y\in \R$ y además es acotada en intervalos compactos.
  \end{lema}
  \begin{lema}\label{LemaA.4a}
   Sean $t>0$ y $\phi_{R,t}$ según se define en \eqref{defnotacionphi}. Entonces existe un $R_0\geq1$, dependiente de $t$, tal que para cualesquiera $0<s<t$ y $R\geq R_0$,
   \[
      c_t\leq \int_\R\phi_{R,t}^2(s,y)dy\leq C_t,
   \]
   en donde la cota inferior es válida siempre que $t/2<s<t$.
  \end{lema}
\backmatter

\bibliographystyle{amsplain}
\bibliography{build/Referencias_tesis}
\end{document}