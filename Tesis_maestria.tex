\documentclass[letterpaper,twoside,12pt]{book} 
%\usepackage[left = 0.5in, right = 0.5in, top = 0.9in, bottom = 0.9in]{geometry}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage[spanish,es-nodecimaldot,es-tabla]{babel}
\usepackage[utf8]{inputenc}
\usepackage[svgnames,table]{xcolor}  
\usepackage{CIMATpreamble}  
\usepackage[dvipsnames]{xcolor}
%\usepackage[12pt]{extsizes}
\linespread{1.1}


\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathrsfs}
\usepackage{tikz-cd}
\usepackage{mathrsfs}
\usepackage[bbgreekl]{mathbbol}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage[colorlinks=true,linkcolor=red,citecolor=red]{hyperref}
\graphicspath{{img/}}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\B}{\mathcal{B}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\W}{\dot{W}}
\newcommand{\1}{\mathds{1}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\inv}{^{-1}}
\renewcommand{\to}{\rightarrow}
\newcommand{\ent}{\Longrightarrow}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\renewcommand{\qedsymbol}{$\blacksquare$}

\theoremstyle{definition}
\newtheorem{dfn}{Definición}
\theoremstyle{definition}
\newtheorem{teo}{Teorema}
\theoremstyle{definition}
\newtheorem{cor}{Corolario}
\theoremstyle{definition}
\newtheorem{prop}{Proposición}
\theoremstyle{definition}
\newtheorem{obs}{Observación}
\theoremstyle{definition}
\newtheorem{ejem}{Ejemplo}
\theoremstyle{definition}
\newtheorem{lema}{Lema}



\title{\textbf{}}
\author{Iván Irving Rosas Domínguez}
\date{\today}

\DeclareSymbolFontAlphabet{\mathbbm}{bbold}
\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
\DeclareMathSymbol\bbDelta  \mathord{bbold}{"01}


\author{Ivan Irving Rosas Domínguez}
\documentType{T E S I S}  
\title{Sobre convergencia de densidades de promedios espaciales para la ecuación estocástica del calor}
\degree{Maestría en Ciencias con Orientación en Probabilidad y Estadística}
\supervisor{Dr. Arturo Jaramillo Gil}
%\supervisorSecond{} 
\cityandyear{Guanajuato, Gto., ?? de ?? de 2024}


\begin{document}

\maketitle 

\thispagestyle{empty}  

\frontmatter

% % Dedication
% \chapter*{}
% \begin{flushright}%
%   \emph{Dedicatoria \ldots}
%   \thispagestyle{empty}
% \end{flushright}

% % Abstract
% \chapter*{Resumen}
% \addcontentsline{toc}{chapter}{Resumen}


% \paragraph{Palabras clave:} modelación espacio-temporal, modelo jerárquico bayesiano, ciencia de datos, cómputo estadístico,, datos de áreas

% % Acknowledgements
% \chapter*{Agradecimientos}
% \addcontentsline{toc}{chapter}{Agradecimientos}

% A mis padres \ldots

% Table of contents and list of figures
\tableofcontents

% \listoffigures

% Chapters





\mainmatter

\chapter{Elementos de ecuaciones diferenciales parciales estocásticas}

En este capítulo hablaremos acerca del concepto de ecuación diferencial parcial estocástica.
Dichos objetos comenzaron a ser estudiados a mediados del siglo pasado con la llegada de la integral de Itô.
El trabajo de Itô y demás colegas para tratar ecuaciones diferenciales estocásticas inmediatamente sugirió el estudio de la posible generalización a ecuaciones diferenciales en varias variables.
Durante las décadas de los 50, 60 y 70, varios artículos hacían alusión a estos objetos, aunque no fue sino hasta finales de la década de 1970 cuando finalmente estas ecuaciones comenzaron a ser estudiadas como objetos matemáticos en sí.
Finalmente, en la década de 1980, varias monografías surgieron con el propósito de construir una teoría que cohesionara las ideas existentes en el área. Destaca en particular la monografía escrita por Walsh \cite{Walsh_J.B_Introduction_to_SPDEs} y publicada en 1986, en donde el autor aborda el estudio de éstas ecuaciones, dando un significado preciso a lo que significa plantear y resolver una ecuación diferencial que involucre procesos estocásticos cuyas trayectorias generalmente no son diferenciables, y que tengan variables tanto espaciales como una temporal.

Desde esta década múltiples trabajos en esta área han sido publicados. Esto, junto con el uso de herramientas como cálculo de Malliavin en el estudio de las mismas, han hecho de las Ecuaciones diferenciales parciales estocásticas una área bastante activa en la probabilidad moderna. 
\section{Introducción a SPDEs}
Comenzamos en este capítulo con la pregunta clave: ¿qué es una ecuación diferencial parcial estocástica? Para responder a esta pregunta, podemos repasar cada uno de los conceptos que nos llevan a esta idea. Partimos del mundo clásico en $\R^{N}:$ sean $x\in \R^{N}$, $t\in [0,\infty)$ y $u:\R^{N}\times[0,\infty)\to \R$ una función en las variables $x$ y $t$. Una ecuación diferencial parcial es una ecuación del tipo 

\begin{equation}\label{ec_dif_parc}
    F(t,x,u,Du,D^2u,...)=0,    
\end{equation}

donde $x$ y $t$ se interpretan como las variables espacial y temporal respectivamente, y $F$ es una función arbitraria que depende tanto de las variables espaciales como de la temporal, así como de la misma función $u$ y de sus derivadas de orden $\alpha=(\alpha_1,...,\alpha_n)$, donde $\alpha_i$ representa la derivada parcial en la $i$-ésima coordenada.

Si existe una función $u$ tal que $u\in C^{\alpha}\left(\R^{N}\times [0,\infty)\right)$ y además $u$ satisface la ecuación \eqref{ec_dif_parc}, donde $\alpha$ es el multi-índice más grande tal que el orden de todas las derivadas que aparecen en la ecuación son cubiertas por el mismo, entonces decimos que $u$ es una solución clásica a la ecuación diferencial parcial anterior. De esta manera, y en particular para la ecuación estocástica del calor homogénea clásica, la cual está dada por 
\[
    \partial_tu(x,t)-\frac{1}{2}\Delta_x u(x,t)=0,
\]
se entiende por solución clásica a una función $u:\R^{N}\times[0,\infty)\to \R$ tal que $u\in C^{2,1}\left(\R^{N}\times [0,\infty)\right)$, esto es, al menos todas las segundas derivadas en las variables espaciales existen y son continuas, y al menos la derivada en la variable temporal existe y es continua. 

Cabe destacar que la noción de solución de un ecuación diferencial parcial en el sentido clásico coincide con lo esperable: una función $u$ será solución de una ecuación diferencial si al menos tiene tantas derivadas como aquellas involucradas en la igualdad \eqref{ec_dif_parc}.

No obstante, en algunas ocasiones los problemas planteados requieren soluciones que no necesariamente cumplan tales características de regularidad, pero que intuitivamente deberían tener sentido.
 Un ejemplo muy conocido es aquél de la ecuación de onda con una condición inicial que no necesariamente es regular. Por ejemplo, si hablamos del problema homogéneo de la ecuación de onda unidimensional dado por:
\begin{equation}\label{wave_eq_sobolev}
    \begin{cases}
        \partial_{tt}u(x,t)-\kappa^2\partial_{xx}u(x,t)=0 & (x,t)\in [-\pi,\pi]\times[0,\infty),\\
        u(-\pi,t)=u(\pi,t)=0, & t\in [0,\infty),\\
        u(x,0)=u_0(x)=\pi-\abs{x} & x\in [-\pi,\pi],\\
        u_t(x,0)=0 & (x,t) \in [-\pi,\pi]\times[0,\infty),\\
    \end{cases}
\end{equation}
es claro que tenemos una inconsistencia al momento de colocar la condición inicial dada por el valor absoluto $\pi-|x|$: dicha función no es derivable en $x=0$. No obstante, desde un punto de vista intuitivo, claramente el problema tiene sentido: estamos modelando el comportamiento de una cuerda sujeta en sus extremos a los puntos $-\pi$ y $\pi$, la cual al soltarla tiene una posición inicial dada por $\pi-|x|$ y cuya velocidad inicial es constante $0$ en cualquier punto de la cuerda.\newline

La idea para hallar una solución al problema anterior a grandes rasgos consiste en reformular el problema de forma que se conserve la esencia del problema, y posteriormente obtener una solución a este problema reformulado. Una manera estándar es utilizar la teoría de distribuciones. En el caso particular de la ecuación de onda anterior, se procede formalmente de la siguiente manera:

Supongamos que $u$ es una solución al problema $\eqref{wave_eq_sobolev}$. En particular se debería cumplir que 
\[
\partial_{tt}u(x,t)=\kappa^2\partial_{xx}u(x,t),
\]
por lo que para cualquier función $\phi\in C^{\infty}_c\left([-\pi,\pi]\times [0,\infty)\right)$ tal que se anule en la frontera,, se tiene que al multiplicar e integrar, la ecuación anterior se convierte en:
\[
    \int_{[-\pi,\pi]\times [0,\infty)}\partial_{tt}u(x,t)\phi(x,t)dx dt=\kappa^2\int_{[-\pi,\pi]\times [0,\infty)}\partial_{xx}u(x,t)\phi(x,t)dx dt,
\]
y suponiendo que es válido utilizar integración por partes, la igualdad anterior equivale a
\[
\int_{[-\pi,\pi]\times [0,\infty)}u(x,t)\partial_{tt}\phi(x,t)dx dt=\kappa^2\int_{[-\pi,\pi]\times [0,\infty)}u(x,t)\partial_{xx}\phi(x,t)dx dt,
\]
en donde los términos evaluados en la frontera se anulan gracias a que las funciones $\phi$ (a menudo denominadas funciones de prueba) se anulan en la misma. Reordenando los términos anteriores, se obtiene una reformulación de la ecuación diferencial del problema \eqref{wave_eq_sobolev} la cual no necesita que la función $u$ sea derivable. A saber, decimos que $u$ es una solución débil de la ecuación diferencial asociada al problema \eqref{wave_eq_sobolev} si se cumple que 
\[
\int_{[-\pi,\pi]\times [0,\infty)}u(x,t)(\partial_{tt}\phi(x,t)-\kappa^2\partial_{xx}\phi(x,t))dx dt=0    
\]
para cualquier función $\phi\in C^{\infty}_c([-\pi,\pi]\times [0,\infty))$ que se anule en la frontera.

Claramente toda solución clásica (o fuerte) es a su vez una solución débil, ya que precisamente la regularidad de dichas soluciones permiten hacer de manera rigurosa los cálculos formales anteriores.
No obstante, la existencia de una solución débil en el sentido anterior no garantiza que dicha solución sea regular, por lo que en principio es más sencillo demostrar la existencia o unicidad de las soluciones débiles, a pesar de que estas gocen de menos propiedades que las soluciones fuertes o clásicas, las cuales no siempre existirán, como en el caso del problema $\eqref{wave_eq_sobolev}$.

Llegados a este punto, reflexionamos ahora en dirección de la probabilidad. Supongamos nuevamente que tenemos la ecuación de onda unidimensional, pero considerando el caso no homogéneo. Tenemos entonces la siguiente ecuación
\begin{equation}\label{wave_spde}
\partial_{tt}u(x,t)=\kappa^2\partial_{xx}u(x,t)+F(x,t), \qquad (x,t)\in [-\pi,\pi]\times[0,\infty)
\end{equation}
donde ahora $F$ es una función que representa la cantidad de presión por unidad de longitud que se aplica a la cuerda. Bajo condiciones de regularidad en la función $F$, podemos resolver el problema de manera clásica utilizando el método de separación de variables. Sin embargo, nos preguntamos ¿qué sucede si ahora $F$ representa una función que no sea diferenciable? Más aún, ¿qué pasa si consideramos a $F$ una \textit{perturbación aleatoria}?

Para dar un sentido más preciso a las ideas anteriores podemos pensar, para $x$ y $t$ fijos, a $F(x,t)$ como una variable aleatoria. Dicha variable depende tanto del espacio como del tiempo, y por ende podemos interpretar a $F$ como un proceso estocástico que evoluciona de manera espacio-temporal. Tal y como se comentó al inicio, es conocido que existen procesos estocásticos cuyas trayectorias son no diferenciables, por lo que en principio pensar en resolver una ecuación diferencial de este estilo de manera clásica se vislumbra una tarea complicada.

Más aún, ¿en qué sentido se debe interpretar una ecuación como \eqref{wave_spde}?, o bien, considerando la siguiente ecuación del calor no homogénea 
\begin{equation}\label{heat_spde}
    \partial_t{u(x,t)}=\frac{1}{2}\Delta_{x}u(x,t)+F(x,t), \qquad (x,t)\in \R^{N}\times[0,\infty),
\end{equation}
en donde el término $F$ sea aleatorio, ¿bajo qué significado hablamos de una solución de dicha ecuación?

Obsérvese que, si bien la teoría de distribuciones nos permite lidiar con la parte de la no regularidad de las condiciones del problema, para replicar el mecanismo que fue utilizado para reformular\eqref{wave_eq_sobolev} en un sentido débil, nos encontramos con una dificultad, ya que a no ser que el proceso $F(x,t)$ sea de variación finita, no será posible realizar de manera rigurosa la transición al momento de integrar por partes.

\subsection{Heurística de la formulación Mild}

Un vistazo más a detalle de lo anterior puede arrojarnos luz sobre cómo sortear el problema. Suponiendo que lo anterior es válido, una manera de proceder con la solución de la ecuación del calor no homogénea será la siguiente. Supongamos que tenemos la siguiente ecuación diferencial del calor unidimensional no lineal:
\[
\partial_tu(x,t)=\frac{1}{2}\partial_{xx}u(x,t)+W(x,t)u(x,t), \qquad (x,t)\in [0,L]\times [0,\infty) 
\]
donde $W(x,t)$ representa nuestra fuente de aleatoriedad (aunque bien puede pensarse como una función suficientemente suave en principio). Utilizando la técnica presentada en el ejemplo de la ecuación de onda, podríamos tomar una función $\phi \in C^{\infty}([0,L])$ cuya derivada se anule en la frontera, multiplicarla por la ecuación anterior e integrar con respecto al espacio y al tiempo, obteniendo que 
\[
\int_{0}^{L}u(x,t)\phi(x)dx-\int_{0}^{L}u(x,0)=\int_{0}^{L}\int_{0}^{t}\partial_{xx}u(x,s)\phi(x)dx ds+\int_{0}^{L}\int_{0}^{t}\partial_{xx}f(x,s)\phi(x)W(dx,ds)
\]
 
Resta entonces estudiar en qué sentido podemos integrar la ecuación \eqref{wave_spde} o \eqref{heat_spde} para sortear las dificultades que la aleatoriedad añade al problema.


\section{Integral de Itô-Walsh}
En esta sección presentamos la construcción de la integral de Itô-Walsh. Dicha integral nos permite construir procesos estocásticas que más adelante y en cierto sentido pueden verse como la solución de una ecuación diferencial parcial estocástica. Para lograr dicho objetivo, presentamos primeramente la teoría necesaria para construir la integral.
\subsection{Variables Gaussianas y Procesos Gaussianos}
Recordamos aquí las propiedades fundamentales de los procesos gaussianos. Salvo algunos casos especiales, la mayoría de las propiedades se enuncian sin demostración, por lo que nos referiremos a \cite{gall2016brownian} para revisar las pruebas de las mismas. Comenzamos definiendo la distribución Gaussiana.

\begin{dfn}
Sea $(\Omega, \F, \P)$ un espacio de probabilidad y consideremos a los reales dotados de la $\sigma-$álgebra de Borel, la cual denotamos por $\B(\R)$. Decimos que una variable aleatoria $X:(\Omega, \F)\to (\R,\B(\R))$ tiene distribución normal estándar (o bien, es una variable Gaussiana estándar), si la función de distribución de la misma está dada por: 
\[
F_X(t)=\int_{-\infty}^{t}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}dx.    
\]
En otras palabras, si la variable aleatoria $X$ tiene densidad (con respecto a la medida de Lebesgue en $\R$) dada por la función 
\[
f_X(t)=\frac{1}{\sqrt{2\pi}}e^{-t^2/2}.
\]
En tal caso, denotamos dicha situación por $X\sim$ Normal(0,1).
\end{dfn}

Directamente de la definición, tenemos la caracterización de la variable Gaussiana en términos de su transformada de Laplace y de su transformada de Fourier.
\begin{prop} 
 Sea $X$ una variable aleatoria con distribución normal estándar. Su transformada de Laplace en todo $\C$ está dada por 
 \[
 \mathcal{L}_X(\lambda):=\E\left[e^{\lambda X}\right]=e^{\lambda^2/2}, \qquad \lambda\in \C,
 \]
mientras que su transformada de Fourier (o función característica) en $\R$ está dada por 
\[
\mathcal{F}_X(\xi):=\E\left[e^{-i\xi X}\right]=e^{-\xi^2/2}, \qquad \xi \in \R. 
\]
 \end{prop}
Se puede ver usando alguna de las transformadas anteriores y usando un argumento inductivo que una variable normal estándar tiene momentos $n$-ésimos de cualquier orden, y que los mismos están dados por 
 \begin{itemize}
   \item $\E\left[X^{2n}\right]=(n-1)!!=\frac{(2n)!}{2^nn!}=1\cdot3\cdot5\cdot_...\cdot (2n-1), \qquad n\geq0$.
   \item $\E\left[X^{2n+1}\right]=0, \qquad n\geq0$.
 \end{itemize}
 En particular, se cumple que $\E\left[X^{n+1}\right]=n\cdot \E\left[X^{n-1}\right]$, para $n\geq1$.

Diremos también que una variable aleatoria Gaussiana tiene media $\mu\in \R$ y varianza $\sigma^2>0$ si se cumple que $Y=\sigma X+\mu$ para $X$ una variable normal estándar. En tal caso, denotaremos la situación por $Y\sim$ Normal$(\mu,\sigma^2)$. En vista de la proposición anterior y de la definición, se tiene la siguiente proposición.
\begin{prop} 
 Sea $Y$ una variable aleatoria normal con media $\mu\in \R$ y varianza $\sigma^2>0$. Entonces las siguientes tres proposiciones son equivalentes a este hecho.
 \begin{itemize}
    \item $\mathcal{L}_Y(\lambda)=e^{\mu\lambda+\sigma^2\lambda^2/2}, \qquad \lambda \in \C,$
    \item $\F_X(\xi)=e^{i\mu\xi-\sigma^2\xi^2/2}, \qquad \xi \in \R$,
    \item La distribución de $Y$ tiene densidad con respecto a la medida de Lebesgue en $\R$ dada por 
    \[
        f_Y(y)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(y-\mu)/2\sigma^2}, \qquad t\in \R.    
    \]
 \end{itemize}
 \end{prop}
 Por convención, diremos que una variable $Y$ tiene distribución normal con media $\mu\in \R$ y varianza $\sigma^2=0$ (denotado por $Y\sim$ Normal$(\mu,0)$) si se cumple que $Y=\mu$, \  $\lambda$-casi seguramente, donde $\lambda$ denota la medida de Lebesgue en $\R$.

 Pasamos ahora a definir los vectores Gaussianos o vectores normales conjuntos. 
 \begin{dfn} 
  Sea $(\R^d, \langle\cdot,\cdot\rangle)$ el espacio $\R^{d}$ dotado de su producto interno usual. Decimos que una variable aleatoria $X:(\Omega,\F)\to (\R^{d},\B(\R^d))$ es un vector gaussiano, o un vector normal conjunto, si para cualquier $u\in \R^d$, se tiene que $\langle u,X\rangle$ es una variable aleatoria Gaussiana (en $\R$).
  \end{dfn}
Presentamos un primer resultado sobre vectores aleatorios Gaussianos.
\begin{prop} 
Sea $X$ un vector aleatorio Gaussiano en $\R^{d}$. Entonces existe una función $\mu_X\in\R^d$ y una forma cuadrática no negativa $q_X:\R^d\to \R$ tales que para cualquier $u\in \R^d$,
\begin{itemize}
    \item $\E\left[\langle u,X\rangle\right]=\langle u,\mu_X\rangle$
    \item $\text{Var}\left(\langle u,X\rangle\right)=q_X(u)$.
\end{itemize}
De hecho, si tenemos una base ortonormal $(e_{j})_{j\geq1}$ de $\R^{d}$, entonces podemos escribir a $X$ de la siguiente manera:
\[
X=\sum_{j=1}^{d}\langle X,e_j\rangle e_j,  
\]
donde $X_j:=\langle X,e_j\rangle$, por definición, es una variable aleatoria Gaussiana, para cualquier $j\geq1$. De lo anterior, se sigue que, escribiendo $u\in \R^d$ como $u=\sum_{j=1}^{d}u_je_j$, se tiene que 
\begin{itemize}
    \item $\mu_X=\E\left[X_j\right]e_j$.
    \item $q_X(u)=\sum_{j=1}^{d}\sum_{i=1}^{d}u_iu_j \text{Cov}\left(X_i,X_j\right)$.
\end{itemize}
 \end{prop}

Se sigue de la proposición anterior la fórmula de la función característica de un vector gaussiano.
\begin{prop} 
 Sea $X$ un vector aleatorio gaussiano y $(e_j)_{j\geq1}$ una base ortonormal de $\R^{d}$. Entonces, al ser $\langle u,X\rangle\sim$ Normal$(\langle u,\mu_X\rangle,q_X(u))$, se tiene que %Si denotamos por $\E\left[X\right]:=\sum_{j=1}^{d}\E\left[X_j\right]e_j$, entonces 
 \[
 \E\left[e^{i\langle u,X\rangle}\right]=e^{i \langle u,\mu_X\rangle-\frac{1}{2}q_X(u)}, \qquad u\in \R^d.
 \]
 \end{prop}
Y en vista de las dos proposiciones anteriores, está la siguiente caracterización de variables aleatorias Gaussianas independientes.
\begin{prop} 
 Sea $X$ un vector aleatorio gaussiano en $\R^{d}$, y sea $(e_j)_{j\geq1}$ una base ortonormal de $\R^{d}$. Entonces $X_1,...,X_d$ son variables aleatorias independientes si y solo sí la matriz de covarianzas $\Sigma=(\text{Cov}\left(X_i,X_j\right))_{1\leq i,j\leq d}$ es diagonal. Equivalentemente, $X_1,...,X_d$ son independientes si y solo si la forma cuadrática $q_X$ está en forma diagonal en la base $(e_j)_{j\geq1}$.
 \end{prop}

 Pasamos ahora a estudiar procesos estocásticos formados por variables aleatorias gaussianas.
\begin{dfn} 
 Sea $T$ un conjunto arbitrario y sea $G=(G(t))_{t\in T}$ una colección de variables aleatorias indexadas por $T$. Decimos que $G$ es un proceso Gaussiano, o un campo aleatorio Gaussiano si para cualesquiera $t_1,...,t_k\in T$, se tiene que $X=(G(t_1),...,G(t_k))$ es un vector gaussiano, $k\geq1$. 
\end{dfn}

Equivalentemente, un proceso estocástico $(G(t))_{t\in T}$ es Gaussiano si cualquier combinación lineal finita de variables $G(t)$, $t\in T$ es Gaussiana, lo cual de acuerdo a nuestra exposición, también equivale a decir que las distribuciones finito-dimensionales de un proceso Gaussiano $(G(t))_{t\in T}$ están dictadas por medio de variables aleatorias gaussianas.

A menudo es importante saber cuando un proceso gaussiano es independiente de otro proceso gaussiano. Para ello, recordamos la definición de $\sigma$-álgebra generada por un conjunto de variables.

\begin{dfn} 
 Sea $H$ una familia de variables aleatorias definidas en un espacio de probabilidad $(\Omega, \F, \P)$. La $\sigma$-álgebra generada por $H$ es la $\sigma$-álgebra más pequeña en $\Omega$ tal que todas las variables $X\in H$ son medibles con respecto a dicha $\sigma$-álgebra. A tal estructura la denotamos como $\sigma(H)$.  
 \end{dfn}
Enunciamos ahora un importante resultado con respecto a la independencia de dos conjuntos de variables aleatorias gaussianas. Este resultado nos dice que, en cierto sentido, dos conjuntos de variables aleatorias Gaussianas son ortogonales si y solo si son independientes entre sí. 

\begin{prop}\label{Gaussi_indep} 
 Sean $G_1$, $G_2$ dos familias de variables aleatorias Gaussianas centradas (i.e. con media cero). Denotemos por $H_1$ y $H_2$ al subespacio lineal cerrado de $L^{2}(\P)$ generado por dichas variables. Entonces dichos espacios están formados por variables aleatorias Gaussianas centradas.

 Más aún, son equivalentes 
 \begin{itemize}
    \item Los subespacios $H_1$ y $H_2$ son ortogonales entre sí en $L^{2}(\P)$.
    \item Las $\sigma$-álgebras $\sigma(H_1)$ y $\sigma(H_2)$ son independientes.
 \end{itemize}
 Es importante recordar que esta propiedad es muy particular del contexto Gaussiano.
 \end{prop}
Ahora bien, dado un proceso Gaussiano $G$, tenemos dos funciones asociadas al mismo.
\begin{dfn} 
 Sea $G=(G(t))_{t\in T}$ un proceso gaussiano. Definimos las funciones de media y covarianza del proceso $G$ respectivamente como sigue.
 \begin{enumerate}
    \item $\mu(t):=\E\left[G(t)\right]$, \qquad para cualquier $t\in T$,
    \item $\Gamma(s,t):= \text{Cov}\left(G(s),G(t)\right)$, \qquad para cualesquiera $s,t\in T$.
 \end{enumerate}
 \end{dfn}
Dichas funciones con valores en $\R$ determina la colección de distribuciones finito-dimensionales del proceso. 
De hecho, gracias a las proposiciones anteriores es claro que para cualesquiera subíndices $(t_1,...,t_k)\subseteq T$, la ley del vector Gaussiano $X=(G(t_1),...,G(t_k))$ está determinada de manera única. 

Lo anterior pues $\mu_X:=(\E\left[G(t_1)\right],...,\E\left[G(t_k)\right])=(\mu(t_1),...,\mu(t_k))$ y $q_X$ la forma cuadrática asociada que está determinada por la matriz de covarianzas de $X$, está en términos de la función $\Gamma$ definida antes, a saber, $\Sigma_X=\left(\text{Cov}\left(G(t_i),G(t_j)\right)\right)_{1\leq i,j\le k}=\left(\Gamma(t_i,t_j)\right)_{1\le i,j\le k }$.

Es consecuencia de que la forma cuadrática $q_X$ para cualquier vector $X$ que componga una distribución finito-dimensional de $G$, sea no negativa definida y simétrica, el hecho de que la función de covarianzas $\Gamma$ sea no negativa definida. A saber, si $c:T\to \R$ es una función con soporte finito, entonces 
\[  
    \sum_{T\times T}^{}c(s)c(t)\Gamma(s,t)\geq0
\]
Lo valioso de conocer las funciones anteriores es que, al caracterizar completamente las distribuciones finito-dimensionales del proceso Gaussiano $G$, todo el proceso está caracterizado por medio de dichas funciones, por lo que basta conocer dichas funciones para construir un proceso Gaussiano. Este es el contenido del siguiente teorema.

\begin{prop} 
 Sean $\Gamma:T\times T\to \R$ y $\mu:T\to \R$, tales que $\Gamma$ es simétrica y no negativa definida. Entonces existe un proceso gaussiano $(G(t))_{t\in T}$ en un espacio de probabilidad $(\Omega, \F,\P)$ apropiado, tal que para cualesquiera $t\in T$, $G(t)\sim$ Normal$(\mu(t),\Gamma(t,t))$, con $\mu$ y $\Gamma$ las respectivas funciones de media y covarianza del proceso $G$.
 \end{prop}
 La demostración de lo anterior es una consecuencia del Teorema de Consistencia (o extensión) de Kolmogorov (ver Teorema 6.3 en \cite{gall2016brownian} ). Para terminar esta sección, a continuación presentamos un par de ejemplos de procesos Gaussianos construidos de esta forma.
\begin{ejem}[\textbf{El movimiento Browniano}]
Consideremos el caso en el que $T=[0,\infty)$, $\mu(t)=0$ para cualquier $t\geq0$ y $\Gamma(s,t)=s\wedge t=\min\{s,t\}$, para $s,t\geq0$. Claramente $\Gamma$ es simétrica, y además nótese que para cualesquiera $c:T\to\R$ función de soporte finito, se tiene que
\begin{align*}
\sum_{T\times T}^{}c(s)c(t)\Gamma(s,t)&=\sum_{s\in T}^{}\sum_{t\in T}c(s)c(t)(s\wedge t)\\
&=\sum_{s\in T}^{}\sum_{t\in T}c(s)c(t)\int_{0}^{\infty}\1_{[0,s]}(x)\1_{[0,t]}(x)dx\\
&=\int_{0}^{\infty}\sum_{s\in T}\sum_{t\in T}\1_{[0,s]}(x)c(s)\1_{[0,t]}(x)c(t)dx\\
&=\int_{0}^{\infty}\abs{\sum_{t\in T}\1_{[0,t]}(x)c(t)}^2dx\\
&\geq0,
\end{align*}
por lo que por la proposición anterior, existe en un espacio de probabilidad adecuado un proceso gaussiano que denotaremos por $(B(t))_{t\geq0}$, con función de medias 0 y función de covarianza $\Gamma(s,t)=s\wedge t$. Dicho proceso es el movimiento Browniano estándar.
 \end{ejem}
El siguiente ejemplo es de suma importancia y es parte esencial en el resto del texto.

\begin{ejem}[\textbf{El ruido blanco en $\R^{d}$}] 
Sea $T=\B(\R^{d})$ el conjunto de los borelianos en $\R^d$.
Dado que estamos hablando de subconjuntos de $\R^{d}$, cambiaremos la notación al indicar con letras mayúsculas a los elementos de $T$.
Consideremos nuevamente la función de medias $\mu(A):=0$, para cualquier $A\in \B(\R^{d})$ y ahora consideremos la función $\Gamma(A,B):=\lambda^{d}(A\cap B)$, donde $\lambda^{d}:\B(\R^{d})\to [0,1]$ es la medida de Lebesgue en $\R^{d}$.

 Claramente $\Gamma$ es una función simétrica, y además, para $c:T\to\R$ función de soporte finito, se tiene que 
 \begin{align*}
    \sum_{T\times T}^{}c(A)c(B)\Gamma(A,B)&=\sum_{A\in T}^{}\sum_{B\in T}c(A)c(B)\lambda^{d}(A\cap B)\\
    &=\sum_{A\in T}^{}\sum_{B\in T}c(A)c(B)\int_{\R^d}\1_{A}(x)\1_{B}(x)\lambda^{d}(dx)\\
    &=\int_{\R^{d}}\sum_{A\in T}\sum_{B\in T}\1_{A}(x)c(A)\1_{B}(x)c(B)\lambda^{d}(dx)\\
    &=\int_{\R^{d}}\abs{\sum_{A\in T}\1_{A}(x)c(A)}^2\lambda^{d}(dx)\\
    &\geq0,
    \end{align*}
por lo que existe un proceso Gaussiano que denotaremos por $(\dot{W}(A))_{A\in \B(\R^{d})}$ en un espacio de probabilidad adecuado, tal que su función de medias es $0$ y su función de covarianzas es $\Gamma(A,B)=\lambda^{d}(A\cap B)$. Dicho proceso estocástico es conocido como \textit{ruido blanco} en $\R^{d}$.

Observemos que, si $A\cap B=\varnothing$, entonces $\Gamma(A,B)=\text{Cov}\left(\W(A),\W(B)\right)=0$, por lo que al ser variables Gaussianas, estas son independientes según lo visto antes.

Se sigue que si $A,B\in \B(\R^d)$, entonces aprovechando que el proceso es Gaussiano centrado, tenemos que
\begin{align*}
    &\text{Cov}\left(\W(A\cup B)-\W(A)-\W(B)+\W(A\cap B),\W(A\cup B)-\W(A)-\W(B)+\W(A\cap B)\right)\\
    &=\text{Cov}\left(\W(A\cup B),\W(A\cup B)\right)+\text{Cov}\left(\W(A),\W(A)\right)+\text{Cov}\left(\W(B),\W(B)\right)\\
    &\quad +\text{Cov}\left(\W(A\cap B),\W(A\cap B)\right)-2 \text{Cov}\left(\W(A\cup B),\W(A)\right)-2 \text{Cov}\left(\W(A\cup B), \W(B)\right)\\
    &\quad +2 \text{Cov}\left(\W(A\cup B), \W(A\cap B)\right)+2 \text{Cov}\left(\W(A),\W(B)\right)-2 \text{Cov}\left(\W(A),\W(A\cap B)\right)\\
    &\quad -2 \text{Cov}\left(\W(B),\W(A\cap B)\right)\\
    &=\lambda^{d}(A\cup B)+\lambda^{d}(A)+\lambda^{d}(B)+\lambda^{d}(A\cap B)-2\lambda^{d}((A\cup B)\cap A)-2\lambda^{d}((A\cup B)\cap B)\\
    &\quad+2\lambda^{d}((A\cup B)\cap (A\cap B))+2\lambda^{d}(A\cap B)-2\lambda^{d}(A\cap (A\cap B))-2\lambda(B\cap(A\cap B))\\
    &=\lambda^{d}(A\cup B)+\lambda^d(A)+\lambda^{d}(B)+\lambda^{d}(A\cap B)-2\lambda^{d}(A)-2\lambda^{d}(B)+2\lambda^{d}(A\cap B)\\
    &\quad +2\lambda^{d}(A\cap B)-2\lambda^{d}(A\cap B)-2\lambda^{d}(A\cap B)\\
    &=\lambda^{d}(A\cup B)-\lambda^d(A)-\lambda^{d}(B)+\lambda^{d}(A\cap B)\\
    &=\lambda^{d}(A\cup B)-\lambda^{d}(A\cup B)\\
    &=0.
\end{align*}

por lo que la variable $\W(A\cup B)-(\W(A)+\W(B)-\W(A\cap B))$ tiene varianza cero. Luego,
\begin{equation}\label{Wdotmeasure}    
    \W(A\cup B)=\W(A)+\W(B)-\W(A\cap B) \qquad \P-\text{casi seguramente}.
\end{equation}


\end{ejem}
Lo anterior nos podría hacer pensar que el ruido blanco es una medida signada. Sin embargo, esto es falso (ver ejemplo 3.16 de \cite{Khoshnevisan2009}). No obstante, podemos utilizar dicho proceso como base para construir una integral.

\begin{ejem}[\textbf{El proceso isonormal}] 
Dado un ruido blanco $\left(\W(A)\right)_{A\in \B(\R^d)}$, nos gustaría definir $\W(h)$ para alguna función $h$ adecuada. Es bien conocido en teoría de la medida un mecanismo estándar para definir nuevos objetos a partir de objetos más sencillos, y luego extender por aproximación. Seguiremos esta maquinaria estándar a continuación.

Sea $A\in \B(\R^d)$. Definimos $\W(\1_A):=\W(A)$, y para cualesquiera $A_1,...,A_n\in \B(\R^{d})$ disjuntos, y constantes $c_1,...,c_n\in \R$,  definimos
\[
\W \left(\sum_{k=1}^{n}c_k\1_{A_k}\right):=\sum_{k=1}^{n}c_k\W(A_k)    
\]
Nótese que si existen dos representaciones distintas de una función simple, de acuerdo a \eqref{Wdotmeasure}, la variable de la definición anterior es consistente ya que habrá una igualdad $\P-$ casi seguramente. Por otro lado, dado que los conjuntos $A_1,...,A_n$ son disjuntos, las variables $\W(A_1),...,\W(A_n)$ son independientes entre sí. Luego, observamos que al ser centradas,
\begin{align*}
    \norm{\W \left(\sum_{k=1}^{n}c_k\1_{A_k}\right)}_{L^{2}(\P)}^2&=\text{Var}\left(\sum_{k=1}^{n}c_k\W(A_k)\right)\\
    &=\sum_{k=1}^{n}c_k^2 \E\left[\W^2(A_k)\right]\\
    &=\sum_{k=1}^{n}c_k^2 \Gamma(A_k,A_k)\\
    &=\sum_{k=1}^{n}c_k^2\lambda^{d}(A_k)\\
    &=\int_{\R^d}\sum_{k=1}^{n}c_k^2\1_{A_k}\lambda^d(dx)\\
    &=\norm{\sum_{k=1}^{n}c_k\1_{A_k}}^2_{L^{2}(\R^{d})},
\end{align*}
donde en la última igualdad utilizamos que, al ser $A_i\cap A_j=\varnothing$, se tiene que $\1_{A_i}\1_{A_j}=0$ para $i\neq j$. De lo anterior deducimos que la aplicación $f\longmapsto \W(f)$, que envía funciones simples en variables Gaussianas, preserva la norma de los respectivos espacios.

Es un resultado conocido en teoría de la medida que, si tenemos ahora una función $f\in L^{2}(\R^{d})$, existe una sucesión de variables aleatorias simples en $L^{2}(\R^d)$ tales que $f_n\to f$ en la norma de dicho espacio. Luego, la sucesión $(f_n)_{n\ge 1}$ forma una sucesión de Cauchy de funciones simples en $L^2(\R^{d})$. Dado que la norma se preserva para estas funciones entre los espacios $L^2$ según lo visto antes, la sucesión de variables $(\W(f_n))_{n\ge 1}$ es de Cauchy en $L^{2}(\P)$, por lo que al ser un espacio completo, existe una única variable en $L^{2}(\P)$, que denotaremos por $\W(f)$, tal que $\W(f_n)\longrightarrow \W(f)$ en norma $L^{2}(\P)$.

Consideramos así al espacio $L^{2}(\R^{d})$, y a la colección de variables aleatorias $(\W(f))_{f\in L^{2}(\R^{d})}$. A dicho proceso estocástico se le conoce como el \textit{proceso isonormal}. Una característica fundamental de dicho proceso es que la aplicación $A\mapsto\W(A)$ es una isometría, conocida como \textit{isometría de Wiener}. Al elemento $\W(f)$ para $f\in L^2(\R^{d})$ se le conoce como \textit{integral de Wiener de f}, y a menudo se denota como
\[
\W(f)=\int f W(dx).    
\] 
El proceso isonormal se puede también obtener vía su función de medias y su función de covarianzas. En efecto, tomando $T=L^{2}(\R^{d})$, haciendo nuevamente $\mu(f)=0$ para cualquier $f\in L^2(\R)$, y definiendo la función de covarianzas $\Gamma$ como
\[
\Gamma(f,g)=\int_{\R^d}fg \lambda^d(dx)=\langle f,g\rangle_{L^{2}(\R^{d})}, \qquad f,g\in L^2(\R^{d}),
\]
entonces el proceso Gaussiano resultante, que denotamos por $(\W(f))_{f\in L^{2}(\R^{d})}$ es el proceso isonormal que construimos antes vía un ruido blanco. 
 \end{ejem}

\subsection{Medidas martingala}

En esta parte del texto, estudiamos el concepto de medidas martingala. Dichos procesos estocásticos son una pierda angular en la construcción de la integral de Itô-Walsh, como veremos a continuación.

Recordemos que si tenemos un ruido blanco $(W(A))_{A\in \B(R^{d})}$, entonces para cualesquiera $A,B \in \B(\R^{d})$, se tiene la siguiente igualdad
\[
\W(A\cup B)=\W(A)+\W(B)-\W(A\cap B), \qquad \P-\text{casi seguramente.}   
\]
Sin embargo, como también se mencionó en la subsección pasada, a pesar de que esto nos puede hacer pensar en que el ruido blanco constituye una medida aleatoria signada, lo anterior es falso. No obstante, se tiene el siguiente resulado con respecto al mismo.
\begin{prop}\label{Finito_aditiv_ruido_blanco}
 El proceso $(\W(A))_{A\in \R^{d}}$ cumple las siguientes propiedades
 \begin{itemize}
    \item $\W(\varnothing)=0$, \qquad $\P-\text{casi seguramente,}$
    \item Para cualquier sucesión de conjuntos ajenos $(A_n)_{n\geq 1}\subseteq \B(\R^d)$, se cumple que 
    \[
    \W \left(\bigcup_{k=1}^\infty A_k\right)=\sum_{k=1}^{\infty}\W(A_k), \qquad \P-\text{casi seguramente,} 
    \]
 \end{itemize}
 en donde la suma infinita anterior converge en $L^2(\P)$. En otras palabras, el 
 ruido blanco es una medida signada, $\sigma$-finita, $L^2(\P)$-valuada.
\end{prop}
\begin{proof} 
   Es un resultado conocido de teoría de la medida que, si queremos probar que la aplicación anterior es una medida signada, y ya hemos probado que es finito-aditiva, entonces basta con tomarnos una sucesión decreciente de conjuntos $(A_n)_{n\geq1}\subseteq\B(\R^{d})$ tal que al intersecar todos los elementos obtengamos el conjunto vacío, y probar que $\W(A_n)\to 0$ en $L^{2}(\P)$ para concluir.

   En efecto, sea $(A_n)_{n\geq1}$ una sucesión como la descrita antes. Directamente por definición del ruido blanco, 
   \[
   \norm{\W(A_n)}_{L^2(\P)}^2=\E\left[\W^2(A_n)\right]=\Gamma(A_n,A_n)=\lambda^{d}(A_n)\xrightarrow[n\to\infty]{}0,    
   \]
   ya que al ser $\lambda^d$ una medida, se cumple la propiedad de continuidad con la sucesión $(A_n)_{n\geq1}$.

   Para concluir que la medida es $\sigma-$finita, dado que $\R^{d}$ puede ser cubierto con una sucesión numerable de conjuntos compactos, basta ver que para cualquier conjunto compacto $K\subseteq\R^{d}$, se tiene que $\norm{\W(K)}_{L^ {2}(\R^{d})}^2<\infty$. Pero esto es claro, ya que 
   \[
       \norm{\W(K)}_{L^ {2}(\R^{d})}^2=\E\left[\W^2(K)\right]=\lambda^{d}(K)<\infty,
   \]
   ya que los compactos son acotados en $\R^{d}$.
 \end{proof}  
 \begin{obs}
   Es importante distinguir el que el ruido blanco forme una medida $L^{2}(\P)$-valuada a que forme una medida signada aleatoria en los borelianos de $\R^d$. Esto es, nótese que la segunda propiedad de la proposición posee el cuantificador fuera del conjunto de probabilidad 1. Más específicamente, la proposición anterior \textit{no} está asegurando que 
   \[
   \P\left(\left\{\forall \ (A_n)_{n\geq1}\subseteq\B(\R^{d}), \ \ \W \left(\bigcup_{k=1}^\infty A_k\right)=\sum_{k=1}^{\infty}\W(A_k)\right\}\right)=1  
   \]
\end{obs}
Ahora, a menudo es posible considerar una dimensión como la dimensión temporal, de forma que se puede estudiar un ruido blanco definido en el espacio $[0,\infty)\times\R^{d}$. El hecho de pensar al intervalo $[0,\infty)$ como una dimensión temporal nos permite definir un nuevo proceso conocido como el \textit{proceso de ruido blanco}, $(W_t)_{t\geq0}$, definido como 
\[
W_t(A):=\W \left([0,t]\times A\right), \qquad A\in \B(\R^{d}).    
\]
Este es un proceso estocástico que 'evoluciona en el tiempo', pero a su vez, es un ruido blanco en $A$. Recordemos también que la noción de filtración como sucesión de $\sigma$-álgebras ordenadas con respecto a la contención concreta la idea de proceso que evoluciona en el tiempo. Definimos entonces la filtración del proceso de ruido blanco como sigue
\[
\F_t:=\sigma \left(\W([0,t]\times A):0\leq s\leq t, A\in \B(\R^{d})\right), \qquad t\geq0.    
\]
Claramente la familia $(\F_t)_{t\geq0}$ forma una filtración, y además el proceso $(W_t)_{t\geq0}$ es adaptado a dicha filtración.

Una propiedad muy interesante que posee el proceso $(W_t)_{t\geq0}$, visto como un proceso que evoluciona en el tiempo, es que es una medida aleatoria (en el sentido visto antes), y también una martingala. 

\begin{prop} 
Sea $(W_t(A))_{t\geq0, A\in \B(\R)}$ un ruido blanco en $[0,\infty)\times \R^{d}$. Entonces dicho proceso forma una 'medida martingala' en el siguiente sentido:
\begin{itemize}
   \item Para cualquier $A\in \B(\R^{d})$, se cumple $W_0(A)=0, \ \P-$casi seguramente.
   \item Si $t>0$, entonces $W_t$ es una medida signada $\sigma$-finita, $L^{2}(\P)$-valuada.
   \item Para cualquier $A\in \R^{d}$, el proceso $(W_t(A))_{t\geq0, A\in \B(\R)}$ es una martingala de media cero.
\end{itemize}
\end{prop}
\begin{proof} 
 Nótese que por definición, $W_0(A)=\W(\{0\}\times A)$, y dicha variable es tal que 
 \[
 \E\left[\W^2(\{0\}\times A)\right]=\Gamma(0\times A,0\times A)=\lambda{0}\cdot\lambda^{d}(A)=0,  
 \]
 por lo que dicha variable aleatoria es la variable 0 con probabilidad 1.

 Por otro lado, para $t>0$, el que $W_t:\B(\R^{d})\to L^{2}(\P)$ sea una medida signada $\sigma$-finita se sigue de la propiedad siguiente: para cualquier $A\in \B(\R^{d})$, 
 \[
 \E\left[\W([0,t]\times A)\right]=\Gamma([0,t]\times A, [0,t]\times A)=\lambda([0,t])\cdot\lambda^{d}(A)=t\lambda^{d}(A). 
 \] 
 Por lo tanto, si $A=\varnothing$, claramente $\W_t(\varnothing)=0$ con probabilidad 1, ya que $\Gamma([0,t]\times\varnothing,[0,t]\times \varnothing)=0$. Ahora, para mostrar la $\sigma$-aditividad y la $\sigma$-finitud se procede de manera análoga a lo hecho en la proposición \ref{Finito_aditiv_ruido_blanco}: primero, se prueba que la aplicación $W_t:\B(\R)\to L^{2}(\P)$ es finito aditiva, y posteriormente se generaliza. Para la parte de la $\sigma$-finitud se procede igual.

 Finalmente, para mostrar que para cualquier $A\in \B(\R^{d})$ el proceso $t\to W_t(A)$ es una martingala con respecto a la filtración $(\F_t)_{t\geq0}$, observamos que el proceso claramente es integrable (lo componen variables Gaussianas) y es adaptado a la filtración por definición. Resta probar que para $A\in \B(\R)$ fijo,
 \[
 \E\left[W_{s+t}(A)\lvert \F_t\right]=W_t(A), \qquad s,t\geq0  
 \]
 Para ello haremos uso de la proposición \ref{Gaussi_indep}. Consideramos a la familia de variables aleatorias $C_1=\left\{W_u(A):u\leq t\right\}$ y
 $C_2=\left\{W_{t+s}-W_t(A):s>0 \right\}$, para $t\geq0$. Notemos que para $0\le u\le t$,
 \begin{align*}
   \E\left[(W_{t+s}(A)-W_t(A))W_u(A)\right]&=\Gamma([0,t+s]\times A, [0,u]\times A)-\Gamma([0,t]\times A,[0,u]\times A)\\
   &=\lambda([0,t+s]\cap[0,u])\cdot\lambda^{d}(A)-\lambda([0,t]\cap[0,u])\cdot\lambda^{d}(A)\\
   &=\lambda([0,u])\cdot\lambda^{d}(A)-\lambda([0,u])\cdot\lambda^{d}(A)\\
   &=0.
 \end{align*}
 Por lo tanto, para cualesquiera dos variables $\xi_1$ y $\xi_2$ en $C_1$ y $C_2$ respectivamente, se tiene que $\langle \xi_1,\xi_2\rangle_{L^2(\P)}= \E\left[\xi_1\xi_2\right]=0$, esto es, son ortogonales. Por lo tanto, si $H_1$ y $H_2$ representan la cerradura del subespacio lineal generado por las familias $C_1$ y $C_2$ respectivamente, dichos subespacios son ortogonales.
 
 Por lo tanto, por la proposición mencionada antes, las $\sigma$-álgebras generadas por las familias $C_1$ y $C_2$ son independientes. En particular, para $s,t\geq0$, $\F_t\subseteq \sigma(C_1)$, por lo que al ser $W_{t+s}-W(t)$ una variable $\sigma(C_2)$ medible, se sigue que 
 \[
     \E\left[W_{t+s}(A)|\F_t\right]=\E\left[W_{t+s}(A)-W_{t}(A)+W_t(A)|\F_t\right]=\E\left[W_{t+s}(A)-W_t(A)\right]+W_t(A)=W_t(A),
     \]
     por lo que la propiedad de martingala se satisface.  
\end{proof}

El proceso de ruido blanco es entonces un ejemplo de medida martingala, no obstante, este concepto se puede definir de manera mucho más general, como vemos a continuación.

\begin{dfn} 
Sea $(\F_t)_{t\geq0}$ una sucesión de $\sigma$-álgebras continuas por derecha. Un proceso $(M_t(A))_{t\geq0, A\in \B(\R^{d})}$ es una medida martingala con respecto a dicha filtración si 
\begin{itemize}
   \item $M_0(A)=0$ $\P$- casi seguramente.
   \item Si $t>0$, entonces $M_t$ es una medida signada, $\sigma$-finita, $L^{2}(\P)$-valuada.
   \item Para cualquier $A\in \B(\R^{d})$, se tiene que $(M_t(A))_{t\geq0}$ es una martingala con respecto a la filtración $(\F_t)_{t\geq0}$ de media 0.
\end{itemize}
\end{dfn}
Por construcción, el proceso del ruido blanco es un primer ejemplo de medida martingala. La razón de introducir este concepto, es que este conjunto de procesos forman una clase adecuada de integradores. A continuación, definimos un concepto relacionado íntimamente con las medidas martingala, y que es en cierto sentido una generalización del proceso de covariación para dos martingalas en el caso unidimensional.

\begin{dfn} 
 Sea $M$ una medida martingala. El funcional de covarianza de $M$ se define como
 \[
 \overline{Q}_t(A,B):= \langle M_\cdot(A),M_\cdot (B)\rangle_t, \qquad t\geq 0, A, B \in \B(\R^{d}). 
 \]
 \end{dfn}
 Dado que para $M$ una medida martingala, si $A$ y $B \in \B(\R^{d})$ se tiene que $(M_t(A))_{t\geq0}$ y $(M_t(B))_{t\geq0}$ son martingalas, entonces el proceso de covariación de ambas martingalas evaluado en $t\geq0$ coincide con el funcional de covarianza evaluado en los borelianos $A$ y $B$, y en $t\geq0$.

 Gracias a lo anterior, se deducen directamente de las propiedades de la covariación entre dos martingalas las siguientes propiedades del funcional de covarianza de una medida martingala.

 \begin{prop} 
  Sea $M$ una medida martingala y denotemos por $\overline{Q}$ a su funcional de covarianza. Entonces $\overline{Q}$ cumple que
  \begin{itemize}
   \item $\overline{Q}_t(A,B)=\overline{Q}_t(B,A)$, \ $\P$- casi seguramente.
   \item Si $B\cap C=\varnothing$, entonces $\overline{Q}_t(A,B\cup C)=\overline{Q}_t(A,B)+\overline{Q}_t(A,C)$, \ $\P$- casi seguramente.
   \item $\abs{\overline{Q}_t(A,B)}^2\leq \overline{Q}_t(A,A)\overline{Q}_t(B,B)$ \ $\P$-casi seguramente, y 
   \item $t\mapsto\overline{Q}_t(A,A)$ es $\P$- casi seguramente no decreciente.
  \end{itemize}
  \end{prop}

Con el concepto de funcional de covarianza, podemos definir una función aleatoria de conjuntos en $\B(\R^{d})\times \B(\R^{d})\times \B([0,\infty))$, que denotaremos $Q$, como sigue:
\[
   Q(A\times B\times(s,t]):=\overline{Q}_t(A,B)-\overline{Q}_s(A,B), \qquad A,B\in \B(\R^{d}), \ t\geq s\geq 0
   \]
Y extendemos dicha función a uniones disjuntas de elementos (rectángulos)
$(A_i\times B_i\times(s_i,t_i])$ para $1\leq i \leq m$ y $A_i,B_i\in \B(\R^{d})$, $(s_i,t_i]\in \B(\R^{d})$ como 
\[
Q \left(\bigcup_{i=1}^{n}(A_i\times B_i\times(s_i,t_i])\right):=\sum_{i=1}^{n}Q(A_i\times B_i\times (s_i,t_i]).
\]
  
Nuestro objetivo es extender la función aleatoria de conjuntos $Q$ a conjuntos más generales. El problema es que en general no es posible hacerlo. No obstante, con estas dos herramientas teóricas, podemos definir el concepto de \textit{worthiness} de una medida martingala, introducido por Walsh en $\cite{Walsh_J.B_Introduction_to_SPDEs}$.
Dicho concepto es de carácter técnico, pero será fundamental en la construcción de la integral de Itô-Walsh al permitirnos extender la definición de $Q$ a conjuntos más generales.


\begin{dfn} 
Sea $M$ una medida martingala. Decimos que $M$ es \textit{worthy} (digna) si existe una medida $\sigma$-finita $K:\B(\R^{d})\times \B(\R^{d})\times \B([0,\infty))\times \Omega \to [0,1]$ tal que para cualesquiera $A,B\in \B(\R^{d})$, $C\in \B([0,\infty))$ y $\omega \in \Omega$, 
\begin{itemize}
   \item $A\times B\mapsto K(A\times B \times C,\omega)$ es no negativa definida y simétrica,
   \item $\{K(A\times B \times (0,t])\}_{t\geq0}$ es un proceso predecible para cualesquiera $A, B \in \B(\R^{d})$,
   \item Para cualesquiera dos conjuntos compactos $A,B \in \B(R^{d})$, y $t>0$, 
   \[
   \E\left[K(A\times B \times (0,t])\right]<\infty,    
   \]
   \item Para cualesquiera $A,B\in \B(\R^{d})$ y $t>0$,
   \[
   \abs{Q(A\times B \times (0,t])}\leq K(A\times B \times (0,t]) \qquad \P-\text{ casi seguramente.}   
   \]
\end{itemize}
Como es usual en probabilidad, la dependencia en $\omega$ es omitida en la notación. Cuando dicha medida $K$ existe, decimos que es una \textit{medida dominante} para $M$.
\end{dfn}


\subsection{Definición de la Integral}
En este punto tenemos suficiente teoría para definir la integral de Itô-Walsh con respecto a medidas martingala. Conforme vayamos desarrollando la construcción, vamos a ir notando cierta familiaridad con la construcción de la integral de Lebesgue y la integral de Itô.

\begin{dfn} 
Una función $f:\R^{d}\times [0,\infty)\times \Omega\to \R$ se dice que es \textit{elemental} si es de la forma 
\[
   f(x,t,\omega)=X(\omega)\1_{(a,b]}(t)\1_A(x) 
\]
donde $X$ es una variable aleatoria acotada y $\F_a$-medible, y además $A\in \B(\R^{d})$. 

Definimos la integral de Itô-Walsh para una función elemental de la forma anterior como 

\[
(f\cdot M)_t(B)(\omega):= X(\omega)\left[M_{t\wedge b}(A\cap B)-M_{t\wedge a}(A\cap B)\right](\omega)  
\]
\end{dfn}

Como una primera propiedad, tenemos la siguiente proposición
\begin{prop} 
Sea $f$ una función elemental y $M$ una medida martingala. Entonces $(f\cdot M)$ es nuevamente una medida martingala. En particular, la integral de funciones elementales con respecto a martingalas se vuelve una forma de construir nuevas medidas martingala.
\end{prop}
\begin{proof} 
 Pendiente 
\end{proof}
\begin{ejem} 
Sea $\W$ un ruido blanco en $\R^{d}\times[0,\infty)$ y consideremos a $(W_t(A))_{t\geq0,A\in \B(\R^d)}$ el proceso de ruido blanco. Ya vimos que dicho proceso forma una medida martingala. Sea $f$ una función elemental. Entonces $(f\cdot W)$ es una medida martingala.
\end{ejem}

El siguiente paso en la construcción de la integral es extender la definición a funciones simples. 

\begin{dfn} 
   Decimos que una función $f$ es una función simple si existen $c_1,...,c_n$ constantes reales, y $f_1,...,f_n$ funciones elementales, tales que 
   \[
     f=\sum_{k=1}^{n}c_kf_k.  
   \] Denotaremos por $\mathscr{S}$ el conjunto de las funciones simples. Definimos la integral de Itô-Walsh para funciones simples como 
   \[
   (f\cdot M)_t(B):=\sum_{j=1}^{k}c_j(f_j\cdot M)_t(B)    
   \]
 \end{dfn}
 \begin{prop} 
  La definición anterior es consistente, esto es, no depende de la representación de $f$ en términos de funciones simples.
  \end{prop}
Resulta ser que una clase adecuada de funciones integradoras son las funciones $f$ que son predecibles. Esto es, aquella $\sigma$-álgebra que es generada por todas las funciones simples. Dicha $\sigma$-álgebra la denotamos por $\mathscr{P}$

En este punto, deseamos extender la definición de la integral para funciones más generales, definidas sobre conjuntos no necesariamente rectangulares. Es en este punto en donde entra en juego la función $Q$.  Dicha función es crucial, según se ve en el siguiente 

\begin{prop} 
Sea $f\in \mathscr{S}$ y supongamos que $M$ es una medida martingala digna. Entonces 
\[
\E\left[((f\cdot M)_t(B))^2\right]=\E\left[\int_{B\times B\times (0,t]}f(x,t)f(y,t)Q(dx,dy,dt)\right].   
\]
\end{prop}

\begin{proof} 
   Primero debemos de hacer la demostración para funciones elementales, y posteriormente para funciones simples.
 \end{proof}
Nótese en la proposición anterior que estamos integrando sobre un rectángulo del estilo $B\times B\times (0,t]$, y la idea es extender la integral a regiones más generales. Para ello, necesitamos extender la definición de $Q$ a dichas regiones, tarea que como anticipamos antes, es posible gracias al concepto de martingala digna.

De hecho, si $M$ es una martingala digna, $Q_M$ puede ser extendida a una medida en todo $\B(\R^{d})\times \B^{\R^{d}}\times [0,\infty)$. 

\begin{ejem} 
Sea $\W$ un ruido blanco en $\R^{d}\times[0,\infty)$ y consideremos al proceso de ruido blanco asociado al mismo. Entonces dicho proceso forma una medida martingala digna con medida dominante $K(A\times B \times C):=\lambda^{d}(A\cap B)\lambda(C)$.
\end{ejem}

Tenemos a continuación una proposición con respecto a las medidas martingala dignas y la integral de Itô-Walsh de funciones simples.

\begin{prop} 
Sea $M$ una medida martingala digna y $f$ una función simple. Entonces $(f\cdot M)$ es una medida martingala digna. 
Mas aún, si $Q_N$ y $K_N$ representan al funcional de covarianza y a la medida dominante de una medida martingala digna $N$, entonces 
\begin{align*}
   &Q_{f\cdot M}(dx \ dy \ dz)=f(x,t)f(y,t)Q_M(dx \ dy \ dt)\\
   &K_{f\cdot M}(dx \ dy \ dt)=\abs{f(x,t)f(y,t)}K_M(dx \ dy \ dt). 
\end{align*}
\end{prop}
\begin{proof} 
  La prueba nuevamente va primero por funciones elementales y luego por funciones simples. 
 \end{proof}

A partir de ahora, solo estaremos interesados en el caso cuando la variable temporal está en algún intervalo finito $(0,T]$. Siguiendo la maquinaria estándar para definir nuevos objetos a partir de aproximaciones, definimos ahora una norma apropiada para dicho propósito.

\begin{dfn} 
Sea $M$ una medida martingala digna. Supongamos que $K_M$ es su medida dominante. Sea $f\in \mathscr{P}$ una función en el conjunto de las funciones que son predecibles (es decir, que son medibles con respecto a la $\sigma$-álgebra $\mathscr{P}$ generada por las funciones simples). Definimos la norma $\norm{\cdot}_M$ como sigue 

\end{dfn}
\[
   \norm{f}_M^2:=\E\left[\int_{\R^{d}\times\R^{d}\times (0,T]} \abs{f(x,t)f(y,t)}K_M(dx \ dy \ dt)\right]
\]

Observamos que dicha norma está bien definida. Denotamos por $\mathscr{P}_M$ al conjunto de todas las funciones $f$ predecibles tales que $\E\left[\norm{f}_M\right]<\infty$.

\begin{prop} 
La norma anteriormente definida en efecto es una norma, y el conjunto $\mathscr{P}_M$ es completo con esta norma.
\end{prop}
\begin{proof} 
  pendiente 
 \end{proof}
A continuación un resultado esencial de aproximación, el cual se remite directamente a \cite{Walsh_J.B_Introduction_to_SPDEs} para su demostración.

\begin{teo} 
El conjunto $\mathscr{S}$ es denso en $\mathscr{P}_M$.
\end{teo}
Lo anterior junto con la proposición 5.18 (referenciar) nos dice que 
\[
\E\left[(f\cdot M)_t^{2}(B)\right]\leq \norm{f}_M^2,\qquad \text{ para } t\in (0,T], f\in \mathscr{S}, B\in \B(\R^d).
\]
Luego, gracias a la cota anterior, si $(f_m)_{m\geq1}$ es una sucesión de Cauchy en $(\mathscr{S},\norm{\cdot}_M)$, entonces la sucesión $\left((f_m\cdot M)_t(B)\right)_{m\geq1}$ es una sucesión de Cauchy en $L^{2}(\P)$. Dado que ambos espacios son completos, se sigue la existencia de objetos en $\mathscr{P}$ y $L^{2}(\P)$, que denotamos por $f$ y $(f\cdot M)_t(B)$ respectivamente, de tal forma que 
\[
f_m\xrightarrow[m\to\infty]{\norm{\cdot}_M}f \quad \ent \quad (f_m\cdot M)_t(B) \xrightarrow[m\to\infty]{L^{2}(\P)} (f\dot M)(B).   
\]

De lo anterior, concluimos con el siguiente teorema.

\begin{teo} 
 Sea $M$ una medida martingala digna. Entonces para cualquier $f\in \mathscr{P}_M$, la integral $(f\cdot M)$ es una medida martingala digna qe satisface la proposición 5.23 (referenciar). Más aún, para cualquier $t\in (0,T]$ y $A,B\in \B(\R^{d})$, 
 \[
 \left\langle (f\cdot M)(A),(f\cdot M)(B)\right\rangle _t= \int_{A\times B\times (0,t]}f(x,s)f(y,s)Q_M(dx \ dy \ ds),
 \]
 y además se tiene la siguiente cota en $L^{2}(\P)$.
 \[
 \E\left[(f\cdot M)^{2}_t(B)\right] \leq \norm{f}_M^{2}.
 \]
 \end{teo}
 De hecho, es posible extender la cota en $L^2(\P)$ a una cota en $L^{p}(\P)$, creando así una desigualdad del tipo Burkholder.
 \begin{teo}[\textbf{Desiguadades de Burkholder}]
  Sea $M$ una medida martingala digna. Entonces para cualquier $p\geq2$ existe una constante $c_p\in (0,\infty)$ tal que para cualquier $f$ predecible y cualquier $t>0$, 
  \[
  \E\left[\abs{(f\cdot M)_t(B)}^p\right]\leq c_p \E\left[\left(\int_{\R^{d}\times \R^{d}\times (0,T]}\abs{f(x,t)f(y,t)}K_M(dx \ dy \ dt)\right)^{p/2}\right] 
  \]
  \end{teo}
  Con esto terminamos la construcción de la integral de Itô-Walsh. Ya nos es posible hablar de integrales con respecto a un ruido blanco $\W$, pues sabemos que este proceso en $\R^{d}\times [0,\infty)$ induce una medida martingala digna, y por lo tanto la noción de integral está bien definida para funciones aleatorias adecuadas. 

\section{Formulación Mild de una SPDE. Existencia y unicidad de las soluciones}
Una vez que tenemos construida la teoría de la integral de Itô-Walsh, podemos pasar darle un significado riguroso a una ecuación diferencial parcial estocástica, así como definir con claridad qué significa resolverla. Como vimos en la introducción del capítulo, la heurística para llegar a un problema en cierto sentido equivalente a aquél que involucra derivadas sigue de cerca las ideas de las soluciones débiles en ecuaciones diferenciales parciales clásicas. Concretamente, el uso de integración por partes para lograr el objetivo es clave. 

Consideremos la ecuación diferencial estocástica formal dada por 
\[
\begin{cases}
   \partial_tu(x,t)=\partial_{xx}u(x,t)+f(u(x,t))\W, & \text{ si } t>0, x\in [0,L]\\
   \partial_xu(0,t)=\partial_xu(L,t)=0, & \text{ si } t>0,\\
   u(x,0)=u_0(x), & \text{ si } x\in [0,L].\\
\end{cases}
\]
donde $\W$ es un ruido blanco con respecto a alguna friltración $(\F_t)_{t\geq0}$, y $u_0:[0,L]\to\R$ es una función determinista, medible y acotada, mientras que $f:\R\to\R$ es una función globalmente Lipschitz y acotada.

Si formalmente multiplicamos el problema por una función $\phi \in C^{\infty}(\R)$ tal que $\phi(0)=\phi(L)=0$ e integramos con respecto al tiempo y posteriormente con respecto al espacio, obtendremos que el problema anterior se puede reformular como 

\begin{align*}
   \int_{0}^{L}u(x,t)\phi(x)dx-\int_{0}^{L}u_0(x)\phi(x)dx&=\int_0^{t}\int_0^{L}\partial_{xx}u(x,s)\phi(x)dxds\\
   &+\int_{0}^{t}\int_{0}^{L}f(u(x,s))\phi(x)W(dx \ ds).    
   \end{align*}
Es destacable que ahora tenemos una ecuación integral estocástica en donde la integral con respecto al ruido blanco cobra total sentido. No obstante, lo anterior aún no puede ser tratado de manera rigurosa, ya que $u$ al ser aleatorio no necesariamente será una función derivable. Por lo tanto, si de manera formal utilizamos integración por partes, al ser $\phi$ una función que se anula en la frontera, la integral de la parcial de $u$ multiplicado por $\phi$ se transforma en
\[
   \int_{0}^{t}\int_{0}^{L}\partial_{xx}u(x,s)\phi(x)dx ds=\int_{0}^{t}\int_{0}^{L}u(x,s)\phi''(x)dx ds,
\]
por lo que sustituyendo la expresión anterior en la reormulación de nuestro problema, obtenemos una ecuación integral estocástica. Tal ecuación integral tiene completo sentido, y dado que es una reformulación de nuestro problema inicial pero que puede tratarse de manera rigurosa, podemos considerar que una manera de resolver nuestro problema inicial, es resolver la reformulación integral anterior. Esta es conocida como la solución mild de nuestro problema anterior.

\begin{dfn} 
Decimos que $u$ es una solución \textit{mild} del problema inicial anterior, si existe un proceso estocástico $u(x,t)$ tal que para cualquier función $\phi\in C^{\infty}([0,L])$, donde $\phi'(0)=\phi'(L)=0$, se tiene la igualdad 

\begin{align*}
   \int_{0}^{L}u(x,t)\phi(x)dx-\int_{0}^{L}u_0(x)\phi(x)dx=\int_{0}^{t}\int_{0}^{L}u(x,s)\phi''(x)dx ds+\int_{0}^{t}\int_{0}^{L}f(u(x,s))\phi(x)W(dx \ ds).  
\end{align*}
\end{dfn}

Es conocida teoría que resuelve la ecuación diferencial estocástica del calor. En particular, se tiene el siguiente resultado.

\begin{teo} 
La ecuación estocástica del calor, con función $f$ Lipschitz y acotada, tiene una única solución $\P$- casi seguramente, que satisface lo siguiente para cualquier $T>0$.
\[
\sup_{0\leq x\leq L}\sup_{0\leq t \leq T}\E\left[\abs{u(x,t)}^2\right]<\infty.   
\]
\end{teo}
Este es el resultado principal del capítulo. Y para su prueba, se requiere el siguiente lema bastante conocido en el ámbito de las ecuaciones diferenciales.

\begin{teo}[\textbf{Lema de Gronwall}] 
Supongamos $\phi_1,\phi_2,...:[0,T]\to[0,+\infty)$ son medibles y no decrecientes. Supongamos también que existe una constante $A\in \R$ tal que para cualquier entero $n\geq1$ y para cualquier $t\in[0,T]$,
\[
\phi_{n+1}(t)\leq A\int_{0}^{t}\phi_n(s)ds.   
\]
Entonces 
\[
\phi_n(t)\leq \phi_1(T)\frac{(At)^{n-1}}{(n-1)!}, \qquad \text{ para cualquier }n\geq1, \text{ y } t\in[0,T].   
\]
\end{teo}
Finalmente, el siguiente teorema nos muestra que no solamente hay existencia y unicidad de las solución a la ecuación estocástica del calor, sino que también existe una modificación continua de la misma.

\begin{teo} 
Sea $\Tilde{u}(x,t)$ la solución única $\P$- casi seguramente de la ecuación estocástica del calor. Entonces existe una modificación $u(x,t)$ de dicha solución cuyas trayectorias son continuas.
\end{teo}

Retomaremos la teoría desarrollada hasta aquí sobre ecuaciones diferenciales parciales estocásticas cuando lleguemos al capítulo 4, en donde estudiamos convergencia de las densidades de los promedios espaciales de la solución a la ecuación estocástica del calor. 


% \begin{itemize}
%     \item Planteamiento de la ecuación integral como una versión débil del problema planteado. Definición de la solución Mild utilizando funciones de Green.
%     \item Existencia de una única solución bajo condiciones Lipschitz. Argumento usando Lema de Gronwall para unicidad. Argumento usando iteraciones de Piccard para la existencia.
% \end{itemize}

\chapter{Elementos de Cálculo de Malliavin}% y Método de Stein}
En este capítulo se revisa material esencial de cálculo de Malliavin. Esta disciplina nació a finales de la década de 1970 a partir de los trabajos de Paul Malliavin. La intención de Malliavin era hallar una prueba probabilista del teorema de Hörmander sobre el efecto regularizante de las soluciones de cierta ecuación diferencial estocástica, puesto que Lars Hörmander en 1967 encontró una prueba que utilizaba herramientas puramente analíticas. Con dicha motivación inicial, en las últimas décadas el cálculo de Malliavin se ha convertido en una poderosa herramienta matemática para estudiar problemas diversos en probabilidad. 

En palabras de Nourdin y Peccati (ver prefacio de \cite{Nourdin_Peccati_2012}), el cálculo de Malliavin se puede entender como un cálculo diferencial en infinitas dimensiones, cuyos operadores actúan sobre funcionales de procesos Gaussianos generales, y que en conjunto con el método de Stein, se obtiene una amalgama poderosa para deducir teoremas tipo límite central, así como tasas de convergencia explícitas para los mismos. 

Tres operadores son esenciales en esta área: la derivada de Malliavin $D$, el operador de divergencia $\delta$ (o integral de Skorokhod) y el generador infinitesimal. El propósito de este capítulo es estudiar el material nuclear correspondiente a estos operadores.
\section{Cálculo de Malliavin en el caso unidimensional}
Siguiendo de cerca la exposición hecha por \cite{Nourdin_Peccati_2012}, comenzamos el estudio del tópico por el caso unidimensional. Aquí es más sencillo observar propiedades de estos objetos, los cuales se generalizan a dimensiones infinitas en la siguiente sección.

\subsection{La derivada de Malliavin.}

El operador con el que comenzamos nuestro estudio es la derivada de Malliavin. La idea es definir la derivada de variables aleatorias del tipo $f(N)$, donde $f$ es una función determinista, y $N$ es una variable aleatoria con distribución Normal estándar.

En el contexto unidimensional, la definición de este operador se realiza utilizando ideas similares a aquellas que nos llevan a la definición de derivada débil en los espacios de Sobolev. Dichas ideas consisten, a grandes rasgos, en generalizar la noción de derivada a objetos más generales que las funciones diferenciables en el sentido usual. Para ello primero buscamos una clase de funciones diferenciables con propiedades nobles pero que a su vez sea suficientemente rica, y posteriormente extendemos el operador en algún sentido.

Dado que buscamos definir la derivada de funciones de variables normales estándar, es natural definir el operador derivada en un subconjunto de los espacios $L^q(\R,\B(\R))$ con $q\in [1,\infty)$, pero dotados de la medida gaussiana estándar. 

Dicho lo anterior, denotemos por $\gamma:\B(\R)\to [0,1]$ a la medida mencionada, a saber, 
\[
\gamma(A)=\int_A\frac{1}{2\pi}e^{-\frac{x^2}{2}}dx.    
\]
Consideremos el espacio $L^{q}(\R,\B(\R),\gamma(dx))$, con $q\in [1,\infty)$, que durante esta subsección denotaremos simplemente como $L^{q}(\gamma)$. En este espacio, se valen algunas propiedades resultan de gran utilidad. Tal es el caso del siguiente lema.

\begin{lema}\label{lema1}
Sea $f:\R\to\R$ una función absolutamente continua con respecto a la medida de Lebesgue y tal que $f'\in L^{1}(\gamma)$. Entonces la función $h:\R\to\R$ dada por $h(x)=xf(x)$ está en $L^1(\gamma)$ y se cumple que 
\[
\int_\R xf(x)d\gamma(x)=\int_\R f'(x)d\gamma(x).
\]
\end{lema}
\begin{proof} 
  Supongamos primero que $f(0)=0$. Probamos que $h\in L^{1}(\gamma)$. En efecto,  
  \begin{align*}
   \int_{-\infty}^{\infty}|x||f(x)|d\gamma(x)&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty\abs{\int_0^{x}f'(y)dy}|x|e^{-\frac{x^2}{2}}dx\\
   &\leq \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \left(\int_0^{x}\abs{f'(y)}dy\right) \ |x|e^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \left(\int_x^{0}\abs{f'(y)}dy\right) \ (-x)e^{-\frac{x^2}{2}}dx\\
   & \ \ \  +\frac{1}{\sqrt{2\pi}}\int_{0}^\infty \left(\int_0^{x}\abs{f'(y)}dy\right) \ xe^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \int_{-\infty}^{y}\abs{f'(y)}(-x)e^{-\frac{x^2}{2}}dx \ dy+\frac{1}{\sqrt{2\pi}} \int_{0}^{\infty}\int_{y}^\infty \abs{f'(y)}xe^{-\frac{x^2}{2}}dx \ dy\\
   &=\int_{-\infty}^0 \abs{f'(y)}\frac{1}{\sqrt{2\pi}}\left(e^{-\frac{x^2}{2}}\Big|_{-\infty}^{y}\right) dy + \int_{0}^{\infty}\abs{f'(y)}\frac{1}{\sqrt{2\pi}}\left(-e^{-\frac{x^2}{2}}\Big|_{y}^{\infty}\right)dy\\
   &=\int_{-\infty}^{\infty}|f'(y)|d\gamma(y)<\infty,
  \end{align*}
  en donde hemos hecho uso del teorema de Fubini en la tercera igualdad. Por lo tanto, la función $h(x)=xf(x)$ en efecto está en $L^1(\gamma)$. Realizando cuentas similares, obtenemos que 
  \begin{align*}
   \int_{-\infty}^{\infty}xf(x)d\gamma(x)&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \left(\int_0^{x}f'(y)dy\right) xe^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \left(\int_x^{0}f'(y)dy\right) \ (-x)e^{-\frac{x^2}{2}}dx\\
   & \ \ \  +\frac{1}{\sqrt{2\pi}}\int_{0}^\infty \left(\int_0^{x}f'(y) dy\right) \ xe^{-\frac{x^2}{2}}dx\\
   &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 \int_{-\infty}^{y}f'(y)(-x)e^{-\frac{x^2}{2}}dx\ dy+\frac{1}{\sqrt{2\pi}} \int_{0}^{\infty}\int_{y}^\infty f'(y)xe^{-\frac{x^2}{2}}dx\ dy\\
   &=\int_{-\infty}^0 f'(y)\frac{1}{\sqrt{2\pi}}\left(e^{-\frac{x^2}{2}}\Big|_{-\infty}^{y}\right) dy + \int_{0}^{\infty}f'(y)\frac{1}{\sqrt{2\pi}}\left(-e^{-\frac{x^2}{2}}\Big|_{y}^{\infty}\right)dy\\
   &=\int_{-\infty}^{\infty}f(y)d\gamma(y),
  \end{align*}
  como queríamos. Para el caso en el que $f(0)\neq 0$, notamos que 
\end{proof}
En la búsqueda de la generalización de la derivada a funciones más generales, definimos a la clase de Schwarz $\mathcal{S}$ como el conjunto de funciones en $C^{\infty}(\R,\R)$ tales que ella y sus derivadas tienen crecimiento a lo más polinomial, es decir,
\[
\mathcal{S}:=\left\{f\in C^{\infty}(\R) : \forall p\in \{0,1,2,...\} \ \exists n\geq1 \text{ tal que }\lim_{x\to\pm\infty}\frac{|f^{(p)}(x)|}{1+|x|^{n}}<\infty \right\}.    
\]
Observamos que este conjunto dotado de la restricción correspondiente de la norma $L^q(\gamma)$ forma subespacio lineal de $L^{q}(\gamma)$, puesto que las combinaciones lineales de elementos infinitamente derivables vuelven a ser infinitamente derivables, y las combinaciones lineales también son compatibles con los límites. Es de destacar que este espacio es suficientemente amplio como para poseer a los polinomios. Más aún, este último conjunto es denso en $L^{q}(\gamma)$ para cualquier $q\in [1,\infty)$. Esto es consecuencia del siguiente lema.

\begin{lema} 
Para cualquier $q\in [1,\infty)$, el conjunto de los monomios de la forma $\{x^n: n\geq0\}$ genera un subespacio denso del espacio $L^{q}(\gamma)$. En particular, la clase de Schwarz $\S$ forma un subconjunto denso de $L^q(\gamma)$.
\end{lema}
\begin{proof} 
   Un corolario del teorema de Hann-Banach caracteriza la propiedad de ser un subespacio lineal denso de un espacio $L^{q}$. En nuestro caso basta con probar que para cualquier elemento $g \in L^{p}(\gamma)$, 
   \[
   \int_\R g(x)x^{n} \ d\gamma(x)=0, \ \ \forall n\geq0  \quad \ent \quad g=0 \ \ \gamma- \text{casi en todas partes},
   \]
   donde $p\in (1,\infty]$ es el exponente conjugado de $q$. Sea pues $g\in L^{p}(\gamma)$ y supongamos que para cualquier $n\geq0$, el antecedente de la implicación anterior es válido. Usaremos un argumento que involucra la transformada de Fourier. Notamos primero que la transformada de $g(x)e^{-\frac{x^2}{2}}$ existe. En efecto, por la desigualdad de Hölder, para $t\in \R$ arbitrario,
      \begin{align*}
         \int_{\R}|g(x)e^{-\frac{x^2}{2}}e^{itx}|dx&\leq \int_{\R}|g(x)|e^{-\frac{x^2}{2}}dx\\
         &=\int_{\R}|g(x)|d\gamma(x)\\
         &\leq\norm{g}_{L^{p}(\gamma)}\left(\int_{\R}d\gamma(x)\right)^{\frac{1}{q}}\\
         &=\norm{g}_{L^{p}(\gamma)}<\infty.
      \end{align*}
   
   Por otro lado, para cualquier $x\in \R$,  
   \[
   \abs{g(x)\sum_{k=0}^{n}\frac{(itx)^k}{k!}}\leq |g(x)|\sum_{k=0}^{n}\frac{|tx|^{k}}{k!}\leq |g(x)|\sum_{k=0}^{\infty}\frac{|tx|^{k}}{k!}=|g(x)|e^{|tx|},
   \]
   y la cota anterior vale para cualquier $n\geq1$. Nuevamente usando la desigualdad de Hölder,
   \[
   \int_\R\abs{g(x)\sum_{k=0}^{n}\frac{(itx)^{k}}{k!}}d\gamma(x)\leq \int_\R |g(x)|e^{|tx|}d\gamma(x)\leq \norm{g}_{L^{p}(\gamma)}\left(\int_\R (e^{|tx|})^qd\gamma(x)\right)^{\frac{1}{q}}. 
   \]
   Observamos que se tiene la igualdad 
   \[
   \int_\R (e^{|tx|})^{q}d\gamma(x)=\frac{1}{\sqrt{2\pi}}\int_\R e^{q|tx|}e^{-\frac{x^2}{2}}dx,
   \]
   y notamos que esta última integral es finita para cualquier $q\in [1,\infty)$, ya que el término correspondiente a la medida gaussiana domina al otro término exponencial. Por lo tanto, $|g(x)|e^{|tx|}$ es una función en $L^{1}(\gamma)$ que domina a las sumas parciales anteriores. Así, usando el teorema de convergencia dominada, 
   \[
   \int_\R g(x)e^{-\frac{x^2}{2}}e^{itx}dx=\int_\R \lim_{n\to \infty} g(x)\sum_{k=0}^{n}\frac{(itx)^k}{k!}d\gamma(x)=\lim_{n\to \infty}\sum_{k=0}^{n}\frac{(it)^k}{k!}\int_\R g(x)x^k d\gamma(x)=0, 
   \]
   donde hemos hecho uso de la hipótesis del enunciado en la última igualdad. Hemos hallado que la transformada de Fourier de $g(x)e^{-\frac{x^2}{2}}$ es idénticamente la función 0. Al ser la transformada de Fourier inyectiva en $L^{1}(\R,\B(\R),dx)$, se sigue que $g(x)e^{-\frac{x^2}{2}}$ es la función idénticamente 0 salvo conjuntos de medida de Lebesgue 0. En particular, $g$ es la función idénticamente 0, $\gamma \ -$ casi en todas partes.  

   Concluimos la prueba notando que los monomios son un subconjunto de la clase de Schwarz $\S$.
 \end{proof}
Sabiendo que $\mathcal{S}$ es un subespacio lineal denso de $L^{q}(\gamma)$, podemos generalizar la derivada de las funciones en el espacio de Schwarz a funciones más generales que pertenecen a $L^{q}(\gamma)$. Para ello denotemos por ahora como $\frac{d^p}{dx^p}f$ a la derivada de orden $p$ de una función $f\in \S$, para $p\in \{0,1,2,...\}$. Dicha derivada siempre está definida para tales funciones. Si pensamos en la derivación de orden $p$ como aplicar un operador a un elemento del espacio de Schwarz, resulta que dicho operador es lineal. Dado que buscamos extender la derivada a funciones más generales, es natural pensar en la posibilidad de extender el operador $\frac{d^p}{dx^{p}}:\S\to L^{q}(\gamma)$ a un operador que sea cerrado en un dominio más grande que $\S$. Este es el contenido del siguiente teorema.
\begin{teo} 
Sea $\frac{d^{p}}{dx^p}:\mathcal{S}\longrightarrow L^{q}(\gamma)$ el operador derivada de orden $p$. Dicho operador es cerrable para todo $q\in [1,\infty)$ y todo entero $p\geq1$
\end{teo}
\begin{proof} 
  Usamos la siguiente caracterización de ser un operador cerrable. Supongamos que $(f_n)_{n\geq1}$ es una sucesión en $\S$ y que $g\in L^{q}(\gamma)$ son tales que 
  \[
  f_n\xrightarrow[n\to \infty]{}0 \qquad \text{ y } \qquad \frac{d^{p}}{dx^{p}}f_n\xrightarrow[n\to \infty]{}g.
  \] 
   Buscamos probar que $g=0$, $\gamma \ - $ casi en todas partes.
   
   Supongamos primero el caso $q>1$. Sea $h\in \S$. Definimos $\delta h(x):=xh(x)-h'(x)$ y para $n\geq1$, $\delta^{n+1}h(x):=\delta^{n} h(x)$, donde $\delta^{1}:=\delta$. Observemos primero que para cualquier $n\in \{1,2,...,p\}$, $\delta^{n}h\in \S$. Esto se sigue de que $xh(x)-h'(x)$ sigue siendo una función infinitamente diferenciable con crecimiento a lo más polinomial, y un argumento inductivo.
   
   Ahora bien, usando la igualdad
   \[
   \int_\R \left(\frac{d^{p-1}}{dx^{p-1}}f_n(x)h(x)\right)'d\gamma(x)=\int_\R \frac{d^{p}}{dx^{p}}f_n(x)h(x)d\gamma(x) +\int _\R \frac{d^{p-1}}{dx^{p-1}}f_n(x)h'(x)d\gamma(x),
   \] 
   la cual se obtiene con la derivada del producto, se tiene que 
   \begin{align*}
       \int_{\R}h(x)g(x)d\gamma(x)&=\lim_{n\to\infty}\int_\R \frac{d^{p}}{dx^{p}}f_n(x) h(x) d\gamma(x)\\
       &=\lim_{n\to\infty}\left(\int_\R \left(\frac{d^{p-1}}{dx^{p-1}}f_n(x)h(x)\right)'d\gamma(x)-\int _\R \frac{d^{p-1}}{dx^{p-1}}f_n(x)h'(x)d\gamma(x)\right)\\
       &=\lim_{n\to\infty}\left(\int_\R x\frac{d^{p-1}}{dx^{p-1}}f_n(x)h(x)d\gamma(x)-\int _\R \frac{d^{p-1}}{dx^{p-1}}f_n(x)h'(x)d\gamma(x)\right)\\
       &=\lim_{n\to\infty}\int_\R \frac{d^{p-1}}{dx^{p-1}}f_n(x) \left(xh(x)-h'(x)\right) d\gamma(x)\\
       &=\lim_{n\to\infty}\int_\R \frac{d^{p-1}}{dx^{p-1}}f_n(x) \delta h(x) d\gamma(x),
   \end{align*}
   donde en la primera igualdad hemos hecho uso del teorema de convergencia dominada, y en la tercera igualdad hemos hecho uso del lema \ref{lema1}. Utilizando un argumento inductivo y la continuidad del valor absoluto, obtenemos que 
   \[
       \abs{\int_{\R}h(x)g(x)d\gamma(x)}\leq\int_{\R}\abs{h(x)g(x)} d\gamma(x)=\lim_{n\to\infty}\int_\R \abs{f_n(x) \delta^{p} h(x)} d\gamma(x).
   \]
   Por hipótesis, dado que la sucesión $f_n$ converge a 0 en $L^{q}(\gamma)$ y $\delta^{p}h \in \S\subseteq L^{\frac{q}{q-1}}$ según el Lema 2, usando la desigualdad de Hölder, 
    \[
       \abs{\int_{\R}h(x)g(x)d\gamma(x)}\leq\lim_{n\to\infty}\int_\R |f_n(x) \delta^{p} h(x)| d\gamma(x)\leq \lim_{n\to \infty}\norm{f_n}_{L^{q}(\gamma)}\norm{\delta^{p}h}_{L^{\frac{q}{q-1}}(\gamma)}=0,
    \]
 \end{proof}
 de donde deducimos que para cualquier $h\in \S$, 
 \[
 \int_{\R}h(x)g(x)d\gamma(x)=0.
 \]
 Dado que $\S$ es denso en $L^{q}(\gamma)$, se sigue que $g=0$, $\gamma \ - $ casi en todas partes.

Hemos demostrado que el operador derivada de orden $p$ es cerrable en $\S$. Por lo tanto, por definición existe un operador cerrado, que vamos a denotar por $D^{p}$, con dominio $\text{Dom}(D^{p})$ un subespacio lineal de $L^{q}(\gamma)$ tal que $\S\subseteq \text{Dom}(D^{p})$ y $D^{p}f=\frac{d^{p}}{dx^{p}}f$ para cualquier $f\in \S$. Estos elementos nos permiten definir la derivada de Malliavin.

\begin{dfn}
   Sea $p\geq1$ un entero positivo. Definimos la derivada de Malliavin como el operador $D^{p}$ que extiende a la derivada $\frac{d^{p}}{dx^{p}}$ vista como operador de $\S$ a $L^{q}(\gamma)$, para $q\in [1,\infty)$.
\end{dfn}

Denotaremos al dominio de $D^{p}$ como $\D^{p,q}:=\text{Dom}(D^{p})$. A dichos espacios también se les conoce como espacios de Watanabe-Sobolev. Es conocido que al ser $D^{p}$ un operador cerrado, podemos definir la \textit{norma de la gráfica} de dicho operador en su dominio como la función $\|.\|_{G}:\D^{p,q}\longrightarrow[0,\infty)$ dada por,
\[
       \|f\|_{G}:=\norm{f}_{L^q(\gamma)}+\norm{D^{p}f}_{L^{q}(\gamma)}.
   \]
Dicha norma vuelve a la derivada de Malliavin $D^{p}$ un operador continuo. Más aún, es un resultado conocido de análisis funcional que, al ser $D^{p}$ un operador cerrado, su dominio $\D^{p,q}$ es un espacio de Banach con la norma $\norm{\cdot}_{G}$ definida antes.

Ahora bien, es natural pensar en lo que sucede con una aplicación sucesiva de las derivadas de Malliavin. Se tiene que para cualquier $p\geq1$ entero, y $f\in \D^{p+1,q}$, 
\[
   D^{p+1}f=D(D^{p}f)=D^{p}(Df),
\]
lo que nos dice que la derivada de Malliavin en efecto respeta la composición, tal y como ocurre en la derivación usual. Se sigue de esto que, al ser consistente la derivación sucesiva en en el sentido de Malliavin, para una función $f\in \D^{p,q}$, las derivadas $D^{k}f$ existen para cualquier $k\in \{1,...,p\}$, y por lo tanto, las normas $\norm{D^kf}_{L^2(\gamma)}$ son finitas para cualquier $k\in \{1,...,p\}$. Luego, se puede demostrar que la función $\|.\|_{\mathbb{D}^{p,q}}:\D^{p,q}\longrightarrow[0,\infty)$ dada por
\[
   \|f\|_{\D^{p,q}}:=\left(\norm{f}_{L^{q}(\gamma)}^q+\sum_{k=1}^p\norm{D^{k}f}_{L^{q}(\gamma)}^{q}\right)^{\frac{1}{q}}
\] 
es una norma equivalente a la norma de la gráfica definida en $\D^{p,q}$ antes. Por lo tanto, esta norma también hace a $\D^{p,q}$ un espacio de Banach.

Concluimos observando que $\D^{p,q}$ también puede ser obtenido como la cerradura del subespacio $\S$ con respecto a la norma $\norm{\cdot}_{\D^{p,q}}$ definida antes, pero restringida al espacio $\S$, en donde la derivada de Malliavin coincide con la derivada usual.

Lo anterior permite hacer una caracterización del espacio $\D^{p,q}$ y una definición alternativa de la derivada de Malliavin: una función $f$ está en dicho espacio si y solo si existe una sucesión de funciones $(f_n)_{n\geq1}\subseteq \S$ tales que $f_n\xrightarrow[n\to\infty]{}f$ y $(\frac{d^{p}}{dx^{p}}f_n)_{n\geq1}$ es una sucesión de Cauchy en $L^q(\gamma)$, para cualquier $k\in \{1,...,p\}$. Para tal sucesión de funciones, definimos 
\[
D^{k}f:=\lim_{n\to \infty}\frac{d^k}{dx^{k}}f_n,
\]
en donde el límite es en sentido $L^{q}(\gamma)$, y dicha función límite coincide con la derivada de Malliavin definida antes.

Este operador es de suma importancia. En la siguiente sección veremos que en el caso en que $q=2$, la derivada de Malliavin vista como operador posee propiedades interesantes.

\subsection{El operador de divergencia}

A lo largo de esta sección consideraremos el espacio $L^2(\gamma)$, el cual es un espacio de Hilbert con producto interno dado por 
\[
\langle f,g\rangle=\int_\R f(x)g(x)\ d\gamma(x).
\]
A partir de ahora, y para el resto del texto, denotaremos simplemente por $D^{p}$ a la derivada de Malliavin de orden $p$, aún cuando nos estemos refiriendo a la derivada de orden $p$ convencional de una función $f$ en el espacio de Schwarz.

Hasta ahora tenemos que la derivada de Malliavin es un operador $D^{p}:\mathbb{D}^{p,2}\subseteq L^2(\gamma)\longrightarrow L^2(\gamma)$ y dicho operador, es cerrado por construcción, extiende al operador derivada convencional $D^{p}$ definido en $\mathcal{S}\subseteq L^2(\gamma)$. 

Siendo $L^{2}(\gamma)$ un espacio de Hilbert, si denotamos por $\text{Dom}(\delta^{p})$ como el conjunto de funciones $g\in L^{2}(\gamma)$ tales que existe una constante $c_g>0$ y se cumple que

\[
\abs{\langle D^{p}f,g\rangle_{L^2(\gamma)}}\leq c_g\|f\|_{L^2(\gamma)}, \qquad \forall f\in \mathbb{D}^{p,2},                 xd
\]
entonces para cada $g\in \text{Dom}(\delta^{p})$ el funcional lineal, con dominio $\D^{2,q}$ y con regla de correspondencia $f\longmapsto \langle D^{p}f,g\rangle$, es acotado precisamente por la condición anterior. De esta forma, por el teorema de Hanh-Banach podemos extender dicho funcional a un funcional lineal acotado cuyo dominio es todo $L^2(\gamma)$.

Con el funcional anterior podemos usar el teorema de representación de Riesz para obtener la existencia de un único elemento en $L^2(\gamma)$, que denotaremos por $\delta^{p}g$ tal que 
\[
\langle D^pf,g\rangle_{L^2(\gamma)}=\langle f,\delta^p g\rangle_{L^2(\gamma)}, \qquad \forall f\in L^2(\gamma).
\]
En términos de integrales, se tiene que para cualquier $g\in \text{Dom}(\delta^p)$, existe un único elemento $\delta^p g\in L^2(\gamma)$ tal que 
\[
 \int_\R D^{p}f(x) g(x) \ d\gamma(x)=\int_\R f(x)\delta^{p}g(x) \ d\gamma(x), \qquad \forall f\in L^2(\gamma).  
\]
Con los elementos anteriores, podemos definir el operador de divergencia.

\begin{dfn}
   Sea $p\geq1$ un entero positivo. Denotamos $$\text{Dom}(\delta^p):=\left\{g\in L^2(\gamma) \ : \ \abs{\langle D^{p}f,g\rangle_{L^2(\gamma)}}\leq c\|f\|_{L^2(\gamma)}, \qquad \forall f\in \D^{2,p}\right\},$$ 

   y definimos el operador de divergencia $p$-ésimo como la aplicación $\delta^p:\text{Dom}(\delta^{p})\longrightarrow L^2(\gamma)$ tal que asigna a cada $g\in \text{Dom}(\delta^{p})$ al único elemento $\delta^{p} g\in L^2(\gamma)$ que, gracias al teorema de representación de Riesz, cumple la igualdad
       \[
       \int_\R D^{p}f(x)g(x) \ d\gamma(x)=\int_\R f(x)\delta^{p}g(x) \ d\gamma(x), \qquad \forall f\in L^2(\gamma).
      \]
\end{dfn}
En el caso en que $p=1$, directamente escribimos $\delta^1=\delta$. Observamos que el operador $\delta^p$ es directamente el operador adjunto de $D^{p}$, y como tal, es un operador cerrado.

Es de interés encontrar una forma de calcular el operador $\delta$ aplicado a alguna función en $\text{Dom}(\delta)$. En este sentido, existe una fórmula para calcular $\delta g$ para $g\in \D^{1,2}$. Este es el contenido del siguiente lema.

\begin{lema}\label{formuladelta}
   Se cumple que $\S\subseteq \text{Dom}(\delta)$ y para cualquier $g\in \D^{1,2}$, 
   \[
      \delta g(x)=xg(x)-Dg(x)=xg(x)-g'(x).
   \]
 En particular, para cualquier $g\in \S$, se cumple la fórmula anterior.
 \end{lema}

 \begin{proof} 
    Nótese que para $f, g\in \S$, por el lema \ref{lema1},
   \[
   \int_\R (f(x)g(x))'d\gamma(x)=\int_\R xf(x)g(x)d\gamma(x), 
   \]
   por lo que 
   \[
   \int_\R xf(x)g(x)d\gamma(x)=\int_\R D \left(f(x)g(x)\right) d\gamma(x)=\int_\R Df(x)g(x)d\gamma(x) +\int_\R f(x)Dg(x)d\gamma(x),
   \]
   así que restando, 
   \[
   \int_\R Df(x)g(x)d\gamma(x)=\int_\R xf(x)g(x)d\gamma(x)-\int_\R f(x)Dg(x)d\gamma(x),
   \]
   y por definición de $\delta g$, se tiene que 
   \[
   \int_\R f(x)\delta g(x)d\gamma(x)=\int_\R xf(x)g(x)d\gamma(x)-\int_\R f(x)Dg(x)d\gamma(x).   
   \]
   Pero lo anterior ocurre para cualquier $f\in \mathcal{S}$. Esto nos dice que $\mathcal{S}\subseteq \text{Dom}(\delta)$ y que para cualquier $g\in \mathcal{S}$, la fórmula anterior es válida. 

Utilizando un argumento de aproximación, tenemos que para cualquier $g\in \mathbb{D}^{1,2}$, $\delta g(x)=xg(x)-Dg(x)$.

  \end{proof}

\subsection{El semigrupo de Ornstein-Uhlenbeck}
El semigrupo de Ornstein-Uhlenbeck es una generalización del operador Laplaciano a dimensión infinita. Comenzamos directamente definiendo el semigrupo de Ornstein-Uhlenbeck. 
\begin{dfn}
   El semigrupo de Ornstein-Uhlenbeck, denotado por $(P_t)_{t\geq0}$, se define como una familia de operadores del espacio $\S$ dotado de la norma $\norm{\cdot}_{L^q(\gamma)}$ a $L^q(\gamma)$ como sigue: para $f\in \mathcal{S}$ y $t\geq0$,
   \[
   P_tf(x)=\int_\R f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y), \qquad  \forall x\in \R.
   \]
\end{dfn}
De manera inmediata se observa que esta es una familia de operadores lineales. Tal como se puede esperar, esta familia de operadores cumple la propiedad de semigrupo, y está directamente relacionada con el proceso de Ornstein-Uhlenbeck. Ambos hechos se exploran más adelante. Comenzamos estudiando propiedades básicas de $(P_t)_{t\geq0}$.

\begin{teo}
   Sea $f\in \mathcal{S}$ y $q\in [1,\infty)$. Entonces se cumplen las siguientes proposiciones:
   \begin{itemize}
       \item $P_0f(x)=f(x)$.
       \item $P_\infty f(x):=\displaystyle\lim_{t\to\infty}P_tf(x)=\int_\R f(y)d\gamma(y)$.
       \item $\displaystyle\int_\R \abs{P_tf(x)}^qd\gamma(x)\leq \int_\R\abs{f(x)}^qd\gamma(x)$.
   \end{itemize}
\end{teo}
\begin{proof} 
  \begin{itemize}
   \item  Calculamos el semigrupo en $t=0$. Dado que $\gamma(\R)=1$, se tiene que 
    \[
    P_0f(x)=\int_\R f\left(e^{0}x+\sqrt{1-e^{0}}y\right)d\gamma(y)=\int_\R f(x)d\gamma(y)=f(x).
    \]
    \item Como $f\in \S$, tiene crecimiento a lo más polinomial. Por lo tanto existe un real $M>0$ y un natural $n\geq1$ tal que para cualquier $y\in \R\setminus[-M,M]$, $|f(y)|\leq C(1+|y|^n)$, para alguna constante $C>0$. Observamos que esta última cota es una función en $L^q(\gamma)$. Por otro lado, es claro que en $[-M,M]$, la función $|f|$ alcanza su máximo, por lo que $\norm{f\1_{[-M,M]}}_\infty<\infty$. 
    
    Finalmente, notamos que la función $g(y)=e^{-t}x+\sqrt{1-e^{-2t}}y$ es una función continua y por lo tanto medible, así que
    
    \begin{align*}
       \abs{f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)}&\leq C(1+|y|^n)\1_{g^{-1}[-M,M]^{c}}(y)+\norm{f\1_{[-M,M]}}_\infty\1_{g^{-1}[-M,M]}(y)\\
       &\leq C(1+|y|^n)+\norm{f\1_{[-M,M]}}_\infty,
    \end{align*}
    función que está en $L^1(\gamma)$, y que es independiente de $t$. Por lo tanto, por el teorema de convergencia dominada, 
    \[
    P_\infty f(x)=\lim_{t\to\infty}P_tf(x)=\int_\R\lim_{t\to\infty}f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)=\int_\R f(y)d\gamma(y).
    \]
    \item Para $q\in [1,\infty)$, por definición,
    \begin{align*}
       \int_\R |P_tf(x)|^q d\gamma(x)&=\int_\R \abs{\int_\R f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)}^qd\gamma(x)\\
       &\leq \int_\R \int_\R \abs{f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)}^qd\gamma(y)d\gamma(x)\\
       &=\E\left[f\left(\abs{e^{-t}X+\sqrt{1-e^{-2t}}Y}^q \right)\right],
    \end{align*}
    donde hemos hecho uso de la desigualdad de Jensen, y $X,Y$ son variables aleatorias normales estándar independientes. Pero notemos que, denotando $Z:= e^{-t}X+\sqrt{1-e^{-2t}}Y$, la función característica de $Z$ es
    \[
    \phi_Z(z)=\E\left[e^{ize^{-t}X+iz\sqrt{1-e^{-2t}}Y}\right]=\E\left[e^{ize^{-t}X}\right]\E\left[e^{iz\sqrt{1-e^{-2t}}Y}\right]=e^{-\frac{z^2e^{-2t}}{2}}e^{-\frac{z^2(1-e^{-2t})}{2}}=e^{-\frac{z^2}{2}},
    \]  
    por lo que $Z$ tiene un distribución normal estándar. Se sigue que 
    \[
       \int_\R |P_tf(x)|^q d\gamma(x)=\E\left[f\left(\abs{e^{-t}X+\sqrt{1-e^{-2t}}Y}^q\right)\right]\leq\E\left[\abs{f(Z)}^q\right]=\int_\R \abs{f(x)}^qd\gamma(x).
    \]
  \end{itemize}
\end{proof}
Del último ítem de la proposición anterior se deduce que para cualquier $f\in \S$ y cualquier $t\geq0$, $P_tf\in L^q(\gamma)$. Más aún, la familia de operadores $(P_t)_{t\geq0}$ que van del espacio $(\S,\norm{\cdot}_{L^q(\gamma)})$ a $L^q(\gamma)$ son uniformemente acotados por la constante $1$.

Al ser $\S$ un subconjunto denso de $L^q(\gamma)$ y ser este último un espacio completo, por el conocido teorema BLT de análisis funcional, los operadores de $(P_t)_{t\geq0}$ se pueden extender de manera única a una familia de operadores, que también denotaremos por $(P_t)_{t\geq0}$, con dominio todo el espacio $L^q(\gamma)$, y cuyas normas siguen siendo acotadas por la constante 1. Esto último hace a $(P_t)_{t\geq0}$ una familia de operadores lineales contractivos.

Probamos ahora que $(P_t)_{t\geq0}$ en efecto forma un semigrupo.
\begin{teo} 
Sean $s,t\geq0$. Se tiene que $P_tP_s=P_{t+s}$ en $L^q(\gamma)$, para $q\in [1,\infty)$.
\end{teo}
\begin{proof} 
  Dado que la medida del espacio es finita, basta con probar el resultado para $L^1(\gamma)$. Sea $f\in L^1(\gamma)$. Tenemos las siguientes igualdades para $x\in \R$:
\begin{align*}
   P_{t}(P_sf)(x)&=\int_\R P_sf\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(x)\\
   &=\int_\R \int_\R f\left(e^{-s}(e^{-t}x+\sqrt{1-e^{-2t}}y)+\sqrt{1-e^{-2s}}z\right)d\gamma(z)d\gamma(y)\\
   &=\int_\R\int_\R f \left(e^{-(t+s)}x+e^{-s}\sqrt{1-e^{-2t}}y+\sqrt{1-e^{-2s}}z\right)d\gamma(z)d\gamma(y)\\
   &=\E\left[f \left(e^{-(t+s)}x+e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z\right)\right],
\end{align*}
donde $Y$ y $Z$ denotan variables aleatorias normales estándar e independientes. Ahora bien, nótese que la función característica de la variable $W:=e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z$ está dada por 

\begin{align*}
   \phi_W(w)&=\E\left[\exp \left\{iw(e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z)\right\}\right]\\
   &=\E\left[\exp \left\{iwe^{-s}\sqrt{1-e^{-2t}}Y\right\}\right]\E\left[\exp \left\{iw\sqrt{1-e^{-2s}}Z\right\}\right]\\
   &=\exp \left\{\frac{-w^2e^{-2s}(1-e^{-2t})}{2}\right\}\exp \left\{\frac{-w^2(1-e^{-2s})}{2}\right\}\\
   &=\exp \left\{\frac{-w^2(e^{-2s}-e^{-2(t+s)}+1-e^{-2s})}{2}\right\}\\
   &=\exp \left\{\frac{-w^2(1-e^{-2(t+s)})}{2}\right\},
\end{align*}
la cual corresponde con la función característica de una variable $V:=\sqrt{1-e^{-2(t+s)}}N$, donde $N$ es gaussiana estándar. Se sigue que 
\begin{align*}
   P_{t}(P_sf)(x)&=\E\left[f \left(e^{-(t+s)}x+e^{-s}\sqrt{1-e^{-2t}}Y+\sqrt{1-e^{-2s}}Z\right)\right]\\
   &=\E\left[f \left(e^{-(t+s)}+\sqrt{1-e^{-2(t+s)}}V\right)\right]\\
   &=\int_\R f \left(e^{-(t+s)}+\sqrt{1-e^{-2(t+s)}}v\right)d\gamma(v)\\
   &=P_{t+s}f(x).
\end{align*}
 \end{proof}
Finalmente, la siguiente proposición nos dice que tanto $P_t$ como $D$, vistos como operadores, pueden ser intercambiados en el espacio $\mathbb{D}^{1,2}$.
\begin{teo} 
 Sea $f\in \D^{1,2}$ y $t\geq0$. Entonces $P_tf\in \D^{1,2}$ y $DP_tf=e^{-t}P_tDf$.
 \end{teo}
 \begin{proof} 
   Sea $f\in \S$. La derivada de Malliavin coincide con la derivada usual en dicho espacio y por lo tanto, dado que la función $e^{-t}f' \left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)$ está dominada por una función similar a la del ítem 3 del teorema 8, podemos derivar bajo el signo de la integral, hallando que
   \begin{align*}
       DP_tf(x)&=\frac{d}{dx}\int_\R f \left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)\\
       &=\int_\R e^{-t}\frac{d}{dx}f\left(e^{-t}x+\sqrt{1-e^{-2t}}y\right)d\gamma(y)\\
       &=e^{-t}P_t Df(x),
   \end{align*}
   para cualquier $x\in \R$. Esto prueba la propiedad en $\S\subseteq \D^{1,2}$. Para $f$ en dicho espacio general, usamos que $\S$ es denso en $\D^{1,2}$.
  \end{proof}
  Antes de continuar con la siguiente sección, mostramos la conexión de esta familia de operadores con el semigrupo de Ornstein-Uhlenbeck:
  \begin{teo} 
   Sea $B$ un movimiento browniano estándar. Consideramos la ecuación diferencial estocástica dada por 
   \[
   dX_t^{x}=\sqrt{2}dB_t-X^{x}_tdt, \qquad t\geq0,
   \]
   donde $X^{x}_0=x\in \R$. Se tiene que la solución de la ecuación diferencial anterior está dada por 
   \[
   X_t^x=e^{-t}x+\sqrt{2}\int_0^te^{-(t-s)}dB_s, \qquad t\geq0,
   \]
   el cual es un proceso de Markov. Más aún, el semigrupo asociado a dicho proceso es justamente $(P_t)_{t\geq0}$. Tal proceso es justamente el proceso de Ornstein-Uhlenbeck.
  \end{teo}
   \begin{proof} 
     Comenzamos notando que en efecto el proceso anteriormente definido cumple que $X_0^x=x$. Pero esto es claro ya que 
     \[
     X_0^x=e^{0}x+\int_{0}^{0}e^{-(t-s)}dB_s=x.
     \] 
     Vemos ahora que se cumple la ecuación diferencial estocástica. Sea $t>0$ y nótese que 
     \begin{align*}
       X_t^x&=e^{-t}x+\sqrt{2}\int_0^te^{-(t-s)}dB_s\\
       &=e^{-t}x+\sqrt{2}e^{-t}\int_0^{t}e^{s}dB_s\\
       &=e^{-t}\left(X_0^{x}e^{0}+\sqrt{2}e^{-t}\int_0^te^{s}dB_s \right),
     \end{align*}
     de donde concluimos que 
     \[
     e^{t}X_t^x-e^{0}X_0^x=\sqrt{2}\int_{0}^{t}e^{s}dB_s,
     \]
     lo cual en notación diferencial equivale a que 
     \[
     d \left(e^tX_t^x\right)=\sqrt{2}e^{t}dB_t,  \qquad t>0.
     \]
     Usando ahora la regla del producto y el hecho de que $e^{-t}$ es un proceso de variación finita,  
     \begin{align*}
       dX_t^{x}&=d(e^{-t}e^{t}X_t^{x})\\
       &=e^{-t}d(e^{t}X_t^{x})+e^{t}X_t^{x}d(e^{-t})+0\\
       &=e^{-t}\sqrt{2}e^{t}dB_t-e^{t}X_t^{t}e^{-t}dt\\
       &=\sqrt{2}dB_t-X_t^xdt,
     \end{align*}
     tal como queríamos probar. Por lo tanto el proceso $(X_t^x)_{t\geq0}$ definido como antes es solución a la ecuación diferencial estocástica dada. Como tal, dicho proceso resulta ser una difusión, y en particular un proceso de Markov. Resta ver que su semigrupo justamente es el semigrupo de Ornstein-Uhlenbeck definido antes.

     Directamente de la expresión de $X_t^{x}$, observamos que tenemos una integral de Itô cuyo integrando es estocástico. Por lo tanto, dicha integral forma un proceso Gaussiano adaptado a la filtración generada por $B$. Por otro lado, calculando la función de medias y covarianzas del proceso hallamos lo siguiente:
     \begin{itemize}
        \item $\mu(t)=\E\left[X_t^{x}\right]=e^{-t}x$
        \item \begin{align*}\Gamma(s,t)&=\text{Cov}\left(X_s^x,X_t^x\right)\\
            &=\E\left[\left(X_t^{x}-e^{-t}x\right)\left(X_s^x-e^{-s}x\right)\right]\\
            &=\E\left[\left(\sqrt{2}e^{-t}\int_{0}^{t}e^{u}dB_u\right)\left(\sqrt{2}e^{-s}\int_{0}^{s}e^{v}dB_v\right)\right]\\
            &=2e^{-(t+s)}\E\left[\int_{0}^{s\wedge t}e^{2u}du\right]\\
            &=\frac{2}{2}e^{-(t+s)}(e^{2s\wedge t}-1)\\
            &=e^{-|t-s|}-e^{-(t+s)}.
        \end{align*}
     \end{itemize}
     En particular, se tiene para $s=t$ que $X_t^x\sim Normal(e^{-t}x,1-e^{-2t})$. Finalmente, si $B=(B_t)_{t\geq0}$ representa el movimiento browniano sobre el cual $(X_t^x)_{t\geq0}$ está definido, notamos que para $t\geq0$,
     \[
     \E\left[e^{-t}x+e^{-t}B_{e^{2t}-1}\right]=e^{-t}x
     \]
     y además, 
     \[
     \E\left[\left(e^{-t}B_{e^{2t}-1}\right) \left(e^{-s}B_{e^{2s}-1}\right)\right]=e^{-(t+s)}(\min(e^{2t},e^{2s})-1)=e^{-(t+s)}(e^{2s\wedge t}-1)=e^{-|t-s|}-e^{-(t+s)}.
     \]
     Concluimos que el proceso de Ornstein-Uhlenbeck $(X_t^{x})_{t\geq0}$ como solución a la ecuación diferencial estocástica dada antes, puede verse como 
     \[
        X_t^{x}=e^{-t}x-e^{-t}B_{e^{2t}-1}, \qquad t\geq0.
     \]
     De lo anterior, es muy sencillo ver que para $f$ medible y acotada, 
     \begin{align*}
      P_tf(x)&=\E\left[f(X_{t+s}^{x})|\F_s\right]\\
      &=\E_x\left[f(e^{-(t+s)}x-e^{-(t+s)}B_{e^{2(t+s)}-1})|\F_s\right]\\
      &=\int_\R f \left(e^{-t}x-\sqrt{1-e^{-2t}}y\right)d\gamma(y),
     \end{align*}
     
     el cual es justamente el semigrupo que hemos estado trabajando.
   \end{proof}

\subsection{El generador infinitesimal}
 Como es clásico en el contexto de procesos de Markov, asociado a un semigrupo tenemos la noción de generador infinitesimal. En nuestro caso consideremos al conjunto siguiente:
 \[
 \text{Dom}(L):=\left\{f\in L^2(\gamma): \lim_{h\to 0} \frac{P_{h}f-f}{h} \text{ existe en } L^{2}(\gamma)\right\}.
 \]  
Dicho es directamente el dominio del generador infinitesimal $L$. Así, dada $f\in \text{Dom}(L)$, se tiene que 
\[
Lf:=\lim_{h\to 0}\frac{P_hf-f}{h}.    
\]
Ya que el límite anterior recrea la definición de derivada en funciones reales, denotaremos
\[
   \frac{d}{dt}P_tf:=\lim_{h\to0}\frac{P_{t+h}f-P_tf}{h}, \qquad \text{ y en particular } \qquad  Lf:=\frac{d}{dt}P_tf\bigg|_{t=0},
\]
omitiendo en ocasiones la función $f$ cuando estemos hablando de una función en un conjunto particular. Ahora bien, en el espacio $\S$, para cualquier $t\geq0$ se tiene el siguiente resultado:
\[
\frac{d}{dt}P_t=\lim_{h\to0}\frac{P_{t+h}-P_t}{h}=\lim_{h\to0}P_t\frac{P_h-Id}{h}=P_t\lim_{h\to0}\frac{P_h-Id}{h}=P_t\frac{d}{dh}\bigg|_{h=0}P_h=P_tL.
\]
Invirtiendo la manera en cómo se toman los límites, se obtiene también que
\[
\frac{d}{dt}P_t=LP_t,  
\]
así que para cualquier $t\geq0$, y $f\in \S$, tenemos que 
\[
\frac{d}{dt}P_t=P_tL=LP_t.
\]

Finalmente, una proposición sumamente importante es el siguiente teorema.
\begin{teo} 
En el espacio de Schwarz, se tiene que $Lf=-\delta Df$. Más específicamente, para cualquier $f\in \S$, 
\[
Lf(x)=-xf'(x)+f''(x).   
\]
\end{teo}
 \begin{proof} 
   Sea $f\in \S$ y $t>0$. Notamos que, utilizando una función dominante, que existe gracias al hecho de que $f$ es suave, intercambiamos derivada con integral, por lo que
   
   \begin{align*}
    \frac{d}{dt}P_tf(x)&=\frac{d}{dt}\int_\R f(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
    &=\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)\left(-e^{-t}x+\frac{-y(-2e^{-2t})}{2\sqrt{1-e^{-2t}}}\right)d\gamma(y)\\
    &=\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)\left(-e^{-t}x+\frac{ye^{-2t}}{\sqrt{1-e^{-2t}}}\right)d\gamma(y)\\
    &=-x\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)e^{-t}d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\int_\R yf'(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y).
   \end{align*}
   Reconocemos en el primer integrando la derivada con respecto a $x$ de $P_tf$, mientras que en el segundo término reconocemos la integral de una función de la forma $yg(y)$, donde $g(y)=f'(e^{-t}x+\sqrt{1-e^{-2t}}y)$, función que está en $L^1(\gamma)$. Por lo tanto, por el Lema \ref{lema1}, 
    \begin{align*}
        &-x\int_\R f'(e^{-t}x+\sqrt{1-e^{-2t}}y)e^{-t}d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\int_\R yf'(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
        &=-x\int_\R \frac{d}{dx}f(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\int_\R \frac{d}{dy}f'(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
        &=-x\frac{d}{dx}\int_\R f(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)+\frac{e^{-2t}}{\sqrt{1-e^{-2t}}}\sqrt{1-e^{-2t}}\int_\R f''(e^{-t}x+\sqrt{1-e^{-2t}}y)d\gamma(y)\\
        &=-x(P_tf(x))'+e^{-2t}P_tf''(x).
    \end{align*}
    Tomando límite cuando $t\to0$ hallamos que 
    \[
    Lf(x)=\frac{d}{dt}P_tf(x)\bigg|_{t=0}=-\left(xf'(x)+f''(x)\right).
    \]
    Finalmente, dado que $f\in \S$, su derivada de Malliavin coincide con su derivada usual, por lo que usando la proposición ???, el generador infinitesimal de una función en el espacio de Schwarz viene dado por 
    \[
    Lf(x)=-\delta Df(x).
    \]
  \end{proof}
Como ejemplo de la potencia de la proposición anterior, se demuestra a continuación la desigualdad de Poincaré en el contexto de los espacios de Watanabe-Sobolev.
Para tener una idea de la comparación entre ambos contextos, presentamos tanto la desigualdad de Poincaré en el contexto de espacios de Sobolev clásicos en $\R$ como la desigualdad de Poincaré en $\D^{2,p}$.

Recordemos que $W^{k,p}(U)=\left\{f\in L^{p}(U,\B(U),dx): D^jf\in L^p(U,\B(U),dx), j\in \{1,2,...,k\}\right\}$ es el espacio de Sobolev que consiste en el conjunto de aquellas funciones en $L^p(U)$ ($U\subseteq \R$ subconjunto abierto) tales que todas sus derivadas hasta la $k$-ésima en el sentido distribucional también pertenecen a $L^p(U,\B(U),dx)$.

\begin{teo}\textbf{(Desigualdad de Poincaré clásica)}
   Sean $p\in [1,\infty)$ y $W_0^{1,p}(U)=\overline{C^{\infty}_c(U)}$ la cerradura del espacio de las funciones test, con respecto a la norma $\|.\|_{W^{1,p}(U)}$ definida como
   \[
     \|f\|_{W^{1,p}(U)}=\left(\|f\|_{L^{p}(U)}^p+\|Df\|_{L^p(U)}^p\right)^{1/p}, \qquad \text{ para } f\in W^{1,p}(U).
    \]
   Entonces si $U$ es un abierto acotado de $\R$, y $u\in W_0^{1,p}(U)$, se tiene la desigualdad 
   \[
   \|u\|_{L^{p}(U)}\leq C\|Du\|_{L^p(U)},   
   \]
   donde $C$ es una constante que solo depende de $p$ y $U$. 
   \end{teo}
   Esta cota nos da la posibilidad de controlar el crecimiento en $L^p(U)$de una función $u\in W_0^{k,p}$ vía la norma de su derivada $Du$ en $L^p(U)$.

   Más aún, gracias a la definición de $\|.\|_{W^{1,p}(U)}$, la cota anterior nos dice que si tenemos un subconjunto $U\subseteq \R$ abierto y acotado, en $W_0^{1,p}(U)$ las normas $\|\cdot\|_{W_{1,p}(U)}$ y $\|D\cdot\|_{L^p(U)}$ son equivalentes. Pasamos ahora a nuestra versión de la desigualdad.
   \begin{teo} \textbf{(Desigualdad de Poincaré unidimensional)} Sea $N\sim Normal(0,1)$ y sea $f\in \D^{1,2}$. Entonces
       \[
       \text{Var}\left(f(N)\right)=\E\left[(Df(N))^2\right]    
       \]     
    \end{teo}
    \begin{proof} 
        Supongamos que $f\in \S$ y para $f\in \D^{1,2}$ utilizamos un argumento de aproximación. Dado que $f\in \S$, podemos intercambiar los símbolos de derivada e integral. Por lo tanto, tenemos que 
        \begin{align*}
            \text{Var}\left(f(N)\right)&=\E\left[f(N)(f(N)-\E\left[f(N)\right])\right]\\
            &=\E\left[f(N)(P_0f(N)-P_\infty)f(N)\right]\\
            &=-\int_{0}^{\infty}\E\left[f(N)\frac{d}{dt}P_tf(N)\right]dt\\
            &=\int_{0}^{\infty}\E\left[f(N)\delta DP_tf(N)\right]dt\\
            &=\int_{0}^{\infty}\E\left[f'(N)DP_tf(N)\right]dt\\
            &=\int_{0}^{\infty}e^{-t}\E\left[f'(N)P_tf'(N)\right]dt\\
            &\leq e^{-t}\sqrt{\E\left[f'^2(N)\right]}\sqrt{\E\left[(P_tf')^2(N)\right]}dt\\
            &\leq \E\left[f'^2(N)\right]\int_{0}^{\infty}e^{-t}dt\\
            &=\E\left[f'^2(N)\right].
        \end{align*} 
     \end{proof}
Nótese que esta desigualdad en términos de la norma $L^2(\gamma)$ se escribe como sigue:
\[
\|f\|^2_{L^2(\gamma)}\leq \E^2\left[f(N)\right]+\|Df\|^2_{L^2(\gamma)}, \qquad \forall f\in \D^{1,2},
\]
de manera que, si $\E\left[f(N)\right]=0$, directamente recuperamos la desigualdad de Poincaré clásica, aunque en distinto contexto. Y como tal, esta desigualdad nos permite controlar el crecimiento de las funciones $f\in \D^{1,2}$ utilizando la norma de su derivada de Malliavin.

Concluimos esta sección con la denominada `relación de conmutatividad de Heisenberg'.
\begin{teo} 
 Para $f\in \S$, y $p\geq1$ se tiene que 
 \[
    (D\delta^p-\delta^p D)f=p\delta^{p-1}f.
 \]
 En particular para $p=1$, se tiene que
 \[
 (D\delta-\delta D)f=f.
 \]
 \end{teo}
\begin{proof} 
   Realizamos la prueba para el caso $p=1$. El caso general se sigue usando inducción. Sea $f\in \S$ y notemos que para cualquier $x\in \R$, 
\begin{align*}
   (D\delta-\delta D)f(x)&=D\delta f(x) - \delta D f(x)\\
   &=D \delta f(x) - (xDf(x)-D^2f(x))\\
   &=D \left(\delta f(x)+Df(x)\right) -xDf(x)\\
   &=D \left(xf(x)\right)-xDf(x)\\
   &=f(x)+xDf(x)-xDf(x)\\
   &=f(x),
\end{align*}
en donde en la segunda y cuarta igualdad hemos usado la fórmula del lema \ref{formuladelta}.
 \end{proof}
\section{Cálculo de Malliavin en el caso general.}

En esta sección se presentan los operadores de Malliavin generales, que serán utilizados en lo subsecuente. Seguimos de cerca los requerimientos de \cite[sección 2.1]{KUZGUN202268}

Denotemos por $\mathfrak{H}:=L^2(\R_+\times\R)$. Ahora denotaremos por $\S$ al siguiente conjunto:
\[
\S:=\left\{F=f(W(h_1),...,W(h_n))) \ : \ f\in C^{\infty}_b, \ h_1,...,h_n\in \mathfrak{H} \right\},
\]
donde $W=\left\{W(h):h\in \mathfrak{H}\right\}$ es el proceso Gaussiano isonormal sobre el cual estamos trabajando. Definimos ahora la derivada de Malliavin.
\begin{dfn} 
 Sea $F\in \S$, y sea $p\geq1$ un entero. La derivada de Malliavin de orden $p$ de $F$ es el elemento de $L^2(\Omega;\mathfrak{H}^{\odot})$ definido por 
 \[
 D^pF:=\sum_{i_1,...,i_p=1}^{n}\frac{\partial^pf}{\partial x_{i_1}...\partial x_{i_p}}(W(h_1),...,W(h_n))h_{i_1}\otimes...\otimes h_{i_p}.
 \] 
 En el caso en que $p=1$ hablaremos simplemente de la <<derivada de Malliavin>>.
 \end{dfn}
 Esta derivada cumple con una condición similar a la del caso unidimensional.

 \begin{prop} 
  Sea $q\in [1,\infty)$, y sea $p\geq 1$ un entero. Entonces el operador 
  \[
  D^{p}:\S\subseteq L^q(\Omega)\to L^q(\Omega;\mathfrak{H}^{\odot p})
  \]
  es cerrable.
  \end{prop}
En consecuencia, para $q\in [1,\infty)$ y $p\geq1$ un entero, definimos la norma $\norm{\cdot}_{\D^{p,q}}:\S\to [0,\infty)$ dada por 
\[
\norm{F}_{D^{p,q}}:= \left(\E\left[\abs{F}^{q}\right]+\E\left[\norm{DF}^{q}_{\mathfrak{H}}\right]+...+\E\left[\norm{D^pF}_{\mathfrak{H}^{\otimes p}}^q\right]\right)^{1/q}.
\]
y denotamos por $\D^{p,q}$ a la cerradura del espacio $\S$ con respecto a la norma anterior.

Pasamos ahora al operador de divergencia. Dado un entero $p\geq1$, definimos el poerador de divergencia de orden $p$ como el operador adjunto del operador derivada $D^{p}:\D^{p,2}\to L^{2}(\Omega,\mathfrak{H})$.

\begin{dfn} 
 Sea $p\geq1$ un entero. Denotamos por $\text{Dom}(\delta^p)$ al subconjunto de $L^2(\Omega;\mathfrak{H}^\otimes p)$ compuesto de aquellos elementos $u$ tales que existe una constante $C>0$ que satisface 
 \[
 \abs{\E\left[\langle D^pF,u\rangle_{\mathfrak{H}^{\otimes p}}\right]}\leq c\sqrt{\E\left[F^2\right]}, \qquad \forall F \in \S.
 \]
 \end{dfn}
 Argumentando de manera similar al caso unidimensional, por el teorema de representación de Riesz, existe un único elemento en $L^2(\Omega)$, denotado por $\delta^p(u)$ tal que cumple la relación 
 \[
 \E\left[\langle D^pF,u\rangle_{\mathfrak{h}^{\otimes p}}\right]=\E\left[F\delta^{p}(u)\right], \qquad \forall f\in \S.
 \]
 Llegamos así a la siguiente definición.

 \begin{dfn} 
  Si $u\in \text{Dom}(\delta^p)$, entonces $\delta^p(u)$ es el único elemento de $L^2(\Omega)$ caracterizado por la siguiente fórmula:
\[
 \E\left[\langle D^pF,u\rangle_{\mathfrak{h}^{\otimes p}}\right]=\E\left[F\delta^{p}(u)\right], \qquad \forall f\in \S.
 \]
  \end{dfn}
Este operador se denomina el operador de divergencia múltiple de orden $p$. Cuando $p=1$ simplemente lo llamamos el <<operador de divergencia>>,  escribiendo $\delta$ en lugar de $\delta^1$. La fórmula anterior se suele llamar <<fórmula de integración por partes>>.

El siguiente resultado nos permite <<sacar>> una variable aleatoria escalar dentro del operador de divergencia $\delta$. Este resultado se puede consultar en \cite[proposición 2.5.4]{Nourdin_Peccati_2012}, sin embargo, enunciamos una versión general que se encuentra en \cite[lema 1]{Caballero1998-hz}.

\begin{teo}\label{factordeltafuera}
 Sean $p,p'>1$ tales que $\frac{1}{p}+\frac{1}{p'}=1$. Supongamos que $F\in \D^{1,p'}$, y sea $u \in \text{Dom}(\delta)$ tal que $u\in L^{p}(\Omega;\mathfrak{H})$ y $\delta(u)\in L^{p}(\Omega)$. Entonces $Fu\in \text{Dom}(\delta)$ y se cumple la igualdad 
 \begin{equation}\label{eqfactordeltafuera}
   \delta (Fu)=F\delta(u)-\langle DF,u\rangle_{\mathfrak{H}}.
\end{equation}
 
 \end{teo}

 Tenemos también la propiedad de conmutatividad de Heisenberg en este caso. Dicha propiedad es compleamente análoga al caso unidimensional.

\begin{teo} 
Se cumple que 
\[
D\delta(u)-\delta(Du)=u.
\]
 \end{teo}

Finalmente, es de destacar una propiedad muy interesante del operador de divergencia: cuando estamos en el caso en que $\mathfrak{H}=L^2(\R_+\times\R)$, el operador delta coincide con la integral de Itô-Walsh.

\begin{teo} 
 Sea $u$ un proceso adaptado y cuadrado-integrable. Entonces $u\in \text{Dom}(\delta)$ y se cumple que 
 \[
 \delta(v)=\int_{\R_+\times \R}^{}v(s,y)W(ds,dy).
 \]
 \end{teo}
 
 \chapter{Aplicaciones a Teoremas Límite y estudio de densidades}
En este capítulo se visita la aplicación de los conceptos del cálculo de Malliavin al estudio de Teoremas límite y al estudio de densidades. 
Una herramienta clave en esta tarea es el conocido como Método de Stein. 
Dicho método, desarrollado por Charles Stein en la década de 1970, es una herramienta bastante popular para evaluar la distancia entre las leyes de dos leyes de probabilidad. Lo anterior es posible lograrlo estudiando operadores diferenciales involucrados con las leyes de probabilidad en cuestión.

\section{Método de Stein}

Dentro del mundo de la probabilidad, la distribución normal juega un papel importante debido a la gran cantidad de propiedades que la misma presenta. Entre ellas, dicha distribución está determinada por sus momentos. Esto hace posible caracterizar la distribución en términos de los mismos. Ésta propiedad, la cual está codificada en el conocido como `lema de Stein', forma parte importante de este texto. 

Comenzamos dando una prueba de que la ley de una variable Gaussiana estándar está caracterizada por sus momentos. Recordemos que, dada una medida $\nu$ en $\R$, el $n$-ésimo momento de la misma, cuando existe, está dado por 
\[
   m_n(\nu):=\int_{\R}x^{n}d\nu(x), \qquad n\geq0.
\]

\begin{lema} 
 Sea $\gamma$ la distribución gaussiana estándar. Dicha distribución está caracterizada por sus momentos.
 \end{lema}
 \begin{proof} 
    Debemos mostrar que, si existe otra medida definida en $\R$ con todos sus momentos definidos, digamos $\mu$, y  tales momentos coinciden con los de $\gamma$, entonces $\mu=\gamma$.

    Sea entonces $\mu$ una medida como antes. Es suficiente mostrar que la función característica de ambas medidas coincide en todo $\R$, esto es, que 
    \[
      \int_\R e^{itx}d\mu(x)=\int_\R e^{itx}d\gamma(x), \qquad \forall t\in \R.
    \]
    Utilizando la forma de Lagrange del residuo en el teorema de Taylor y la desigualdad de Cauchy-Schwarz, tenemos para cualquier $n\geq1$ que
    \begin{align*}
      \abs{\int_{\R}e^{itx}d\nu(x)-\int_\R e^{itx}d\gamma(x)}&\leq \int_\R \abs{e^{itx}-\sum_{k=0}^{n}\frac{(itx)^k}{k!}}d\nu(x)+\int_\R \abs{e^{itx}-\sum_{k=0}^{n}\frac{(itx)^k}{k!}}d\gamma(x)\\
      &=\int_\R \abs{\frac{(it)^{n+1}e^{it\xi_L}x^{n+1}}{(n+1)!}}d\nu(x)+\int_\R \abs{\frac{(it)^{n+1}e^{it\eta_L}x^{n+1}}{(n+1)!}}d\gamma(x)\\
      &\leq\int_\R \frac{\abs{tx}^{n+1}}{(n+1)!}d\nu(x)+\int_\R \frac{\abs{tx}^{n+1}}{(n+1)!}d\gamma(x)\\
      &\leq \left(\int_{\R}\frac{\abs{tx}^{2n+2}}{(n+1)!^2}d\nu(x)\right)^{1/2}+\left(\int_{\R}\frac{\abs{tx}^{2n+2}}{(n+1)!^2}d\gamma(x)\right)^{1/2}\\
      &=\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\nu)\right)^{1/2}+\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\gamma)\right)^{1/2}\\
      &=2\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\gamma)\right)^{1/2}.
    \end{align*}
   Recordando que para la distribución Gaussiana se cumple que $m_{2n}(\gamma)=\frac{(2n)!}{2^{n}n!}$, y usando cotas superior e inferior de la fórmula de Stirling, tenemos que 
   \begin{align*}
      2\left(\frac{\abs{t}^{2n+2}}{(n+1)!^2}m_{2n+2}(\gamma)\right)^{1/2}&=2|t|^{n+1}\left(\frac{(2n+2)!}{2^{n+1}(n+1)!^3}\right)^{1/2}\\
      &\leq 2|t|^{n+1}\left(\frac{\sqrt{4\pi(n+1)}\left(\tfrac{2(n+1)}{e}\right)^{2n+2}e^{-12\cdot 2(n+1)}}{2^{n+1}\left(\sqrt{2\pi(n+1)}\left(\tfrac{n+1}{e}\right)^{n+1}e^{-12(n+1)+1}\right)^{3}}\right)^{1/2}\\
      &\leq 2|t|^{n+1}\left(\frac{2^ne^{13(n+1)+3}}{\pi(n+1)^{n+2}}\right)^{1/2}\\
      &=\frac{K_t^{\tfrac{13}{2}(n+1)}}{(n+1)^{n/2+1}},\\
   \end{align*} 
   donde $K_t$ es una constante dependiente sólo de $t$. Pero esta última expresión tiende a cero cuando $n$ tiende a infinito. Concluimos que las funciones características de ambas medidas coinciden y por lo tanto, son equivalentes.
  \end{proof}

Ahora enunciamos y demostramos el conocido como `lema de Stein'.
\begin{teo}\textbf{(Lema de Stein)} \label{LemaStein} Sea $X$ una variable aleatoria. $X$ tiene una distribución normal estándar si y sólo sí, para cualquier función $f$ derivable y tal que $f'\in L^{1}(\gamma)$, se tiene que
   \begin{enumerate}
       \item $\E\left[Xf(X)\right]<\infty \ $ y $ \ \E\left[f'(X)\right]<\infty$.
       \item $\E\left[Xf(X)\right]=\E\left[f'(X)\right]$.
   \end{enumerate}
\end{teo}
\begin{proof} 
  Si directamente $X\sim$ Normal$(0,1)$, cualquier función que cumpla las condiciones del enunciado cumple las del lema \ref{lema1}, el cual precisamente estipula la relación anterior. 

  Supongamos ahora que $X$ satisface la relación $\E\left[Xf(X)\right]=\E\left[f'(X)\right]$ para cualquier función $f$ con las propiedades del enunciado. Si definimos $f_n(x)=x^n$ para cualquier $n\geq1$, tenemos que la variable $X$ tiene todos sus momentos definidos, y además 
  \[
   \E\left[X^{n+1}\right]=\E\left[Xf(X)\right]=\E\left[f'(X)\right]=n\E\left[X^{n-1}\right],
  \] 
  de tal forma que los momentos de la variable $X$ coinciden con los de una variable normal estándar. Dado que la distribución normal está caracterizada por sus momentos según vimos en el resultado previo, se sigue que $X\sim$ Normal$(0,1)$.
 \end{proof}
El resultado anterior es interesante por varios motivos. Uno de ellos es que nos permite caracterizar plenamente la distribución normal a partir de funciones y sus derivadas aplicadas a una variable aleatoria que siga dicha distribución. Por otro lado, un instante de reflexión nos puede llevar a la siguiente pregunta.

Supongamos que ahora $X$ es una variable aleatoria y que para una cierta clase suficentemente rica de funciones suaves, se cumple que 
\[
\E\left[Xf(X)-f'(X)\right]\approx0.    
\]
En virtud del lema de Stein, ¿es posible decir que la distribución de $X$ está cerca (en algún sentido) de la distribución normal estándar?

La pregunta anterior se conoce como la \textit{heurística de Stein}, y la respuesta es afirmativa en algunos casos. Para llegar a una formulación explícita de dicha respuesta, debemos introducir la idea de `distancia entre dos medidas de probabilidad' y también la famosa `ecuación de Stein' asociada a alguna función $h\in L^1(\gamma)$. 

La definición de distancia se otorga para variables que tomen valores en $\R^d$. Comenzamos definiendo la noción de distancia entre dos variables aleatorias. Recordamos la noción de clase separante de funciones.

\begin{dfn} 
 En un espacio de probabilidad $(\Omega, \F, \P)$, sea $\mathscr{H}$ un conjunto de funciones borelianas complejas con valores en $\R^d$, para algún $d\geq1$. Decimos que $\mathscr{H}$ es una familia de funciones separantes si para cualesquiera dos variables aleatorias $F$ y $G$ con valores en $\R^{d}$ tales que $h(F)$ y $h(G)$ son elementos de $L^1(\Omega)$ y $\E\left[h(F)\right]=\E\left[h(G)\right]$, para cualquier elemento $h\in \mathscr{H}$ implica que $F$ y $G$ tienen la misma ley.
 \end{dfn}
Ejemplos clásicos de familias $\mathscr{H}$ separantes son el conjunto de funciones borelianas continuas y acotadas, el conjunto de funciones indicadoras de los borelianos de $\R^{d}$, o el conjunto de funciones exponenciales de la forma $\mathscr{H}=\left\{e^{i \langle v,\cdot\rangle}: v\in \R^{d} \right\}$, donde el producto interno anterior es el producto interno usual de $\R^{d}$.

Definimos a continuación, y de manera rigurosa, la noción de distancia entre las leyes de dos variables aleatorias.
\begin{dfn} 
 Sea $\mathscr{H}$ una clase separante de funciones, y sean $F$ y $G$ dos variables aleatorias con valores en $\R^{d}$ y tales que $h(F)$ y $h(g)$ están en $L^1(\Omega)$ para cualquier $h\in \mathscr{H}$. Definimos la distancia inducida por $\mathscr{H}$ entre las distribuciones de $F$ y $G$ como 
 \[
 d_\mathscr{H}(F,G):=\sup_{h\in \mathscr{H}}\left\{\abs{\E\left[h(F)\right]-\E\left[h(G)\right]}\right\}.
 \]
 \end{dfn}

Es sencillo demostrar que para una clase de funciones separantes $\mathscr{H}$, el mapeo $d_\mathscr{H}$ definido anteriormente en efecto define una métrica en cierto subconjunto de las medidas de probabilidad sobre $\R^{d}$. Es pertinente hacer hincapié en que la distancia inducida entre las leyes de variables depende de la clase $\mathscr{H}$. Ello resulta en una amplia variedad de opciones con las cuales considerar la distancia entre dos distribuciones de probabilidad. Algunas de las más usadas se encuentran a continuación.

\begin{itemize}
   \item \textbf{La distancia de Kolmogorov.} Esta distancia entre dos variables $F$ y $G$, denotada por $d_{\text{Kol}}(F,G)$, se obtiene utilizando a la familia \[\mathscr{H}= \left\{h:\R^d\to \R \ :\ h(x_1,...,x_d)=\1_{(-\infty,z_1]}(x_d)\cdot ... \cdot \1_{(-\infty,z_d]}(x_d), \ \  z_1,...,z_d\in \R\right\},\]
   de manera que dicha distancia está dada por la expresión
   \[
   d_{\text{Kol}}(F,G)=\sup_{z_1,...,z_d\in \R}\abs{\P(F\in (-\infty,z_1]\times...\times(-\infty,z_d])-\P(G\in (-\infty,z_1]\times...\times(-\infty,z_d])}.
   \]
   \item \textbf{La distancia de variación total.} Dicha distancia entre las leyes de las variables $F$ y $G$ se define utilizando la clase \[\mathscr{H}=\left\{h:\R^d\to \R \ :\ h(x_1,...,x_d)=\1_B(x_1,...,x_d), \ \ B\in \B(\R^{d})\right\},\] 
   y dicha distancia se denota por $d_{\text{TV}}(F,G).$ Su expresión explícita es 
   \[
      d_{\text{TV}}(F,G)= \sup_{B\in \B(\R^{d})}\abs{\P(F\in B)-\P(G\in B)}.
   \]
   Observamos directamente que $d_{\text{Kol}}(F,G)\leq d_{\text{TV}}(F,G)$.
   \item \textbf{La distancia de Wasserstein.} Usando la clase de funciones separantes \[\mathscr{H}=\left\{h:\R^{d}\to \R \ :\ \norm{h}_{\text{Lip}}\leq 1\right\},\] donde claramente $\norm{\cdot}_{Lip}$ denota la norma Lipschitz en $\R^{d}$. Esta distancia se denota como $d_{\text{W}}(F,G)$.
\end{itemize}

Armados con la noción de distancia entre dos leyes de probabilidad, pasamos ahora a estudiar brevemente las ecuaciones de Stein. Dichas ecuaciones nos permiten codificar la distancia entre la ley de una variable $F$ y una variable aleatoria normal $N$, en virtud del lema de Stein \ref{LemaStein}. De manera rigurosa, tenemos la siguiente definición.

\begin{dfn} 
 Sea $N$ una variable gaussiana estándar y sea $h:\R\to\R$ una función boreliana tal que $\E\left[h(N)\right]<\infty$. La ecuación de Stein asociada con $h$ es la ecuación diferencial ordinaria 
 
 \begin{equation}\label{eqstein}
   f'(x)-xf(x)=h(x)-\E\left[h(N)\right].
 \end{equation}
 
 Diremos que $f$ es una solución a la ecuación \eqref{eqstein} si dicha función es absolutamente continua y tal que existe función equivalente a la derivada $f'$ que satisface \eqref{eqstein} para cualquier $x\in \R$.
 \end{dfn}

 Es de notar que dicha ecuación diferencial es sencilla de resolver. Este es el contenido del siguiente resultado.

 \begin{teo}\label{soleqstein}
  Las soluciones a la ecuación \eqref{eqstein} son de la forma 
  \[
  f(x)=ce^{x^2/2}+e^{x^2/2}\int_{-\infty}^x(h(y)-\E\left[h(N)\right])e^{-y^2/2}dy, \qquad \forall x\in \R,
  \] 
  donde $c\in \R$ es constante. En particular, si escribimos 
  \[
   f_h(x):=e^{x^2/2}\int_{-\infty}^x(h(y)-\E\left[h(N)\right])e^{-y^2/2}dy,
  \]
  entonces $f_h$ es la única solución a la ecuación de Stein \eqref{eqstein} que satisface la condición 
  \[
  \lim_{x\to \infty}e^{-x^2/2}f(x)=0.
  \]
  \end{teo}
  \begin{proof} 
    Supongamos que $h$ cumple las hipótesis del enunciado. Partiendo de las igualdades 
    \[
    \frac{d}{dx}\left(e^{-x^2/2}f(x)\right)=-xe^{-x^2/2}f(x)+e^{-x^2/2}f'(x)=e^{-x^2/2}(f'(x)-xf(x)),
    \]
    la ecuación de Stein $\eqref{eqstein}$ se puede reformular como 
    \[
      e^{x^2/2}\frac{d}{dx}\left(e^{-x^2/2}f(x)\right)=h(x)-\E\left[h(N)\right].
    \] 
    Usando el método de factor integrante, es claro que las soluciones a la ecuación  \eqref{eqstein} tienen la forma 
    \[
    f(x)=ce^{x^2/2}+^{x^2/2}\int_{-\infty}^{x}(h(y)-\E\left[h(N)\right])e^{-y^2/2}dy, \qquad \forall x\in \R.
    \]
   Para el comportamiento en $\pm\infty$ de las solución $f_h$, usando la función $g(x)=\abs{h(y)-\E\left[h(N)\right]}e^{-y^2/2}$, la cual es integrable e independiente de $x\in \R$, por el teorema de convergencia dominada, tenemos para el límite en $+\infty$ que
   \begin{align*}
      \lim_{x\to \infty}\int_{-\infty}^x \left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}dy&=\int_\R\lim_{x\to\infty}\left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}\1_{(-\infty,x]}(y)dy\\
      &=\int_\R h(y)e^{-y^2/2}dy-\E\left[h(N)\right]\int_\R e^{-y^2/2}dy\\
      &=\sqrt{2\pi}\E\left[h(N)\right]-\sqrt{2\pi}\E\left[h(N)\right]\\
      &=0,
      \end{align*}
      mientras que para el límite en $-\infty$ tenemos que 
      \[
         \lim_{x\to -\infty}\int_{-\infty}^x \left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}dy=\int_\R\lim_{x\to-\infty}\left(h(y)-\E\left[h(N)\right]\right)e^{-y^2/2}\1_{(-\infty,x]}(y)dy=0.
      \]
      Se sigue entonces que si $f$ es una solución como las anteriormente mencionadas, entonces $e^{-x^2/2}f(x)\xrightarrow[x\to\pm\infty]{}0$ si y solo si $c=0$.
   \end{proof}

 Contando ahora con estas herramientas, podemos evaluar de manera rigurosa la distancia que hay entre la ley de una variable $X$ y una variable normal estándar $N$. Para ello, consideremos una función $h:\R\to\R$ que cumple que $\E\left[\abs{h(X)}\right]<\infty$ y $\E\left[\abs{h(N)}\right]<\infty$. Sea $f_h$ la solución única del teorema \ref{soleqstein}. Tomando esperanza a ambos lados de la ecuación de Stein, tenemos que $f_h$ cumple que 
\begin{equation}\label{eqmetodostein}
   \E\left[h(X)\right]-\E\left[h(N)\right]=\E\left[f'_h(X)-Xf_h(X)\right].
 \end{equation}
 
 En particular, notamos que para una clase de funciones adecuada $\mathscr{H}$, 
  \begin{equation*}
   d_\mathscr{H}(X,N)=\sup_{h\in \mathscr{H}}\abs{\E\left[h(X)\right]-\E\left[h(N)\right]}=\sup_{h\in \mathscr{H}}\abs{\E\left[f_h'(X)-Xf_h(X)\right]}.
   \end{equation*}
 Es de destacar que en la expresión anterior, la distancia entre las leyes de las variables $X$ y $N$ están codificadas en la cantidad de la derecha, la cual no involucra la ley Gaussiana estándar de manera directa. Por otro lado, es deseable desarrollar algunas cotas para la norma de las soluciones $f_h$, dependiendo del tipo de funciones $h\in\mathscr{H}$ que estemos considerando. Dependiendo de dicha clase, podremos obtener distintas cotas para las distancias en las que estemos interesados. A dichas cotas se les conoce como \textit{cotas de Stein}.
 
 En nuestro caso, nos vamos a enfocar en dos familias de funciones distintas. La primera de ellas es la familia $\mathscr{H}$ que origina la distancia de Kolmogorov, mientras que en el segundo caso consideramos una familia específica que nos será de utilidad en el capítulo 4.
 
En cuanto a la distancia de Kolmogorov, tenemos el siguiente teorema que nos da una cota universal explícita de las normas de las soluciones a la ecuación de Stein asociada a la familia $\mathscr{H}= \left\{h:\R\to \R \ :\ h(x)=\1_{(-\infty,z]}(x),\ \  z\in \R\right\}$.
  
 \begin{teo}\label{cotakolmogorovstein}
   Sea $z\in \R$ y consideremos a la función $h=\1_{(-\infty,z]}\in \mathscr{H}$ asociada. Denotemos por $f_z$ a la única solución de la ecuación de Stein asociada a la función $h$ anterior. Se tiene la siguiente cota.
  \[
      \norm{f_z}_{\infty}\leq \frac{\sqrt{2\pi}}{4} \qquad \text{y} \qquad \norm{f'_z}_\infty\leq 1.
  \]
En particular, para $N$ una variable aleatoria normal estándar, y para cualquier variable integrable $X$, se tiene que 
\[
   d_{\text{Kol}}(X,N)\leq \sup_{f\in \F_{Kol}}\abs{\E\left[f'(X)\right]-\E\left[Xf(X)\right]},
\]
en donde 
\[\F_{\text{Kol}}:=\left\{f:\R\to\R \text{ abs. cont.}: f \text{ derivable salvo un conjunto finito}, \norm{f}_\infty\leq \frac{\sqrt{2\pi}}{4}, \ \norm{f'}_\infty\leq 1\right\},\]
y donde el supremo anterior se entiende como la cantidad 
\[
\sup \abs{\E\left[g(X)\right]-\E\left[Xf(X)\right]},
\]
donde el supremo se toma sobre todos los pares de funciones borelianas $(f,g)$, con $f$ derivable salvo un conjunto finito de puntos, y $g$ es una versión de $f'$ tal que $\norm{g}_\infty\leq 1$.
\end{teo}

Una guía para la demostración de este resultado se puede encontrar en la sección 3.4 de \cite{Nourdin_Peccati_2012}. Concluimos esta sección con el siguiente resultado, el cual nos da cotas explícitas para una familia de funciones específicas.

\begin{teo}\label{metodosteinfamespecial}
 Consideremos la familia de funciones $\mathscr{H}=\left\{h:\R\to\R : h(z)=\1_{\{z>x\}}z, \ x\in \R\right\}$. Entonces la solución $f_h$ a la ecuación de Stein \eqref{eqstein} satisface 
 \[
 |f'_h(x)|\leq C(\abs{x}+1), \qquad \forall x\in \R,
 \]
 en donde $C$ es una constante.
 \end{teo}
Nuevamente este resultado se enuncia sin demostración. Una versión general del mismo, así como su demostración, se encuentran en el apéndice A de \cite{HU2014814}.

Los dos resultados anteriores se encuentran en el núcleo de las técnicas que se conocen en su conjunto como \textit{método de Stein.}, y que son amplimente usadas para deducir resultados similiares al teorema central del límite, además de deducir tasas de convergencia para los mismos. Ejemplo de lo anterior es el célebre teorema de Berry-Essen, el cual cuantifica la velocidad de convergencia de una sucesión i.i.d de variables con media cero, varianza 1 y tercer momento absoluto finito, a la distribución normal estándar. Es decir, es una versión cuantificable del clásico teorema del límite central.

Es posible otorgar una demostración del mismo utilizando el método de Stein, y más específicamente el teorema \ref{cotakolmogorovstein}. Para el enunciado y demostración del mismo, nos referimos a la sección 3.7 de \cite{Nourdin_Peccati_2012}.

\section{Existencia de densidades y estimación de la distancia uniforme}

En preparación para el capítulo 4, en esta sección estudiamos brevemente un par de resultados. El primero de ellos nos garantiza la existencia y unicidad de una variable aleatoria unidimensional bajo ciertas hipótesis, además de proveer una fórmula para la misma. También enunciamos un resultado que consiste en una cota para la distancia uniforme entre la densidad de una variable aleatoria y la densidad de una variable normal estándar, siendo este último un resultado medular en el capítulo 4. Es de destacar que estos resultados son consecuencias del teorema \ref{factordeltafuera} y ambos, por orden de aparición, se pueden encontrar en \cite[proposición 1]{Caballero1998-hz} y en \cite[teorema 3.2]{KUZGUN202268}.

Comenzamos directamente con el primer resultado. La demostración del mismo se omite en este texto.
\begin{teo}\label{existdensidad}
 Sean $F$ una variable aleatoria en el espacio $\D^{1,1}$ y $\mathfrak{H}$ un espacio de Hilbert. Supongamos que $u$ es una variable en $L^{1}(\Omega;\mathfrak{H})$ tal que $D_uF\neq 0$ casi seguramente. Supongamos también que $\tfrac{u}{D_uF}\in \text{Dom}(\delta)$. Entonces la ley de $F$ tiene una densidad continua y acotada dada por 
   \begin{equation}\label{formuladensidad}
      f_F(x)=\E\left[\1_{\{F>x\}}\delta \left(\frac{u}{D_uF}\right)\right].
   \end{equation}
 \end{teo}
Es de nuestro interés saber en qué caso es cierto que $\tfrac{u}{D_uF}\in \text{Dom}(\delta)$. El siguiente lema nos da una respuesta.

\begin{lema}\label{lemaexistdensidad}
 Sean $p,p'>1$ tales que $\frac{1}{p}+\frac{1}{p'}=1$, y sea $F\in \D^{2,1}$. Supongamos que $u\in \text{Dom}(\delta)$. Si se cumplen las siguientes condiciones 
 \begin{enumerate}
   \item $u\in L^{p}(\Omega;\mathfrak{H})$,
   \item $\delta(u)\in L^{p}(\Omega)$,
   \item $(D_uF)^{-1}\in L^{p'}(\Omega)$,
   \item $(D_uF)^{-2}\left(\norm{D^2F}_{\mathfrak{H}^{\otimes 2}}\norm{u}_{\mathfrak{H}}+\norm{Du}_{\mathfrak{H}}\norm{DF}_{\mathfrak{H}}\right)\in L^{p'}(\Omega)$,
 \end{enumerate}
 entonces $\tfrac{u}{D_uF}\in \text{Dom}(\delta)$ y en particular, se cumple la ecuación \eqref{eqfactordeltafuera}. 
 \end{lema}
 Para una prueba de este lema, se puede consultar \cite[lema 3, lema 4]{Caballero1998-hz}.

Estamos interesados en aplicar tanto el lema anterior como el teorema \ref{existdensidad} en condiciones que aparecerán en el capítulo 4. Por lo tanto, y siguiendo de cerca a \cite[sección 3]{KUZGUN202268}, tenemos el siguiente resultado. 

\begin{teo} 
 Sean $u\in \D^{1,6}(\Omega;\mathcal{\mathfrak{H}})$ y definamos la variable $F:=\delta(u)$. Supongamos que $F\in \D^{2,6}$, con $\E\left[F\right]=0, \ \E\left[F^{2}\right]=1$ y $\left(D_uF\right)^{-1}\in L^4(\Omega)$. Entonces $u/D_uF \in \text{Dom}(\delta)$, la variable $F$ admite una densidad continua y acotada $f_F(x)$ y la misma cumple la siguiente desigualdad
 \begin{equation}\label{cotafundamental}
    \sup_{x\in \R} \abs{f_F(x)-\Phi(x)}\leq \left(\|F\|_4\|\left(D_vF\right)^{-1}\|_4+2\right)\|1-D_vF\|_2+\|\left(D_vF\right)^{-1}\|^{2}\|D_v \left(D_vF\right)\|_2,
 \end{equation}
donde $\Phi(x)$ es la función de densidad de una variable normal estándar.

\end{teo}
\begin{proof} 
   Demostramos primero que $\frac{u}{D_uF}\in \text{Dom}(\delta)$. Buscamos satisfacer las hipótesis del lema \ref{lemaexistdensidad}. Para ello usamos $p=p'=2$. Claramente $p,p'>1$ y se cumple que $\frac{1}{p}+\frac{1}{p'}=1$. Por otro lado, por hipótesis $u\in \text{Dom}(\delta)$ y además, dado que en nuestro caso la variable $F$ es por definición la divergencia de $u$, de la hipótesis y de la contención entre los espacios $\D^{k,q}$ se sigue que $F\in \D^{2,6}\subseteq\D^{2,1}$. Resta mostrar que las cuatro condiciones del lema \ref{lemaexistdensidad} se satisfacen.
   \begin{enumerate}
      \item Dado que $u\in \D^{1,6}(\Omega;\mathfrak{H})\subseteq\D^{1,2}(\Omega;\mathfrak{H})$, y que por construcción este último espacio es un subconjunto de $L^{2}(\Omega;\mathfrak{H})$, se cumple la primera condición.
      \item Por hipótesis $\delta(u)=F\in \D^{2,6}$, y este es subconjunto de $L^{2}(\Omega)$, por lo que la segunda condición también se cumple.
      \item El enunciado nos indica que $(D_uF)^{-1}\in L^4(\Omega)$. Como la medida del espacio es finita, se da la contención $L^4(\Omega)\subseteq L^2(\Omega)$ y la tercera condición se satisface.
      \item Denotemos $a:=\left(\norm{D^2F}_{\mathfrak{H}^{\otimes 2}}\norm{u}_{\mathfrak{H}}+\norm{Du}_{\mathfrak{H}}\norm{DF}_{\mathfrak{H}}\right)$. Obsérvese que dichas cantidades están bien definidas gracias a las hipótesis. Notamos que
      \[
      \norm{a(D_uF)^{-2}}_2^2=\int_\Omega (a(D_uF)^{-2})^2d\P=a^2\int_\Omega ((D_uF)^{-1})^4d\P=a^2\norm{(D_uF)^{-1}}_4^{4}<\infty.
      \] 
      Concluimos que la última condición se satisface.   
   \end{enumerate}
   Por lo tanto, $u/D_uF\in \text{Dom}(\delta)$. Ahora bien, el resto de hipótesis del teorema \ref{existdensidad} se satisfacen, pues se tiene que $F\in \D^{2,6}\subseteq \D^{1,1}$. Además, $u\in \D^{1,6}\subseteq \D^{1,1}$ y este último es subconjunto de $L^{1}(\Omega;\mathfrak{H})$. Finalmente si fuera el caso que $D_uF=0$ en un conjunto medible de probabilidad positiva $A\subseteq \Omega$, tendríamos que 
   \[
   \norm{(D_uF)^{-1}}_1=\int_\Omega \abs{(D_uF)}^{-1}d\P\geq\int_A\abs{(D_uF)}^{-1}d\P=+\infty,
   \]
   lo cual es una contradicción ya que $(D_uF)^{-1}\in L^{4}(\Omega)$, que es subconjunto de $L^{1}(\Omega)$.

   Satisfechas las hipótesis del teorema \ref{existdensidad}, tenemos garantizada la existencia de una densidad continua y acotada para la variable $F=\delta(v)$, la cual está dada por la fórmula \eqref{formuladensidad}. Ahora buscamos una cota uniforme sobre $x\in \R$ para las siguientes cantidades
   \begin{equation}\label{eqdiferencia1}
   \abs{f_F(x)-\phi(x)}=\abs{\E\left[\1_{\{F>x\}}\delta\left(\frac{u}{D_uF}\right)\right]-\E\left[\1_{\{N>x\}}N\right]}, 
   \end{equation}
  
  donde $N\sim$ Normal$(0,1)$.
   En virtud de que se satisfacen las hipótesis del lema \ref{existdensidad}, en la fórmula \eqref{formuladensidad} podemos usar la ecuación \eqref{eqfactordeltafuera} y obtenemos que, para $x\in \R$ arbitrario,
   \begin{align*}
   f_F(x)=\E\left[\1_{\{F>x\}}\delta\left(\frac{u}{D_uF}\right)\right]&=\E\left[\1_{\{F>x\}}\left(\frac{1}{D_{u}F}\delta(u)-\Big\langle D \left(\frac{1}{D_{u}F}\right),u \Big\rangle \right)\right]\\
   &=\E\left[\1_{\{F>x\}}\frac{F}{D_{u}F}\right]+\E\left[\1_{\{F>x\}}\left(\frac{1}{\left(D_{u}F\right)^2}\right)D_{u}D_{u}F\right],
      \end{align*}
      donde hemos hecho uso de que $F=\delta(u)$ y de la regla de la cadena en la segunda igualdad.
  Incrustando esta última igualdad en la fórmula \eqref{eqdiferencia1}, y subsecuentemente sumando y restando el término $\E\left[\1_{\{F>x\}}F\right]$,
  
\begin{align}\label{eqdiferencia2}
   \abs{f_F(x)-\Phi(x)}&=\abs{\E\left[\1_{\{F>x\}}\frac{F}{D_{u}F}\right]+\E\left[\1_{\{F>x\}}\frac{D_uD_uF}{\left(D_{u}F\right)^2}\right]-\E\left[\1_{\{N>x\}}N\right]}\notag\\
   &=+\Bigg|\E\left[\1_{\{F>x\}}\frac{F}{D_{u}F}\right]-\E\left[\1_{\{F>x\}}F\right]+\E\left[\1_{\{F>x\}}\frac{D_uD_uF}{\left(D_{u}F\right)^2}\right]\notag\\
   &\ \ \ +\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]\Bigg|\notag\\
   &\leq \abs{\E\left[\1_{\{F>x\}}\left(\frac{F}{D_uF}-F\frac{D_uF}{D_uF}\right)\right]}+\abs{\E\left[\1_{\{F>x\}}\frac{D_uD_uF}{\left(D_{u}F\right)^2}\right]}\notag\\
   &\ \ \ +\abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}\notag\\
   &\leq \E\left[\abs{\frac{F(1-D_uF)}{D_uF}}\right]+\E\left[\frac{\abs{D_uD_uF}}{(D_uF)^2}\right]+\abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}.
\end{align}
Ahora, en la desigualdad \eqref{eqdiferencia2} usamos sucesivamente la desigualdad de Hölder en el primer y segundo términos, de manera que
\begin{equation}\label{eqdiferencia3}
   \E\left[\abs{\frac{F(1-D_uF)}{D_uF}}\right]\leq \norm{F(D_uF)}_2\norm{1-D_uF}_2\leq \norm{F}_4\norm{(D_vF)^{-1}}_4\norm{1-D_uF}_2,
\end{equation}
y también
\begin{equation}\label{eqdiferencia4}
   \E\left[\frac{\abs{D_uD_uF}}{(D_uF)^2}\right]\leq \norm{D_uD_uF}_2\norm{(D_uF)^{-2}}_2=\norm{D_uD_uF}_2\norm{(D_uF)^{-1}}_4^{2}.
\end{equation}
Finalmente, estudiamos el tercer término de la desigualdad \eqref{eqdiferencia2}. Aquí es donde utilizamos el <<método de Stein>>. Observemos que, para $x\in \R$,
\[
   \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}=\abs{\E\left[h(F)\right]-\E\left[h(N)\right]},
\]
donde $h(z)=\1_{\{z>x\}}z$ es una función perteneciente a la familia $\mathscr{H}$ correspondiente al teorema \ref{metodosteinfamespecial}. En virtud de la ecuación \eqref{eqmetodostein}, la igualdad anterior se convierte en
\begin{equation}\label{eqdiferencia5}
   \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}=\abs{\E\left[f'_h(F)\right]-\E\left[Ff_h(F)\right]}.
\end{equation}
En este punto usamos nuevamente que $F=\delta(u)$, y como tal, cumple la fórmula de integración por partes. Por lo tanto, tomando como variable a $f_h(F)$ y usando regla de la cadena se tienen las siguientes igualdades
\begin{equation*}
   \E\left[Ff_h(F)\right]=\E\left[\delta(u)f_h(F)\right]=\E\left[\langle D(f_h(F)),u\rangle\right]=\E\left[f'_h(F)\langle DF,u\rangle\right]=\E\left[f'_h(F)D_uF\right].
\end{equation*}
Insertando la igualdad anterior en \eqref{eqdiferencia5} se sigue que 
   \begin{equation*}
      \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}=\abs{\E\left[f'_h(F)\right]-\E\left[f'_h(F)D_uF\right]}= \abs{\E\left[f_h'(F)(1-D_vF)\right]}.
   \end{equation*}
Usamos nuevamente la desigualdad de Hölder en el último termino de la ecuación anterior, obtenemos
\begin{equation}\label{eqdiferencia6}
   \abs{\E\left[f_h'(F)(1-D_vF)\right]}\leq \norm{f_h'(F)}_2\norm{1-D_vF}_2.
\end{equation}
Finalmente, usamos el teorema \ref{metodosteinfamespecial}, en \ref{eqdiferencia6}, por lo que 
\begin{equation}\label{eqdiferencia7}
      \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}\leq C(\abs{F}+1)\norm{1-D_vF}_2
\end{equation}
Finalmente, tenemos que $|F|\leq 1$ y $C\leq 1$, por lo que \ref{eqdiferencia7} se convierte en 
\begin{equation}\label{eqdiferencia8}
   \abs{\E\left[\1_{\{F>x\}}F\right]-\E\left[\1_{\{N>x\}}N\right]}\leq2\norm{1-D_vF}.
\end{equation}
Combinando las desigualdades \ref{eqdiferencia3}, \ref{eqdiferencia4} y \ref{eqdiferencia7}, tenemos para $x\in \R$,
\begin{align*}
   \abs{f_F(x)-\Phi(x)}&\leq \norm{F}_4\norm{(D_vF)^{-1}}_4\norm{1-D_uF}_2+\norm{D_uD_uF}_2\norm{(D_uF)^{-1}}_4^{2}+2\norm{1-D_vF}\\
   &=\left(\|F\|_4\|\left(D_vF\right)^{-1}\|_4+2\right)\|1-D_vF\|_2+\|\left(D_vF\right)^{-1}\|^{2}\|D_v \left(D_vF\right)\|_2.   
\end{align*}

Tomando supremo obtenemos la desigualdad \eqref{cotafundamental}.
\end{proof}

Tal y como se comentó al inicio de la sección, el resultado anterior resulta ser la cota principal desde la cual se deducirá la tasa de convergencia de las densidades de los promedios espaciales de la solución a la SHE, a la densidad normal. 

Hasta aquí llega nuestro estudio del método de Stein y dela existencia de las densidades.



\chapter{Propiedades espaciales de la Ecuación estocástica del calor}
En este capítulo presentaremos la parte central de la aplicación de las técnicas de Malliavin-Stein y SPDE's al estudio de la convergencia de las densidades de los promedios espaciales de la solución de la ecuación estocástica del calor, a la densidad de una variable aleatoria gaussiana estándar.

Consideremos la siguiente ecuación diferencial parcial estocástica:

\begin{equation}
\partial u_t=\frac{1}{2}\partial_{xx}u+\sigma(u)\dot{W}, \qquad \forall x\in \R, \forall t>0.
\end{equation}
cuya condición inicial es $u(0,x):=u_0(x)$, y $\dot{W}$ es un ruido blanco en $[0,\infty)\times\R$, esto es, tanto en el espacio como en el tiempo.

Del capítulo 1, tenemos garantizada la existencia y unicidad de una solución mild $u(t,x)$ a la ecuación 1, suponiendo que $\sigma$ es una función Lipschitz, y que $u_0$ es una medida signada que satisface la siguiente condición de integrabilidad:
\[
\int_\R p_t(x)\abs{u_0}(dx)=\int_\R\frac{1}{\sqrt{2\pi t}}\exp \left\{-x^2/2t\right\}\abs{u_0}(dx),    
\]
para $t>0$ y $x\in \R$. Estamos entonces interesados en el comportamiento asintótico de un nuevo objeto construido a partir de dicha solución: los promedios espaciales de la misma. Concretamente, en el caso en el que $u_0(x)=1$, para cualquier $x\in \R$, y $\sigma:\R\to\R$ es una función Lipschitz tal que $\sigma(1)\neq 0$, se definen los promedios espaciales de la solución $u$ como sigue: para $R>0$ fijo, 
\begin{equation}
   F_{R,t}:=\frac{1}{\sigma_{R,t}}\left(\int_{-R}^{R}u(t,x)dx -2R\right), \qquad \text{donde} \ \ \sigma^2_{R,t}=\text{Var}\left(\int_{-R}^{R}u(t,x)dx\right).
\end{equation}
Las variables $F_{R,t}$ son conocidas como los promedios espaciales centrados y normalizados. En \cite{HUANG20207170}, Huang, Nualart y Viitasaari estudian el comportamiento límite de $F_{R,t}$ conforme $R\to \infty$, siendo capaces de probar resultados tipo teorema central del límite para $F_{R,t}$. Más aún, en dicha publicación, utilizando técnicas de Malliavin-Stein, se obtuvieron cotas superiores para la tasa de convergencia de $F_{R,t}$ a la distribución normal estándar en distancia de variación total. De manera precisa, obtuvieron que
\begin{equation}
   d_{TV}(F_{R,t},N)\leq \frac{C_t}{\sqrt{R}}, \qquad \text{para }t>0,\text{ y } R\geq1.  
\end{equation}

Con este trabajo como antecedente, el propósito final del capítulo es obtener cotas superiores para la tasa de convergencia de la distancia uniforme entre las densidades de los promedios espaciales y la densidad de una variable aleatoria normal estándar. Concretamente, se busca probar el siguiente resultado.
\begin{teo} 
Sea $u=\left\{u(t,x):(t,x)\in [0,\infty)\times\R\right\}$ la solución mild de la ecuación estocástica del calor dada antes. Supongamos que $\sigma:\R\to\R$ es una función tal que 
\begin{itemize}
   \item $\sigma\in C^2(\R)$.
   \item $\sigma'$ es acotada.
   \item $\abs{\sigma''(x)}\leq C(1+\abs{x}^{m})$, \qquad para cualquier $x\in \R$ real y para alguna $m>0$.
\end{itemize}
Supongamos también que, para algún $q>10$, se cumple que 
\[
\E\left[\abs{|\sigma(u(t,0))}^{-q}\right]<\infty.    
\]
Entonces para $t>0$ fijo, se cumple que para cualquier $R>0$, la distancia entre las densidades de los promedios espaciales y la densidad de la distribución normal están acotadas uniformemente en $\R$ de la siguiente forma:
\[
\sup_{x\in \R} \abs{f_{F_{R,t}}(x)-\Phi(x)}\leq \frac{C_t}{\sqrt{R}},   
\]
donde $f_{F_{R,t}}$ y $\Phi$ son las densidades de $F_{R,t}$ y $N$ una variable aleatoria normal estándar, respectivamente, y $C_t$ es una constante dependiente de $t$.
\end{teo}

La idea de la prueba es la siguiente. Dados los preliminares de SPDE's y cálculo de Malliavin-Stein, primero se demuestra una cota superior elemental para la distancia uniforme entre la densidad de una variable aleatoria $F$ que está dada coom un funcional de un proceso Gaussiano isonormal, y la densidad de una variable aleatoria normal estándar.

Para lograr dicha estimación, es necesario ver a la función de distribución de la variable aleatoria $F$ como la integral de Skorokhod (o como divergencia en el sentido del cálculo de Malliavin). Esto es, $F=\delta(v)$ para algún $v$.

La idea es aplicar las técnicas de estimación anteriores a los promedios espaciales $F_{R,t}$ de la solución mild a la ecuación del calor. 

Tenemos el siguiente resultado, el cual se encuentra en \cite{Caballero1998-hz}, y del cual haremos uso sin ahondar en su demostración.
\begin{teo} 
Sea $F\in \D^{1,1}$ y  sea $v\in L^{1}(\Omega;\mathcal{H})$ tal que $D_vF\neq 0$ c.s. Supongamos que $v/D_vF\in \text{Dom}(\delta)$. Entonces la distribución de $F$ tiene una densidad continua y acotada dada por 
\[
f_F(x)=\E\left[\1_{\{F>x\}}\delta \left(\frac{v}{D_vF}\right)\right]   
\]
\end{teo}

El resultado anterior es válido para cualquier espacio de Hilbert $\mathfrak{H}$. Aplicando el resultado anterior al contexto de una variable aleatoria adecuada, se tiene lo siguiente:

\begin{teo} 
 Sea $v\in \D^{1,6}(\Omega;\mathcal{H})$ y $F=\delta(v)\in \D^{2,6}$ con $\E\left[F\right]=0, \E\left[F^{2}\right]=1$ y $\left(D_vF\right)^{-1}\in L^4(\Omega)$. Entonces $v/D_vF \in \text{Dom}(\delta)$, $F$ admite una densidad $f_F(x)$ y se tiene la siguiente desigualdad
 \[
   \sup_{x\in \R} \abs{f_F(x)-\Phi(x)}\leq \left(\|F\|_4\|\left(D_vF\right)^{-1}\|_4+2\right)\|1-D_vF\|_2+\|\left(D_vF\right)^{-1}\|^{2}\|D_v \left(D_vF\right)\|_2,
   \]
donde $\Phi(x)$ es la función de densidad de una variable normal estándar.

\end{teo}
\begin{proof} 
  Vamos a denotar por $F_R$ a los promedios espaciales de la ecuación estocástica del calor, y directamente por $\sigma$ a la varianza de los promedios espaciales.

  Buscamos hallar una cota uniforme para $x\in \R$ para las siguientes cantidades
  \[
  \abs{f_F(x)-\phi(x)}. 
  \]
  Gracias a que los promedios espaciales admite una densidad dada por 
  \[
  f_F(x)=\E\left[\1_{\{F_R>x\}}\delta(\frac{v}{D_vF_R})\right], 
  \]
  podemos operar con estas cantidades de la siguiente forma:
   \begin{align*}
   f_F(x)=\E\left[\1_{\{F_R>x\}}\delta(\frac{v}{D_vF_R})\right]&=\E\left[\1_{\{F_R>x\}}\frac{1}{D_{v_R}}\delta(v_R)\right]-\E\left[\1_{\{F_R>x\}}\langle D \left(\frac{1}{D_{v_R}F_R}\right),v_R\rangle_{\mathcal{h}}\right]\\
   &=\E\left[\1_{\{F_R>x\}}\frac{F_R}{D_{v_R}F_R}\right]+\E\left[\1_{\{F_R>x\}}\left(\frac{1}{\left(D_{v_R}F_R\right)^2}\right)D_{v_R}D_{v_R}F_R\right].
   \end{align*}
  Ahora bien, en el término de la derecha de antes, podemos acotar la cantidad de la siguiente forma:
  \[
   \E\left[\1_{\{F_R>x\}}\left(\frac{1}{\left(D_{v_R}F_R\right)^2}\right)D_{v_R}D_{v_R}F_R\right]\leq \sqrt{\E\left[\abs{D_{v_R}F_R}^{-4}\right]\E\left[\abs{D_{v_R}D_{v_R}F_R}^2\right]},
  \]
  de tal forma que el problema simplificado consiste en analizar el término de la izquierda, es decir, analizar.
  \[
   \E\left[\1_{\{F_R>x\}}\frac{F_R}{D_{v_R}F_R}\right] 
  \]
   Pero en cuanto a este término, notamos lo siguiente:
  \begin{align*}
   \E\left[\1_{[x,\infty)}(F_R)\frac{F_R}{D_vF_R}\right]&=\E\left[\1_{[x,\infty)}(F_R)F_R \left(\frac{1}{D_vF_R}-\frac{1}{\sigma^2}\right)\right] + \frac{1}{\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right]\\
  \end{align*}
  Ahora bien, nosotros tenemos la siguiente estimación 
  \[
  \sigma^2\approx \E\left[D_{V_R}F_R\right]\approx 1,  
  \]
  por lo que insertando dicha aproximación en la igualdad anterior, tenemos que 
  \begin{align*}
   \E\left[\1_{[x,\infty)}(F_R)F_R \left(\frac{1}{D_vF_R}-\frac{1}{\sigma^2}\right)\right] &+ \frac{1}{\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right]\\
   &=\frac{1}{(D_vF_R)\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right] \left(\E\left[D_vF_R\right]-D_vF_R\right)\\
   &+\frac{1}{\sigma^2}\E\left[\1_{[x,\infty)}(F_R)F_R\right]\\
  \end{align*}
  Tenemos aquí dos términos nuevamente. El primero de ellos lo podemos acotar de la siguiente forma:
  \[
    \|F_R\|_{L^4(\Omega)}\|\left(D_{v_R}F_R\right)^{-1}\|_{L^{4}(\Omega)}\text{Var}\abs{D_{v_R}F_R}^{1/2}\le C\|DD_{v_R}F_R\|^{1/2}_{L^{2}(\Omega,\mathcal{h})},
  \] 
  donde $C$ es una constante adecuada.

  Resta entonces analizar qué sucede con el siguiente término:
  \[
   \E\left[\1_{[x,\infty)}(F_R)F_R\right].
  \]
  Para ello, utilizamos la herramienta de Malliavin-Stein. Concretamente notemos que, sumando y restando el término $\1_{\{N>x\}}N$, donde $N$ es una variable aleatoria normal estándar, tenemos que, denotando por $g_x(F_R):=\1_{\{F_R>x\}}F_R$ y lo correspondiente para $g_x(N)$, 
  \[
   \E\left[\1_{\{F_R>x\}}\right]=\E\left[g_x(F_R)-g_x(N)\right]+\E\left[g_x(N)\right].
  \]
  El término de la izquierda en la ecuación anterior es sencillo de controlar, por lo que resta analizar la diferencia entre las esperanzas de $g_x$ evaluada en $F_R$ y en $N$. Para ello, de acuerdo al capítulo de método de Stein, notamos que 
  \[
  \E\left[g_x(F_R)-g_x(N)\right]=\E\left[F_R\psi_x(F_R)-\psi_x'(F_R)\right], 
  \]
  donde $y\psi_x(y)-\psi_x'(y)=g_x(y)-\E\left[g_x(N)\right]$.

  Ahora bien, analizando la esperanza en términos de $\psi_x$, tenemos que 
  
  \begin{align*}
   \E\left[F_R\psi_x(F_R)\right]&=\E\left[\delta(v_R)\psi_x(F_R)\right]\\
   &=\E\left[\abs{\langle v_R,DF_R\rangle}\psi_x'(F_R)\right]\\
   &=\E\left[D_{V_R}F_R\psi_x'(F_R)\right]\\
   &=\sigma^2 \E\left[\psi_x(F_R)\right]+\E\left[\left(D_{v_R}F_R-\sigma^2\right)\psi_x'(F_R)\right]\\
   &\le \|\psi_x'\|_\infty \text{Var}\abs{D_{v_R}F_R}^{-1/2}\\
   &\leq \|\psi_x'\|_\infty \|DD_{v_R}F_R\|_{L^2(\Omega,\mathfrak{H})}.
  \end{align*}
  Finalmente, para la prueba de la última cota, si nosotros vemos a la variable aleatoria normal $N$ como una divergencia, esto es, $N=\delta(h)$ para algún $h$ tal que $\|h\|=1$, entonces 
  \[
   f_N(x)=\E\left[\1_{\{N>x\}}\delta(h)\right]=\E\left[\1_{\{N>x\}N}\right]=\E\left[g_x(N)\right],
  \]
  por lo que concluimos la cota.
\end{proof}

Y para la prueba del resultado principal, es esencial el uso de dos resultados distintos: uno de ellos consiste en una cotas de momentos para la segunda derivada de Malliavin de la solución, y los momentos negativos de la proyección $DF_{R,t}$ en $v_{R,t}$, donde $F_{R,t}=\delta(v_{R,t})$. El primero de ellos se encuentra en el siguiente resultado:

\begin{teo} 
Sea $u$ la solución a la ecuación estocástica del calor con condición inicial $u_0=1$. Supongamos que las hipótesis del resultado principal. Fijando $(t,x)\in [0,\infty)\times \R$. Entonces $u(t,x)\in \bigcap_{p\geq2}\D^{2,p}$ y para casi todo $0<r<s<t$, $y,z\in \R$, la segunda derivada de $u$, a saber, $D_{r,z}D_{s,y}
u(t,x)$ satisface la siguiente ecuación diferencial estocástica lineal:

\begin{align*}
   D_{r,z}D_{s,y}u(t,x)&=p_{t-s}(x-y)\sigma'(u(s,y))D_{r,z}u(s,y)\\
   &+\int_{[s,t]\times \R}p_{t-\tau}(x-\xi)\sigma''(u(\tau,\xi))D_{r,z}u(\tau,\xi)D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\\
   &+\int_{[s,t]\times \R}p_{t-\tau}(x-\xi)\sigma'(u(\tau,\xi))D_{r,z}D_{s,y}u(\tau,\xi)W(d\tau,d\xi)\\
\end{align*}
Más aún, para cualesquiera $0\leq r<s<t\leq T$  y $x,y,z\in \R$, tenemos que 
\[
\|D_{r,z}D_{s,y}u(t,x)\|_p\leq C_{T,p}\Phi_{r,z,s,y}(t,x),
\]
donde $C_{T,p}$ es una constante que depende de $T,p$ y $\sigma$, y 

   \begin{align*}
       \Phi_{r,z,s,y}(t,x):=&p_{t-s}(x-y)\cdot \left(p_{s-r}(y-z)+\frac{p_{t-r}(z-y)+p_{t-r}(z-x)+\1_{\abs{y-x}>\abs{z-y}}}{(s-r)^{1/4}}\right)
   \end{align*} 
\end{teo}
Por otro lado, se tiene el siguiente resultado sobre la estimación de momentos negativos.

\begin{teo} 
Sea $u$ la solución mild a la ecuación estocástica del calor. Supongamos que $\sigma$ es Lipschitz. Sea $p\geq2$ fijo, $t>0$ y supongamos que existe un $q>5p$ tal que $\E\left[\abs{\sigma(u(t,0))}^{-2q}\right]<\infty$. entonces, existe una constante $R_0>0$ tal que 
\[
\sup_{R\geq R_0}\E\left[\abs{D_{v_{R,t}}F_{R,t}}^{-p}\right]<\infty.
\]
\end{teo}
La prueba de este resultado se omite en este texto, por lo que se refiere a $\cite{KUZGUN202268}$ para su consulta.




\section{Formulación Mild en el caso sin deriva}
%\section{Una ecuación estocástica para $Du$ y $D^2u$}
\section{Densidad explícita de los promedios espaciales}
%Disclaimer: no se van a manejar los momentos inversos.
\textit{Contribución del Kernel de Stein}
\textit{Contribución de las derivadas de orden superior}

\section{Aproximación uniforme de las densidades} %La sección integradora de los conceptos


\backmatter
\bibliographystyle{amsplain}
\bibliography{build/Referencias_tesis}
\end{document}